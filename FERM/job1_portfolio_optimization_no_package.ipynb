{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4ff26354",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'cvxopt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 12\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpypfopt\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mblack_litterman\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m BlackLittermanModel\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpypfopt\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mefficient_frontier\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m EfficientFrontier\n\u001b[0;32m---> 12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mcvxopt\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m matrix, solvers\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlinalg\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m inv, solve\n\u001b[1;32m     15\u001b[0m gc\u001b[38;5;241m.\u001b[39mcollect()\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'cvxopt'"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "import os\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats.mstats import winsorize\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from pypfopt import black_litterman, risk_models\n",
    "from pypfopt.black_litterman import BlackLittermanModel\n",
    "from pypfopt.efficient_frontier import EfficientFrontier\n",
    "from cvxopt import matrix, solvers\n",
    "from numpy.linalg import inv, solve\n",
    "\n",
    "gc.collect()\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "894a425b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\HYU\\\\Downloads\\\\FERM\\\\FERM'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0948b9b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_datasets(top_n, freq=\"M\"):\n",
    "    \"\"\"\n",
    "    monthly_dict 폴더의 parquet 파일들을 순회하면서 \n",
    "    cshtrm 상위 20개 기업을 추출하여 price_df, returns_df, cshthrm_df, conm_df를 생성\n",
    "\n",
    "    Parameters:\n",
    "    - monthly_dict_path: monthly_dict 폴더 경로\n",
    "    - top_n: 추출할 상위 기업 수 (기본값: 20)\n",
    "\n",
    "    Returns:\n",
    "    - price_df: 가격 데이터 (인덱스: 날짜, 컬럼: tic)\n",
    "    - returns_df: 수익률 데이터 (인덱스: 날짜, 컬럼: tic)\n",
    "    - cshthrm_df: 시가총액 데이터 (인덱스: 날짜, 컬럼: tic)\n",
    "    - conm_df: 회사명 데이터 (인덱스: 날짜, 컬럼: tic)\n",
    "    \"\"\"\n",
    "\n",
    "    # 결과를 저장할 딕셔너리\n",
    "    price_data = {}\n",
    "    returns_data = {}\n",
    "    cshthrm_data = {}\n",
    "    conm_data = {}\n",
    "\n",
    "    if freq == \"M\":\n",
    "        monthly_dict_path = \"data/monthly_dict\"\n",
    "        vol = 'cshtrm'\n",
    "    elif freq == \"D\":\n",
    "        monthly_dict_path = \"data/daily_dict\"\n",
    "        vol = 'cshoc'\n",
    "        \n",
    "    '''base_dir = os.getcwd()\n",
    "\n",
    "    if freq == \"M\":\n",
    "        monthly_dict_path = os.path.join(base_dir, \"data\", \"monthly_dict\")\n",
    "        vol = 'cshtrm'\n",
    "    elif freq == \"D\":\n",
    "        monthly_dict_path = os.path.join(base_dir, \"data\", \"daily_dict\")\n",
    "        vol = 'cshoc'\n",
    "        '''\n",
    "\n",
    "    # monthly_dict 폴더의 모든 parquet 파일 찾기\n",
    "    monthly_dict_dir = os.listdir(monthly_dict_path)\n",
    "\n",
    "    for file_path in sorted(monthly_dict_dir):\n",
    "        file_index = file_path.split('.')[0]\n",
    "        df = pd.read_parquet(os.path.join(monthly_dict_path, file_path))\n",
    "        df['cap'] = df['adj_closed']*df[vol]\n",
    "        valid_data = df.dropna(subset=['cshtrd'])\n",
    "\n",
    "        top_companies = valid_data.nlargest(top_n, 'cshtrd')\n",
    "\n",
    "        # 각 데이터프레임에 데이터 추가\n",
    "        i = 0\n",
    "        for _, row in top_companies.iterrows():\n",
    "            i += 1\n",
    "            # 가격 데이터 (prccd)\n",
    "            if 'adj_closed' in row and pd.notna(row['adj_closed']):\n",
    "                price_data[(file_index, 'Asset_'+str(i))] = row['adj_closed']\n",
    "\n",
    "            # 수익률 데이터 (prccd를 사용하여 계산)\n",
    "            if 'adj_closed_pct_change' in row and pd.notna(row['adj_closed_pct_change']):\n",
    "                returns_data[(file_index, 'Asset_'+str(i))\n",
    "                             ] = row['adj_closed_pct_change']\n",
    "\n",
    "            # 시가총액 데이터 (cshtrm)\n",
    "            if vol in row and pd.notna(row['cap']):\n",
    "                cshthrm_data[(file_index, 'Asset_'+str(i))] = row['cap']\n",
    "\n",
    "            # 회사명 데이터 (conm)\n",
    "            if 'conm' in row and pd.notna(row['conm']):\n",
    "                conm_data[(file_index, 'Asset_'+str(i))] = row['conm']\n",
    "\n",
    "    # price_df 생성\n",
    "    if price_data:\n",
    "        price_df = pd.DataFrame.from_dict(price_data, orient='index')\n",
    "        price_df.index = pd.MultiIndex.from_tuples(\n",
    "            price_df.index, names=['date', 'tic'])\n",
    "        price_df = price_df.unstack(level=1)\n",
    "        price_df.columns = price_df.columns.droplevel(0)\n",
    "    else:\n",
    "        price_df = pd.DataFrame()\n",
    "\n",
    "    # returns_df 생성\n",
    "    if returns_data:\n",
    "        returns_df = pd.DataFrame.from_dict(returns_data, orient='index')\n",
    "        returns_df.index = pd.MultiIndex.from_tuples(\n",
    "            returns_df.index, names=['date', 'tic'])\n",
    "        returns_df = returns_df.unstack(level=1)\n",
    "        returns_df.columns = returns_df.columns.droplevel(0)\n",
    "    else:\n",
    "        returns_df = pd.DataFrame()\n",
    "\n",
    "    # cshthrm_df 생성\n",
    "    if cshthrm_data:\n",
    "        cshthrm_df = pd.DataFrame.from_dict(cshthrm_data, orient='index')\n",
    "        cshthrm_df.index = pd.MultiIndex.from_tuples(\n",
    "            cshthrm_df.index, names=['date', 'tic'])\n",
    "        cshthrm_df = cshthrm_df.unstack(level=1)\n",
    "        cshthrm_df.columns = cshthrm_df.columns.droplevel(0)\n",
    "    else:\n",
    "        cshthrm_df = pd.DataFrame()\n",
    "\n",
    "    # conm_df 생성\n",
    "    if conm_data:\n",
    "        conm_df = pd.DataFrame.from_dict(conm_data, orient='index')\n",
    "        conm_df.index = pd.MultiIndex.from_tuples(\n",
    "            conm_df.index, names=['date', 'tic'])\n",
    "        conm_df = conm_df.unstack(level=1)\n",
    "        conm_df.columns = conm_df.columns.droplevel(0)\n",
    "    else:\n",
    "        conm_df = pd.DataFrame()\n",
    "\n",
    "    return price_df, returns_df, cshthrm_df, conm_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bebffa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DynamicSlidingWindowBL:\n",
    "    \"\"\"\n",
    "    Dynamic Sliding Window Algorithm using PyPortfolioOpt Black-Litterman\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_assets=10, initial_window=60, eta=0.95, h=0.1, c_minus=0.9, c_plus=1.1, risk_aversion=1.0):\n",
    "        self.n_assets = n_assets\n",
    "        self.M = initial_window\n",
    "        self.eta = eta\n",
    "        self.h = h\n",
    "        self.c_minus = c_minus\n",
    "        self.c_plus = c_plus\n",
    "        self.risk_aversion = risk_aversion\n",
    "        # Results storage\n",
    "        self.expected_returns = []\n",
    "        self.portfolio_weights = {}\n",
    "        self.portfolio_volatilities = []\n",
    "        self.window_sizes = []\n",
    "        self.market_returns = []\n",
    "        self.bl_returns = []\n",
    "        self.views_history = []\n",
    "\n",
    "    def generate_random_data(self, n_periods=1000, n_factors=5):\n",
    "        \"\"\"Generate random market data\"\"\"\n",
    "        np.random.seed(42)\n",
    "\n",
    "        # Generate factor data (AR(1) process)\n",
    "        factors = np.zeros((n_periods, n_factors))\n",
    "        for i in range(n_factors):\n",
    "            factors[0, i] = np.random.normal(0, 0.02)\n",
    "            for t in range(1, n_periods):\n",
    "                factors[t, i] = 0.9 * factors[t-1, i] + \\\n",
    "                    np.random.normal(0, 0.02)\n",
    "\n",
    "        # Generate asset returns (factor model + noise)\n",
    "        beta = np.random.normal(0, 0.5, (self.n_assets, n_factors))\n",
    "        alpha = np.random.normal(0.001, 0.002, self.n_assets)\n",
    "\n",
    "        returns = np.zeros((n_periods, self.n_assets))\n",
    "        for t in range(n_periods):\n",
    "            returns[t] = alpha + factors[t] @ beta.T + \\\n",
    "                np.random.normal(0, 0.02, self.n_assets)\n",
    "\n",
    "        # Generate price data for PyPortfolioOpt\n",
    "        prices = np.zeros((n_periods, self.n_assets))\n",
    "        prices[0] = 100  # Initial price\n",
    "        for t in range(1, n_periods):\n",
    "            prices[t] = prices[t-1] * (1 + returns[t])\n",
    "\n",
    "        # Convert to DataFrame\n",
    "        price_df = pd.DataFrame(prices,\n",
    "                                columns=[f'Asset_{i+1}' for i in range(self.n_assets)])\n",
    "\n",
    "        # Generate time-varying market caps\n",
    "        # Initial market caps (random distribution)\n",
    "        initial_caps = np.random.dirichlet(np.ones(self.n_assets)) * 1000000\n",
    "\n",
    "        # Market cap evolution (follows price changes with some noise)\n",
    "        cap_df = np.zeros((n_periods, self.n_assets))\n",
    "        cap_df[0] = initial_caps\n",
    "\n",
    "        for t in range(1, n_periods):\n",
    "            # Market cap changes follow price changes with some additional noise\n",
    "            price_change = prices[t] / prices[t-1]\n",
    "            cap_change = price_change * \\\n",
    "                (1 + np.random.normal(0, 0.01, self.n_assets))  # Add some noise\n",
    "            cap_df[t] = cap_df[t-1] * cap_change\n",
    "\n",
    "        # Convert to DataFrame\n",
    "        cap_df = pd.DataFrame(cap_df,\n",
    "                              columns=[f'Asset_{i+1}' for i in range(self.n_assets)])\n",
    "\n",
    "        return returns, factors, price_df, cap_df\n",
    "\n",
    "    def calculate_views_from_factors(self, factors, returns):\n",
    "        \"\"\"Calculate views using factor model\"\"\"\n",
    "        alphas = []\n",
    "        betas = []\n",
    "\n",
    "        for i in range(self.n_assets):\n",
    "            # Elastic Net regression\n",
    "            model = ElasticNet(alpha=0.1, l1_ratio=0.5, max_iter=1000)\n",
    "\n",
    "            scaler_X = StandardScaler()\n",
    "            scaler_y = StandardScaler()\n",
    "\n",
    "            X_scaled = scaler_X.fit_transform(factors)\n",
    "            y_scaled = scaler_y.fit_transform(\n",
    "                returns[:, i].reshape(-1, 1)).ravel()\n",
    "\n",
    "            model.fit(X_scaled, y_scaled)\n",
    "\n",
    "            beta = model.coef_ / scaler_X.scale_ * scaler_y.scale_\n",
    "            alpha = scaler_y.mean_[0] - np.sum(beta * scaler_X.mean_)\n",
    "\n",
    "            alphas.append(alpha)\n",
    "            betas.append(beta)\n",
    "\n",
    "        alphas = np.array(alphas)\n",
    "        betas = np.array(betas)\n",
    "\n",
    "        # Views setup\n",
    "        P = np.eye(self.n_assets)  # Direct views for each asset\n",
    "        # Factor model based expected returns\n",
    "        Q = alphas + np.mean(factors @ betas.T, axis=0)\n",
    "\n",
    "        # Views uncertainty\n",
    "        Omega = np.diag(np.var(returns, axis=0)) * 0.025\n",
    "\n",
    "        return P, Q, Omega\n",
    "\n",
    "    def calculate_portfolio_volatility(self, weights, returns):\n",
    "        \"\"\"Calculate portfolio volatility\"\"\"\n",
    "        portfolio_returns = returns @ weights\n",
    "        volatility = np.std(portfolio_returns)\n",
    "        return volatility\n",
    "\n",
    "    def adjust_window_size(self, current_vol, previous_vol):\n",
    "        \"\"\"Dynamically adjust window size based on volatility\"\"\"\n",
    "        if current_vol >= (1 + self.h) * previous_vol:\n",
    "            # Increase volatility -> decrease window size\n",
    "            self.M = int(self.c_minus * self.M)\n",
    "        elif current_vol <= (1 - self.h) * previous_vol:\n",
    "            # Decrease volatility -> increase window size\n",
    "            self.M = int(self.c_plus * self.M)\n",
    "\n",
    "        # Limit window size\n",
    "        self.M = max(15, min(self.M, 200))\n",
    "\n",
    "    def dict2numpy(self, w_bl, mu_bl):\n",
    "        # Convert to numpy arrays\n",
    "        # Handle OrderedDict or dict for weights\n",
    "        if isinstance(w_bl, dict):\n",
    "            w_bl = np.array(list(w_bl.values()))\n",
    "        elif hasattr(w_bl, 'values'):\n",
    "            w_bl = w_bl.values\n",
    "            if hasattr(w_bl, '__iter__') and not isinstance(w_bl, np.ndarray):\n",
    "                w_bl = np.array(list(w_bl))\n",
    "            else:\n",
    "                w_bl = np.array(w_bl)\n",
    "        else:\n",
    "            w_bl = np.array(w_bl)\n",
    "\n",
    "        # Handle OrderedDict or dict for returns\n",
    "        if isinstance(mu_bl, dict):\n",
    "            mu_bl = np.array(list(mu_bl.values()))\n",
    "        elif hasattr(mu_bl, 'values'):\n",
    "            mu_bl = mu_bl.values\n",
    "            if hasattr(mu_bl, '__iter__') and not isinstance(mu_bl, np.ndarray):\n",
    "                mu_bl = np.array(list(mu_bl))\n",
    "            else:\n",
    "                mu_bl = np.array(mu_bl)\n",
    "        else:\n",
    "            mu_bl = np.array(mu_bl)\n",
    "\n",
    "        return w_bl, mu_bl\n",
    "    \n",
    "    #-------------------------------#\n",
    "    # *** 직접 구현 새롭게 추가/변경 사항\n",
    "    def Pi(self, market_caps, risk_aversion, cov_matrix): # risk_free_rate은 0으로 고정\n",
    "        \"\"\"\n",
    "        market weight기반으로 implied equilibrium excess return(pi)를 구함\n",
    "        Π = δΣwmkt\n",
    "\n",
    "        Parameters:\n",
    "        - market_caps: 각 자산의 시가총액. {ticker: cap} dict or pd.Series\n",
    "        - risk_aversion: 위험 회피 계수. positive float\n",
    "        - cov_matrix: covariance matrix of asset returns. pd.DataFrame\n",
    "\n",
    "        Returns:\n",
    "        - implied equilibrium excess return. pd.Series\n",
    "        \"\"\"\n",
    "        if not isinstance(cov_matrix, pd.DataFrame):\n",
    "            warnings.warn(\n",
    "                \"If cov_matrix is not a dataframe, market cap index must be aligned to cov_matrix\",\n",
    "                RuntimeWarning,\n",
    "            )\n",
    "        market_caps_series = pd.Series(market_caps)\n",
    "        mkt_w = market_caps_series / market_caps_series.sum()\n",
    "        \n",
    "        return risk_aversion * cov_matrix.dot(mkt_w)\n",
    "    \n",
    "    def posterior_dist(self, cov_matrix, pi, P=None, Q=None, Omega=None, tau = 0.025):\n",
    "        \"\"\"\n",
    "        return: μ_BL, Σ_BL\n",
    "        μ_BL = M⁻¹ * b,  where\n",
    "            M = (τΣ)⁻¹ + Pᵀ Ω⁻¹ P\n",
    "            b = (τΣ)⁻¹ π + Pᵀ Ω⁻¹ q\n",
    "        Σ_BL = Σ + (M)⁻¹\n",
    "        \"\"\"\n",
    "        if P is None or Q is None:\n",
    "            # 전망이 없으면 prior distribution만\n",
    "            return pi.copy(), cov_matrix.copy()\n",
    "        \n",
    "        # (τΣ)⁻¹ 계산\n",
    "        I = np.eye(cov_matrix.shape[0])\n",
    "        inv_ts = solve(tau * cov_matrix, I)\n",
    "\n",
    "        # Ω⁻¹ 계산\n",
    "        I_omega = np.eye(Omega.shape[0])\n",
    "        inv_omega = solve(Omega, I_omega)\n",
    "\n",
    "        P_inv_om = P.T @ inv_omega # Pᵀ Ω⁻¹\n",
    "\n",
    "        M = inv_ts + P_inv_om @ P\n",
    "        b = inv_ts @ pi + P_inv_om @ Q\n",
    "        inv_M = solve(M, I)\n",
    "\n",
    "        mu_bl = solve(M,b)\n",
    "        sigma_bl = cov_matrix + inv_M\n",
    "        return mu_bl, sigma_bl\n",
    "\n",
    "    def mean_var_opt(self, mu, cov_matrix, risk_aversion, long_only = True):\n",
    "        '''\n",
    "        cvxopt 양식:\n",
    "            min (1/2) wᵀ (δΣ) w + (-μᵀ) w\n",
    "            s.t. 1ᵀ w = 1\n",
    "                (-I)w ≤ 0 (no shortselling)\n",
    "        long_only: 롱온리 제약 여부\n",
    "        return: (BL 사후 분포 만족하는) 가중치\n",
    "        '''\n",
    "        n = len(mu) # 자산 개수\n",
    "        # 공분산 singular 방지\n",
    "        eps = 1e-6\n",
    "\n",
    "        if isinstance(cov_matrix, pd.DataFrame):\n",
    "            cov_matrix = cov_matrix.values\n",
    "        if isinstance(mu, (pd.Series, pd.DataFrame)):\n",
    "            mu = np.array(mu).ravel()\n",
    "\n",
    "        P = matrix(risk_aversion * (cov_matrix + eps * np.eye(n))) # n*n\n",
    "        q = matrix(-mu, (n,1)) # n*1\n",
    "\n",
    "        # 부등식 제약. G = -I, h = [0,..,0]ᵀ\n",
    "        G, h = (matrix(-np.eye(n)), matrix(np.zeros(n))) if long_only else (None, None)\n",
    "        \n",
    "        # 만약 가중치 상한선 둘 거면 코드 변경    \n",
    "        '''G_list, h_list = [], []\n",
    "        if long_only:\n",
    "            G_list.append(-np.eye(n))\n",
    "            h_list.append(np.zeros(n))\n",
    "        if w_max is not None:\n",
    "            G_list.append(np.eye(n))\n",
    "            h_list.append(np.full(n, w_max))\n",
    "\n",
    "        G, h = (matrix(np.vstack(G_list), tc='d'), matrix(np.concatenate(h_list), tc='d')) if len(G_list) > 0 else (None, None)'''\n",
    "\n",
    "        # 등식 제약\n",
    "        A = matrix(np.ones(n),(1,n)) # A = [1,1,...,1]\n",
    "        b = matrix(1.0)\n",
    "\n",
    "        # Optimization\n",
    "        solvers.options['show_progress'] = False\n",
    "        sol = solvers.qp(P,q,G,h,A,b)\n",
    "        w = np.array(sol['x']).ravel()\n",
    "        return w\n",
    "    \n",
    "    # 여기까지\n",
    "    #--------------------------------------------------------------------------------#\n",
    "\n",
    "    def run_algorithm(self, returns_df, factor_df, cap_df, n_iterations=50):\n",
    "        \"\"\"Run the complete algorithm\"\"\"\n",
    "        n_periods = len(returns_df)\n",
    "        returns = returns_df.values\n",
    "        factors = factor_df.values\n",
    "        t = self.M\n",
    "\n",
    "        print(f\"Dynamic Sliding Window Algorithm Started\")\n",
    "        print(f\"Initial window size: {self.M}\")\n",
    "\n",
    "        for iteration in range(n_iterations):\n",
    "            if t + self.M >= n_periods:\n",
    "                break\n",
    "\n",
    "            # Step 1: Data Collection\n",
    "            r_data = returns[t-self.M:t]\n",
    "            f_data = factors[t-self.M:t]\n",
    "            r_df = returns_df.iloc[t-self.M:t]\n",
    "            date = str(returns_df.iloc[t].name)\n",
    "            # Step 2: Calculate covariance matrix using PyPortfolioOpt\n",
    "\n",
    "            cov_matrix = r_df.cov()\n",
    "            # Step 3: Calculate market implied returns using PyPortfolioOpt\n",
    "            # Get current market caps from cap_df\n",
    "            # Use market cap from previous period\n",
    "            current_market_caps = cap_df.iloc[t-1][r_df.columns]\n",
    "\n",
    "            # print(current_market_caps)\n",
    "            # *** package black_litterman.market_implied_prior_returns() -> Pi로 변경\n",
    "            pi = self.Pi(\n",
    "                current_market_caps, risk_aversion=self.risk_aversion, cov_matrix=cov_matrix\n",
    "            )\n",
    "\n",
    "            # Step 4: Calculate views using factor model\n",
    "            P, Q, Omega = self.calculate_views_from_factors(f_data, r_data)\n",
    "            # print(pi.values)\n",
    "\n",
    "            # *** Step 5: Optain posterior distribution by Black Litterman\n",
    "            mu_bl, sigma_bl = self.posterior_dist(cov_matrix, pi, P=P, Q=Q, Omega=Omega, tau = 0.025)\n",
    "\n",
    "            # *** Step 6: 논문의 공분산 구현\n",
    "            rrT = np.outer(r_data[-1], r_data[-1])\n",
    "            S_bl = self.eta * sigma_bl + (1 - self.eta) * rrT\n",
    "\n",
    "            # *** NaN 값 & inf 처리\n",
    "            mu_mean = np.nanmean(mu_bl)\n",
    "            mu_bl = np.nan_to_num(mu_bl, nan=mu_mean, posinf=0.0, neginf=0.0)\n",
    "            S_bl = np.nan_to_num(S_bl, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "\n",
    "            # *** 2) 최적화: 공매도 금지 (0 ≤ w ≤ 1)\n",
    "            w_bl = self.mean_var_opt(mu_bl, sigma_bl, risk_aversion= self.risk_aversion, long_only=True)\n",
    "\n",
    "            '''# Step 5: Apply Black-Litterman model using PyPortfolioOpt\n",
    "            bl = BlackLittermanModel(\n",
    "                cov_matrix, pi=pi, P=P, Q=Q, omega=Omega, tau=0.025)\n",
    "\n",
    "            # Step 6: Get Black-Litterman returns and weights\n",
    "            mu_bl = bl.bl_returns()     # posterior expected returns (Series)\n",
    "            rrT = np.outer(r_data[-1], r_data[-1])\n",
    "            S_bl = self.eta * bl.bl_cov() + (1 - self.eta) * rrT\n",
    "            # cov_matrix = risk_models.exp_cov(price_data, returns_data=True)\n",
    "\n",
    "            # NaN값 처리\n",
    "            mu_bl = mu_bl.fillna(mu_bl.mean())\n",
    "            S_bl = S_bl.fillna(0)\n",
    "            # inf값 처리\n",
    "            mu_bl = mu_bl.replace([np.inf, -np.inf], 0)\n",
    "            S_bl = S_bl.replace([np.inf, -np.inf], 0)'''\n",
    "\n",
    "\n",
    "            '''# 2) 최적화: 공매도 금지 (0 ≤ w ≤ 1)\n",
    "            # short-sale not allowed\n",
    "            ef = EfficientFrontier(mu_bl, S_bl, weight_bounds=(0, 1))\n",
    "            # ef.max_quadratic_utility(self.risk_aversion)\n",
    "            ef.max_sharpe(risk_free_rate=0.0)\n",
    "            w_bl = ef.clean_weights()\n",
    "\n",
    "            w_bl, mu_bl = self.dict2numpy(w_bl, mu_bl)'''\n",
    "\n",
    "            # Step 7: Execute transactions and evaluate volatility\n",
    "            current_returns = returns[t:t+self.M]\n",
    "            current_vol = self.calculate_portfolio_volatility(\n",
    "                w_bl, current_returns)\n",
    "\n",
    "            # Step 8: Dynamic window size adjustment\n",
    "            if iteration > 0:\n",
    "                previous_vol = self.portfolio_volatilities[-1]\n",
    "                self.adjust_window_size(current_vol, previous_vol)\n",
    "\n",
    "            # Store results\n",
    "            self.expected_returns.append(mu_bl.copy())\n",
    "            self.portfolio_weights[date] = w_bl.copy()\n",
    "            self.portfolio_volatilities.append(current_vol)\n",
    "            self.window_sizes.append(self.M)\n",
    "            self.market_returns.append(pi.copy())\n",
    "            self.bl_returns.append(mu_bl.copy())\n",
    "            self.views_history.append(Q.copy())\n",
    "\n",
    "            # Step 9: Update time\n",
    "            t += self.M\n",
    "\n",
    "            if iteration % 10 == 0:\n",
    "                print(\n",
    "                    f\"Iteration {iteration}: Window size = {self.M}, Volatility = {current_vol:.4f}\")\n",
    "\n",
    "        print(f\"Algorithm completed: {len(self.portfolio_weights)} iterations\")\n",
    "\n",
    "    def get_results_summary(self):\n",
    "        \"\"\"Get algorithm results summary\"\"\"\n",
    "        print(f\"\\n=== Algorithm Results Summary ===\")\n",
    "        print(f\"Final window size: {self.M}\")\n",
    "        print(\n",
    "            f\"Average portfolio volatility: {np.mean(self.portfolio_volatilities):.4f}\")\n",
    "        print(f\"Volatility std: {np.std(self.portfolio_volatilities):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21fdd566",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dsa = DynamicSlidingWindowBL(\n",
    "#     n_assets=20,           # 10 assets\n",
    "#     initial_window=60,     # Initial window size 60\n",
    "#     eta=0.95,              # EWMA decay factor\n",
    "#     h=0.1,                 # Volatility threshold for window adjustment\n",
    "#     risk_aversion=3.0,     # Risk aversion parameter\n",
    "#     c_minus=0.9,           # Window decrease factor\n",
    "#     c_plus=1.1             # Window increase factor\n",
    "# )\n",
    "# sim_returns, sim_factors, sim_price_df, sim_cap_df = dsa.generate_random_data(\n",
    "#     n_periods=1000, n_factors=5)\n",
    "\n",
    "# print(f\"sim_returns.shape: {sim_returns.shape}\")\n",
    "# print(f\"sim_factors.shape: {sim_factors.shape}\")\n",
    "# print(f\"sim_price_df.shape: {sim_price_df.shape}\")\n",
    "# print(f\"sim_cap_df.shape: {sim_cap_df.shape}\")\n",
    "\n",
    "# dsa.run_algorithm(pd.DataFrame(sim_returns, index=sim_price_df.index, columns=sim_price_df.columns), pd.DataFrame(sim_factors),\n",
    "#                   sim_cap_df, n_iterations=30)\n",
    "\n",
    "# # Display results\n",
    "# dsa.get_results_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c5353ac7",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'create_datasets' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m n_assets \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m20\u001b[39m\n\u001b[1;32m      2\u001b[0m freq \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mD\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m----> 4\u001b[0m price_df, returns_df, cshthrm_df, conm_df \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_datasets\u001b[49m(n_assets, freq)\n\u001b[1;32m      6\u001b[0m price_df \u001b[38;5;241m=\u001b[39m price_df\u001b[38;5;241m.\u001b[39mloc[:\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m2025-07-31\u001b[39m\u001b[38;5;124m'\u001b[39m, :]\u001b[38;5;241m.\u001b[39mdropna()\n\u001b[1;32m      7\u001b[0m returns_df \u001b[38;5;241m=\u001b[39m returns_df\u001b[38;5;241m.\u001b[39mloc[:\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m2025-07-31\u001b[39m\u001b[38;5;124m'\u001b[39m, :]\u001b[38;5;241m.\u001b[39mdropna()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'create_datasets' is not defined"
     ]
    }
   ],
   "source": [
    "n_assets = 20\n",
    "freq = 'D'\n",
    "\n",
    "price_df, returns_df, cshthrm_df, conm_df = create_datasets(n_assets, freq)\n",
    "\n",
    "price_df = price_df.loc[:'2025-07-31', :].dropna()\n",
    "returns_df = returns_df.loc[:'2025-07-31', :].dropna()\n",
    "cshthrm_df = cshthrm_df.loc[:'2025-07-31', :].dropna()\n",
    "conm_df = conm_df.loc[:'2025-07-31', :].dropna()\n",
    "\n",
    "factor_df = pd.read_csv('data/factors.csv', index_col=0)\n",
    "factor_df.index = factor_df.index.astype(str)\n",
    "factor_df.index = pd.to_datetime(factor_df.index)\n",
    "factor_df = factor_df.resample(freq).last().loc[price_df.index, :].ffill()\n",
    "factor_df = factor_df.iloc[:, :-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4ee9338",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "returns_df.shape: (3990, 20)\n",
      "factor_df.shape: (3990, 5)\n",
      "cshthrm_df.shape: (3990, 20)\n",
      "returns_df.index range: 2010-01-05 to 2025-07-31\n",
      "cshthrm_df.index range: 2010-01-05 to 2025-07-31\n",
      "cshthrm_df.index length: 3990\n",
      "returns_df.index length: 3990\n"
     ]
    }
   ],
   "source": [
    "# 데이터 shape과 인덱스 확인\n",
    "print(f\"returns_df.shape: {returns_df.shape}\")\n",
    "print(f\"factor_df.shape: {factor_df.shape}\")\n",
    "print(f\"cshthrm_df.shape: {cshthrm_df.shape}\")\n",
    "print(f\"returns_df.index range: {returns_df.index[0]} to {returns_df.index[-1]}\")\n",
    "print(f\"cshthrm_df.index range: {cshthrm_df.index[0]} to {cshthrm_df.index[-1]}\")\n",
    "print(f\"cshthrm_df.index length: {len(cshthrm_df.index)}\")\n",
    "print(f\"returns_df.index length: {len(returns_df.index)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea2bbbe0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dynamic Sliding Window Algorithm Started\n",
      "Initial window size: 60\n",
      "Iteration 0: Window size = 60, Volatility = 0.1549\n",
      "Iteration 10: Window size = 80, Volatility = 0.1224\n",
      "Iteration 20: Window size = 59, Volatility = 0.2297\n",
      "Iteration 30: Window size = 52, Volatility = 1.3890\n",
      "Iteration 40: Window size = 38, Volatility = 0.5866\n",
      "Iteration 50: Window size = 34, Volatility = 14.8970\n",
      "Iteration 60: Window size = 36, Volatility = 2.3921\n",
      "Iteration 70: Window size = 39, Volatility = 3.2649\n",
      "Iteration 80: Window size = 30, Volatility = 2.0568\n",
      "Algorithm completed: 83 iterations\n",
      "\n",
      "=== Algorithm Results Summary ===\n",
      "Final window size: 29\n",
      "Average portfolio volatility: 2.8843\n",
      "Volatility std: 5.6020\n"
     ]
    }
   ],
   "source": [
    "dsa = DynamicSlidingWindowBL(\n",
    "    n_assets=n_assets,           # 10 assets\n",
    "    initial_window=60,     # Initial window size 60\n",
    "    eta=0.2,              # EWMA decay factor\n",
    "    h=0.1,                 # Volatility threshold for window adjustment\n",
    "    c_minus=0.9,           # Window decrease factor\n",
    "    c_plus=1.1,             # Window increase factor\n",
    "    risk_aversion=2.0\n",
    ")\n",
    "\n",
    "dsa.run_algorithm(returns_df, factor_df, cshthrm_df, n_iterations=140)\n",
    "\n",
    "# Display results\n",
    "dsa.get_results_summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36ac0a6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(83, 20)\n"
     ]
    }
   ],
   "source": [
    "weight_df = pd.DataFrame(dsa.portfolio_weights, index=price_df.columns).T\n",
    "equal_weight_df = pd.DataFrame(1 / len(weight_df.columns), index=weight_df.index, columns=weight_df.columns)\n",
    "print(weight_df.shape)\n",
    "returns_df_winsorized = returns_df.apply(\n",
    "    lambda x: winsorize(x, limits=[0.01, 0.01]), axis=0\n",
    ")\n",
    "\n",
    "weight_df.to_csv('data/weight_df.csv')\n",
    "returns_df.to_csv('data/returns_df.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "research",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

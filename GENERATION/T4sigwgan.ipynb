{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-13T10:39:38.954241Z",
     "start_time": "2025-05-13T10:39:37.278432Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/research/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "\n",
    "import T4sigWGAN as T4\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-13T10:39:39.388405Z",
     "start_time": "2025-05-13T10:39:39.098240Z"
    }
   },
   "outputs": [],
   "source": [
    "total_dataset = T4.StockTimeSeriesDataset(T4.args.window_size)\n",
    "train_size = int(0.9 * len(total_dataset))  # 90% for training\n",
    "val_size = len(total_dataset) - train_size  # 10% for validation\n",
    "train_dataset, val_dataset = random_split(total_dataset, [train_size, val_size])\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=T4.args.batch_size, shuffle=False, num_workers=2,\n",
    "                              drop_last=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=T4.args.batch_size, shuffle=False, num_workers=2, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-13T10:39:39.610171Z",
     "start_time": "2025-05-13T10:39:39.404166Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Parameters: 22890978\n"
     ]
    }
   ],
   "source": [
    "Encoder = T4.LogSigRNNEncoder(**T4.encoder_config).to(T4.args.device)\n",
    "Decoder = T4.TimesFormerDecoder(**T4.decoder_config).to(T4.args.device)\n",
    "Supervisor = T4.ModernTCN(T4.supervisor_config).to(T4.args.device)\n",
    "Generator = T4.LogSigRNNGenerator(**T4.logsig_config).to(T4.args.device)\n",
    "Discriminator = T4.tailGANDiscriminator(T4.discriminator_config).to(T4.args.device)\n",
    "model = T4.T4sigWGAN(Encoder, Decoder, Generator, Supervisor, Discriminator, T4.args.batch_size).to(T4.args.device)\n",
    "trainer = T4.FinetuneTrainer(T4.args, model, train_dataloader, val_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2025-05-13T10:39:39.626267Z"
    },
    "jupyter": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|â–ˆ         | 1/10 [00:09<01:24,  9.40s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# model training\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28mrange\u001b[39m(T4\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mepochs)):\n\u001b[0;32m----> 5\u001b[0m     \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m     val_loss \u001b[38;5;241m=\u001b[39m trainer\u001b[38;5;241m.\u001b[39mvalid(epoch, stage)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# train log image save\u001b[39;00m\n",
      "File \u001b[0;32m/home/GitHub/Study/Study_DL_and_Coding/GENERATION/T4sigWGAN/trainers.py:84\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, epoch, stage)\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mtrain\u001b[39m(\u001b[38;5;28mself\u001b[39m, epoch, stage):\n\u001b[0;32m---> 84\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miteration\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/home/GitHub/Study/Study_DL_and_Coding/GENERATION/T4sigWGAN/trainers.py:185\u001b[0m, in \u001b[0;36mFinetuneTrainer.iteration\u001b[0;34m(self, epoch, stage, dataloader, mode)\u001b[0m\n\u001b[1;32m    183\u001b[0m \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m batch\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m    184\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m stage \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPretrain_1\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 185\u001b[0m     x_tilde, loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    186\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptim_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mER\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m    187\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m/opt/miniconda3/envs/research/lib/python3.9/site-packages/torch/nn/modules/module.py:1051\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1047\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1048\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1049\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1052\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1053\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/home/GitHub/Study/Study_DL_and_Coding/GENERATION/T4sigWGAN/models/models.py:18\u001b[0m, in \u001b[0;36mT4sigWGAN.forward\u001b[0;34m(self, stage, x)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m x \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m stage \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPretrain_1\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m---> 18\u001b[0m         h \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m         x_tilde \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrecovery(h)\n\u001b[1;32m     20\u001b[0m         loss \u001b[38;5;241m=\u001b[39m rmse(x, x_tilde)\n",
      "File \u001b[0;32m/opt/miniconda3/envs/research/lib/python3.9/site-packages/torch/nn/modules/module.py:1051\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1047\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1048\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1049\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1052\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1053\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/home/GitHub/Study/Study_DL_and_Coding/GENERATION/T4sigWGAN/models/sigwgan.py:134\u001b[0m, in \u001b[0;36mLogSigRNNEncoder.forward\u001b[0;34m(self, real_data)\u001b[0m\n\u001b[1;32m    132\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m idx, (t, y_logsig_) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mzip\u001b[39m(time_t, y_logsig)):\n\u001b[1;32m    133\u001b[0m     h \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrnn(torch\u001b[38;5;241m.\u001b[39mcat([last_h, y_logsig_], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))\n\u001b[0;32m--> 134\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m t \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m u_logsigrnn[\u001b[38;5;241m0\u001b[39m]:\n\u001b[1;32m    135\u001b[0m         \u001b[38;5;28;01mdel\u001b[39;00m u_logsigrnn[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    136\u001b[0m         last_h \u001b[38;5;241m=\u001b[39m h\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "stage = \"Pretrain_1\"\n",
    "\n",
    "# model training\n",
    "for epoch in tqdm(range(T4.args.epochs)):\n",
    "    trainer.train(epoch, stage)\n",
    "    val_loss = trainer.valid(epoch, stage)\n",
    "\n",
    "# train log image save\n",
    "trainer.evaluate('RMSE_loss_for_ER')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save(stage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stage = \"Pretrain_2\"\n",
    "\n",
    "# model training\n",
    "for epoch in tqdm(range(T4.args.epochs)):\n",
    "    trainer.train(epoch, stage)\n",
    "    val_loss = trainer.valid(epoch, stage)\n",
    "\n",
    "# train log image save\n",
    "trainer.evaluate('SigW1_supervisor_loss_for_S')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save(stage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stage = \"Finetune\"\n",
    "\n",
    "# model training\n",
    "for epoch in tqdm(range(T4.args.epochs)):\n",
    "    trainer.train(epoch, stage)\n",
    "    val_loss = trainer.valid(epoch, stage)\n",
    "\n",
    "# train log image save\n",
    "trainer.evaluate('SigW1_supervisor_loss + RMSE_for_SR')\n",
    "trainer.evaluate_tail('loss_D and loss_G + SigW1_generator_loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save(stage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_fake = model(600)\n",
    "stacked = torch.stack([total_dataset[i] for i in range(600)])\n",
    "x_real = stacked\n",
    "T4.plot_summary(x_fake=x_fake.detach(), x_real=x_real.detach(), trainer=\"T4sigWGAN\", G=\"LogSigRNN\")\n",
    "plt.savefig('./result/T4sigWGAN.png')\n",
    "plt.close()\n",
    "\n",
    "plt.figure(figsize=(12, 1.5), dpi=400)\n",
    "plt.plot(x_fake.detach().cpu().numpy()[1][:, 1], label=\"Fake\")\n",
    "plt.plot(total_dataset[1].detach().cpu().numpy()[:, 1], label=\"Real\")\n",
    "plt.legend(loc=\"upper right\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# metric_iteration = 5\n",
    "#\n",
    "# discriminative_score = list()\n",
    "# for _ in range(metric_iteration):\n",
    "#     temp_disc = T4.discriminative_score_metrics(ori_data, generated_data)\n",
    "#     discriminative_score.append(temp_disc)\n",
    "#\n",
    "# print('Discriminative score: ' + str(np.round(np.mean(discriminative_score), 4)))\n",
    "#\n",
    "#\n",
    "# predictive_score = list()\n",
    "# for tt in range(metric_iteration):\n",
    "#     temp_pred = T4.predictive_score_metrics(ori_data, generated_data)\n",
    "#     predictive_score.append(temp_pred)\n",
    "#\n",
    "# print('Predictive score: ' + str(np.round(np.mean(predictive_score), 4)))\n",
    "#\n",
    "#\n",
    "# T4.visualization(ori_data, generated_data, 'pca')\n",
    "# T4.visualization(ori_data, generated_data, 'tsne')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "research",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "V28"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3 (ipykernel)",
   "language": "python"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "TPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# 모듈 설치 및 설정"
   ],
   "metadata": {
    "id": "BObOC16LAlS4"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import gc\n",
    "\n",
    "import torch\n",
    "import optuna\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from os import path as pt\n",
    "from lib.utils import pickle_it\n",
    "from sklearn.metrics import f1_score\n",
    "from torch_geometric.nn import GATv2Conv\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "gc.collect()"
   ],
   "metadata": {
    "id": "XgRsALeSVcJX"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "# GAT"
   ],
   "metadata": {
    "id": "XyHQgQwjAiTk"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "- 노드 : 각 시점\n",
    "- 피처 : 10개 주식 종가 + 예측 기업 거래량 + 예측 기업 감정분석\n",
    "- GAT의 역할 : 피처들의 관계(회사 간의 관계 등)을 파악해 **시점별** 임베딩 생성;  다른 시점과의 연관성을 반영    \n",
    "(ex. 1~10일 전과 연결이 되어있는 상태에서, 1일 전 정보는 얼마나 중요하고 10일 전 정보는 얼마나 중요한지 판단)"
   ],
   "metadata": {
    "id": "m6DEYVqJC0_l"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "'''\n",
    "class GAT(nn.Module):\n",
    "    def __init__(self, in_features, out_features, heads, dropout, concat=False):\n",
    "        super(GAT, self).__init__()\n",
    "\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.heads = heads\n",
    "        self.dropout = dropout\n",
    "        self.concat = concat\n",
    "\n",
    "        # 선형 변환을 위한 가중치 행렬\n",
    "        self.lin = nn.Linear(in_features, heads * out_features, bias=False)\n",
    "\n",
    "        # 어텐션 계수 계산을 위한 가중치\n",
    "        self.att = nn.Parameter(torch.Tensor(1, heads, out_features))\n",
    "\n",
    "        # 바이어스\n",
    "        self.bias = nn.Parameter(torch.Tensor(out_features))\n",
    "\n",
    "        # LeakyReLU\n",
    "        self.leakyrelu = nn.LeakyReLU(0.2)\n",
    "\n",
    "        # 드롭아웃\n",
    "        self.dropout_layer = nn.Dropout(dropout)\n",
    "\n",
    "        # 초기화\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        # 가중치 초기화\n",
    "        gain = nn.init.calculate_gain('relu')\n",
    "        nn.init.xavier_normal_(self.lin.weight, gain=gain)\n",
    "        nn.init.xavier_normal_(self.att, gain=gain)\n",
    "        nn.init.zeros_(self.bias)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        \"\"\"\n",
    "        x: 노드 특성 [N, in_features]\n",
    "        edge_index: 엣지 인덱스 [2, E]\n",
    "        \"\"\"\n",
    "        N = x.size(0)  # 노드 수\n",
    "\n",
    "        # 1. 선형 변환\n",
    "        x = self.dropout_layer(x)\n",
    "        x = self.lin(x)  # [N, heads * out_features]\n",
    "        x = x.view(N, self.heads, self.out_features)  # [N, heads, out_features]\n",
    "\n",
    "        # 2. 엣지 리스트로부터 어텐션 계산\n",
    "        edge_src, edge_dst = edge_index[0], edge_index[1]\n",
    "\n",
    "        # 3. GATv2 : 선형변환 후 어텐션\n",
    "        # 각 노드에 어텐션 가중치 적용\n",
    "        x_att = x * self.att  # [N, heads, out_features]\n",
    "\n",
    "        # 이웃 노드 쌍의 어텐션 점수 계산\n",
    "        alpha_src = x_att[edge_src].sum(dim=-1)  # [E, heads]\n",
    "        alpha_dst = x_att[edge_dst].sum(dim=-1)  # [E, heads]\n",
    "        alpha = alpha_src + alpha_dst  # [E, heads]\n",
    "        alpha = self.leakyrelu(alpha)  # [E, heads]\n",
    "\n",
    "        # 4. 소프트맥스로 정규화\n",
    "        alpha = self._edge_softmax(alpha, edge_index[1], N)\n",
    "        alpha = self.dropout_layer(alpha)\n",
    "\n",
    "        # 5. 메시지 집계\n",
    "        out = torch.zeros(N, self.heads, self.out_features, device=x.device)\n",
    "\n",
    "        # 각 엣지에 대해 메시지 전달\n",
    "        for i in range(edge_index.size(1)):\n",
    "            src, dst = edge_index[0, i], edge_index[1, i]\n",
    "            out[dst] += alpha[i].unsqueeze(-1) * x[src]\n",
    "\n",
    "        # 6. 최종 출력 형태 결정\n",
    "        if self.concat:\n",
    "            out = out.view(N, self.heads * self.out_features)\n",
    "        else:\n",
    "            out = out.mean(dim=1)  # 헤드 간 평균\n",
    "\n",
    "        # 7. 바이어스 추가\n",
    "        out = out + self.bias\n",
    "\n",
    "        return out\n",
    "\n",
    "    def _edge_softmax(self, alpha, target_nodes, num_nodes):\n",
    "\n",
    "        norm_alpha = torch.zeros_like(alpha)\n",
    "\n",
    "\n",
    "        for i in range(num_nodes):\n",
    "            # 현재 노드로 향하는 엣지 마스크\n",
    "            mask = (target_nodes == i)\n",
    "            if mask.sum() > 0:\n",
    "                # 해당 노드로 향하는 엣지에 대해 소프트맥스 적용\n",
    "                norm_alpha[mask] = F.softmax(alpha[mask], dim=0)\n",
    "\n",
    "        return norm_alpha\n",
    " '''"
   ],
   "metadata": {
    "id": "GegpQ78WROjC",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 91
    },
    "outputId": "7cc16100-9a36-400c-caf2-5b009a27f982"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# GAT 레이어 정의\n",
    "\n",
    "class GATLayer(nn.Module):\n",
    "    def __init__(self, in_features, out_features, heads, dropout):\n",
    "        super().__init__()\n",
    "        self.gat = GATv2Conv(in_features, out_features, heads=heads, dropout=dropout, concat=False)  # PyG 사용\n",
    "        # self.gat = GAT(in_features, out_features, heads=heads, dropout=dropout, concat=False)            # 상단 구현 코드 (느리고, 느려서 optuna 중간에 끊고 첫번째걸로 해봤는데 ----로 예측됨;;;;)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        return self.gat(x, edge_index)"
   ],
   "metadata": {
    "id": "ZRWMJA0G_U2x"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "# TCN"
   ],
   "metadata": {
    "id": "2_Ee7Aj6A3K_"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# --- 유틸 함수 ---\n",
    "def get_conv1d(in_channels, out_channels, kernel_size, stride, padding, dilation, groups, bias):\n",
    "    return nn.Conv1d(in_channels=in_channels, out_channels=out_channels,\n",
    "                     kernel_size=kernel_size, stride=stride,\n",
    "                     padding=padding, dilation=dilation,\n",
    "                     groups=groups, bias=bias)\n",
    "\n",
    "\n",
    "def get_bn(channels):\n",
    "    return nn.BatchNorm1d(channels)\n",
    "\n",
    "\n",
    "def conv_bn(in_channels, out_channels, kernel_size, stride, padding, groups, dilation=1, bias=False):\n",
    "    if padding is None:\n",
    "        padding = kernel_size // 2\n",
    "    result = nn.Sequential()\n",
    "    result.add_module('conv',\n",
    "                      get_conv1d(in_channels, out_channels, kernel_size, stride, padding, dilation, groups, bias))\n",
    "    result.add_module('bn', get_bn(out_channels))\n",
    "    return result\n",
    "\n",
    "\n",
    "# --- RevIN ---\n",
    "class RevIN(nn.Module):\n",
    "    def __init__(self, num_features: int, eps=1e-5, affine=True, subtract_last=False):\n",
    "        super().__init__()\n",
    "        self.num_features = num_features\n",
    "        self.eps = eps\n",
    "        self.affine = affine\n",
    "        self.subtract_last = subtract_last\n",
    "        if self.affine:\n",
    "            self._init_params()\n",
    "\n",
    "    def _init_params(self):\n",
    "        self.affine_weight = nn.Parameter(torch.ones(self.num_features))\n",
    "        self.affine_bias = nn.Parameter(torch.zeros(self.num_features))\n",
    "\n",
    "    def forward(self, x, mode: str):\n",
    "        if mode == 'norm':\n",
    "            self._get_statistics(x)\n",
    "            x = self._normalize(x)\n",
    "        elif mode == 'denorm':\n",
    "            x = self._denormalize(x)\n",
    "        return x\n",
    "\n",
    "    def _get_statistics(self, x):\n",
    "        dim2reduce = tuple(range(1, x.ndim - 1))\n",
    "        if self.subtract_last:\n",
    "            self.last = x[:, -1:, :].unsqueeze(1)\n",
    "        else:\n",
    "            self.mean = torch.mean(x, dim=dim2reduce, keepdim=True).detach()\n",
    "        self.stdev = torch.sqrt(torch.var(x, dim=dim2reduce, keepdim=True, unbiased=False) + self.eps).detach()\n",
    "\n",
    "    def _normalize(self, x):\n",
    "        x = (x - self.mean) / self.stdev\n",
    "        if self.affine:\n",
    "            x = x * self.affine_weight[None, None, :] + self.affine_bias[None, None, :]\n",
    "        return x\n",
    "\n",
    "    def _denormalize(self, x):\n",
    "        if self.affine:\n",
    "            x = (x - self.affine_bias[None, None, :]) / self.affine_weight[None, None, :]\n",
    "        x = x * self.stdev + self.mean\n",
    "        return x\n",
    "\n",
    "\n",
    "# --- 시계열 분해 ---\n",
    "class moving_avg(nn.Module):\n",
    "    def __init__(self, kernel_size, stride):\n",
    "        super().__init__()\n",
    "        self.kernel_size = kernel_size\n",
    "        self.avg = nn.AvgPool1d(kernel_size=kernel_size, stride=stride, padding=0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        front = x[:, 0:1, :].repeat(1, (self.kernel_size - 1) // 2, 1)\n",
    "        end = x[:, -1:, :].repeat(1, (self.kernel_size - 1) // 2, 1)\n",
    "        x = torch.cat([front, x, end], dim=1)\n",
    "        x = self.avg(x.permute(0, 2, 1))\n",
    "        return x.permute(0, 2, 1)\n",
    "\n",
    "\n",
    "class series_decomp(nn.Module):\n",
    "    def __init__(self, kernel_size):\n",
    "        super().__init__()\n",
    "        self.moving_avg = moving_avg(kernel_size, stride=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        moving_mean = self.moving_avg(x)\n",
    "        return x - moving_mean, moving_mean\n",
    "\n",
    "\n",
    "# --- 커스텀 커널 ---\n",
    "class ReparamLargeKernelConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride, groups, small_kernel, small_kernel_merged=False):\n",
    "        super().__init__()\n",
    "        self.kernel_size = kernel_size\n",
    "        self.small_kernel = small_kernel\n",
    "        padding = kernel_size // 2\n",
    "        if small_kernel_merged:\n",
    "            self.lkb_reparam = nn.Conv1d(in_channels, out_channels, kernel_size, stride, padding, groups=groups,\n",
    "                                         bias=True)\n",
    "        else:\n",
    "            self.lkb_origin = conv_bn(in_channels, out_channels, kernel_size, stride, padding, groups)\n",
    "            if small_kernel is not None:\n",
    "                self.small_conv = conv_bn(in_channels, out_channels, small_kernel, stride, small_kernel // 2, groups)\n",
    "\n",
    "    def forward(self, x):\n",
    "        if hasattr(self, 'lkb_reparam'):\n",
    "            return self.lkb_reparam(x)\n",
    "        out = self.lkb_origin(x)\n",
    "        if hasattr(self, 'small_conv'):\n",
    "            out += self.small_conv(x)\n",
    "        return out\n",
    "\n",
    "\n",
    "# --- 출력층 ---\n",
    "class Flatten_Head(nn.Module):\n",
    "    def __init__(self, d_model):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(d_model, 1)\n",
    "\n",
    "    def forward(self, x):  # x: [B, C, T]\n",
    "        x = x.permute(0, 2, 1)  # → [B, T, C]\n",
    "        x = self.linear(x)  # → [B, T, 1]\n",
    "        return x.squeeze(-1)  # → [B, T]"
   ],
   "metadata": {
    "id": "Ky1M6aXiA47i"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# --- ModernTCN 모델 ---\n",
    "class ModernTCN(nn.Module):\n",
    "    def __init__(self, configs):\n",
    "        super().__init__()\n",
    "        self.revin = RevIN(configs.enc_in, affine=configs.affine) if configs.revin else None\n",
    "        self.decomp = series_decomp(configs.kernel_size) if configs.decomposition else None\n",
    "\n",
    "        self.conv_layers = nn.ModuleList()\n",
    "        self.norm_layers = nn.ModuleList()\n",
    "\n",
    "        c_in = configs.enc_in\n",
    "        for i in range(len(configs.dims)):\n",
    "            conv = ReparamLargeKernelConv(c_in, configs.dims[i],\n",
    "                                          kernel_size=configs.large_size[i],\n",
    "                                          stride=1,\n",
    "                                          groups=1,\n",
    "                                          small_kernel=configs.small_size[i],\n",
    "                                          small_kernel_merged=configs.small_kernel_merged)\n",
    "            self.conv_layers.append(conv)\n",
    "            self.norm_layers.append(nn.BatchNorm1d(configs.dims[i]))\n",
    "            c_in = configs.dims[i]\n",
    "\n",
    "        self.head = Flatten_Head(configs.dims[-1])\n",
    "\n",
    "    def forward(self, x):  # x: [B, T, C]\n",
    "        if self.revin:\n",
    "            x = self.revin(x, 'norm')\n",
    "        if self.decomp:\n",
    "            x, _ = self.decomp(x)\n",
    "        x = x.permute(0, 2, 1)  # [B, C, T]\n",
    "        for conv, norm in zip(self.conv_layers, self.norm_layers):\n",
    "            x = conv(x)\n",
    "            x = norm(x)\n",
    "            x = F.relu(x)\n",
    "        out = self.head(x)  # [B, T]\n",
    "        return out"
   ],
   "metadata": {
    "id": "eySi1894CGFo"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# --- Config 클래스 ---\n",
    "class Configs:\n",
    "    def __init__(self, enc_in):\n",
    "        self.enc_in = enc_in\n",
    "        self.dims = [8, 16, 32]\n",
    "        self.large_size = [5, 5, 3]\n",
    "        self.small_size = [5, 3, 3]\n",
    "        self.small_kernel_merged = False\n",
    "        self.dropout = 0.1\n",
    "        self.head_dropout = 0.2\n",
    "        self.revin = True\n",
    "        self.affine = True\n",
    "        self.decomposition = True\n",
    "        self.kernel_size = 25"
   ],
   "metadata": {
    "id": "Bqr3BMHdCEI6"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "# GAT-TCN"
   ],
   "metadata": {
    "id": "hC824c4hBPKK"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "class GATCNModel(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.gat = GATLayer(node_features.shape[1], config.gat_out_features,\n",
    "                            config.gat_heads, config.gat_dropout)  # GAT\n",
    "        self.tcn = ModernTCN(config)  # TCN\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        embeddings = self.gat(x, edge_index)  # GAT를 통한 임베딩 생성\n",
    "        tcn_input = embeddings.unsqueeze(0)  # TCN 입력 형태로 변환\n",
    "        output = self.tcn(tcn_input)  # TCN으로 예측\n",
    "        return output"
   ],
   "metadata": {
    "id": "nu0GYmnkBOtr"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "\n",
    "# GAT-TCN 모델의 최적 파라미터 탐색\n",
    "\n",
    "def train_model(model, node_features, edge_index, X_train, y_train, X_val, y_val, epochs=30, lr=1e-3, pos_weight=None):\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    #criterion = nn.BCEWithLogitsLoss()\n",
    "    criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight) if pos_weight is not None else nn.BCEWithLogitsLoss()\n",
    "    train_losses, val_losses, val_accs = [], [], []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        # 1.학습 단계\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        train_output = model(node_features, edge_index).squeeze(0)[:len(y_train)]  # 통합 모델에 데이터를 적용한 결과\n",
    "        loss = criterion(train_output, y_train.float())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_losses.append(loss.item())\n",
    "\n",
    "        # 2. 검증 단계\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_output = model(node_features, edge_index).squeeze(0)[-len(y_val):]\n",
    "            val_loss = criterion(val_output, y_val.float()).item()\n",
    "            pred = (torch.sigmoid(val_output) > 0.5).int()\n",
    "            acc = (pred == y_val).float().mean().item()\n",
    "\n",
    "        val_losses.append(val_loss)\n",
    "        val_accs.append(acc)\n",
    "\n",
    "        print(f\"[{epoch + 1}/{epochs}] Train Loss: {loss.item():.4f}, Val Loss: {val_loss:.4f}, Val Acc: {acc:.4f}\")\n",
    "\n",
    "    return train_losses, val_losses, val_accs\n"
   ],
   "metadata": {
    "id": "F5pcxCD-EjI4"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "def objective(trial):\n",
    "    # GAT\n",
    "    gat_out_features = trial.suggest_categorical(\"gat_out_features\", [4, 8, 12])  # 기존 features 수보다는 적은 것이 적합\n",
    "    gat_heads = trial.suggest_categorical(\"gat_heads\", [1, 2, 4, 8])\n",
    "    gat_dropout = trial.suggest_float(\"gat_dropout\", 0.0, 0.3)\n",
    "\n",
    "    # TCN\n",
    "    dims = [\n",
    "        trial.suggest_categorical(\"dim1\", [8, 16, 32, 64]),\n",
    "        trial.suggest_categorical(\"dim2\", [16, 32, 64, 128]),\n",
    "        trial.suggest_categorical(\"dim3\", [32, 64, 128, 256])\n",
    "    ]\n",
    "    large_size = [\n",
    "        trial.suggest_categorical(\"k1\", [3, 5, 7, 9, 11]),\n",
    "        trial.suggest_categorical(\"k2\", [3, 5, 7, 9]),\n",
    "        trial.suggest_categorical(\"k3\", [3, 5, 7])\n",
    "    ]\n",
    "    small_size = [\n",
    "        trial.suggest_categorical(\"s1\", [1, 3, 5]),\n",
    "        trial.suggest_categorical(\"s2\", [1, 3]),\n",
    "        trial.suggest_categorical(\"s3\", [1, 3])\n",
    "    ]\n",
    "    dropout = trial.suggest_float(\"dropout\", 0.0, 0.3)\n",
    "    head_dropout = trial.suggest_float(\"head_dropout\", 0.0, 0.3)\n",
    "    kernel_size = trial.suggest_categorical(\"kernel_size\", [5, 11, 15, 25, 31])\n",
    "    decomposition = trial.suggest_categorical(\"decomposition\", [True, False])\n",
    "    revin = trial.suggest_categorical(\"revin\", [True, False])\n",
    "    affine = trial.suggest_categorical(\"affine\", [True, False])\n",
    "\n",
    "    # 통합\n",
    "    class TrialConfig:\n",
    "        def __init__(self):\n",
    "            self.gat_out_features = gat_out_features\n",
    "            self.gat_heads = gat_heads\n",
    "            self.gat_dropout = gat_dropout\n",
    "\n",
    "            self.enc_in = gat_out_features\n",
    "            self.dims = dims\n",
    "            self.large_size = large_size\n",
    "            self.small_size = small_size\n",
    "            self.small_kernel_merged = False\n",
    "            self.dropout = dropout\n",
    "            self.head_dropout = head_dropout\n",
    "            self.revin = revin\n",
    "            self.affine = affine\n",
    "            self.decomposition = decomposition\n",
    "            self.kernel_size = kernel_size\n",
    "\n",
    "    model = GATCNModel(TrialConfig())\n",
    "\n",
    "    # Accuracy 기준 최적화 #####\n",
    "    _, _, val_accs = train_model(model, node_features, edge_index, X_train, y_train, X_val, y_val, epochs=50)\n",
    "    return max(val_accs)\n",
    "    ############################\n",
    "\n",
    "    # F1 Score 기준 최적화 #####################\n",
    "    # pos_weight 계산 (불균형 데이터 보정)\n",
    "    '''\n",
    "    pos_weight = torch.tensor([(y_train == 0).sum() / (y_train == 1).sum()]).to(y_train.device)\n",
    "\n",
    "    train_model(model, node_features, edge_index, X_train, y_train, X_val, y_val, epochs=15, pos_weight=pos_weight)\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        full_pred = model(node_features, edge_index).squeeze(0)\n",
    "        pred = full_pred[-len(y_val):]\n",
    "        probs = torch.sigmoid(pred).cpu().numpy()\n",
    "        preds = (probs > 0.5).astype(int)\n",
    "\n",
    "    y_true = y_val.cpu().numpy()\n",
    "    return f1_score(y_true, preds)\n",
    "    '''\n",
    "    ##########################################"
   ],
   "metadata": {
    "id": "Ri6Us2oFMJlq"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 시각화 코드"
   ],
   "metadata": {
    "id": "e6IIhl_Vo5x9"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def visualize_training(company_name, train_losses, val_losses, val_accs):\n",
    "    plt.figure(figsize=(12, 2), dpi=400)\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(train_losses, label='Train Loss')\n",
    "    plt.plot(val_losses, label='Val Loss')\n",
    "    plt.legend()\n",
    "    plt.title(\"Loss over Epochs\")\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(val_accs, label='Val Accuracy')\n",
    "    plt.legend()\n",
    "    plt.title(\"Validation Accuracy\")\n",
    "    plt.savefig(f'data/images/{company_name}_loss.png')\n",
    "    plt.close()\n",
    "\n",
    "from matplotlib.patches import Patch\n",
    "\n",
    "def visualize_match_only(company_name, pred_probs, true_labels, threshold=0.5):\n",
    "    # 이진 예측\n",
    "    pred_labels = (pred_probs >= threshold).astype(int)\n",
    "\n",
    "    # 정답과 예측이 일치하면 1, 다르면 0\n",
    "    match = (pred_labels == true_labels).astype(int)\n",
    "\n",
    "    # 시각화\n",
    "    plt.figure(figsize=(12, 2), dpi=400)\n",
    "    bar_heights = np.ones_like(match)\n",
    "    bar_colors = ['green' if m else 'red' for m in match]\n",
    "    plt.bar(np.arange(len(match)), bar_heights,\n",
    "            color=bar_colors,\n",
    "            width=1.0)\n",
    "\n",
    "    plt.title(f\"{company_name} - Prediction Match\")\n",
    "    plt.ylabel('Match')\n",
    "    plt.xlabel('Time')\n",
    "    plt.yticks([0, 1], ['Wrong', 'Correct'])\n",
    "\n",
    "    # ✅ 범례 추가\n",
    "    legend_elements = [\n",
    "        Patch(facecolor='green', label='Correct'),\n",
    "        Patch(facecolor='red', label='Wrong')\n",
    "    ]\n",
    "    plt.legend(handles=legend_elements, loc='upper right')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'data/images/{company_name}_match_only.png')\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def visualize_cumulative_return(pred_probs, true_labels, prices):\n",
    "    signal = (pred_probs > 0.5).astype(int)\n",
    "    returns = (prices[1:] / prices[:-1]) - 1\n",
    "    strategy_returns = returns * signal[:-1]\n",
    "\n",
    "    cumulative = (strategy_returns + 1).cumprod()\n",
    "    market = (returns + 1).cumprod()\n",
    "\n",
    "    plt.plot(cumulative, label='Strategy')\n",
    "    plt.plot(market, label='Market (buy & hold)')\n",
    "    plt.legend();\n",
    "    plt.title(\"Cumulative Return\")\n",
    "    plt.show()"
   ],
   "metadata": {
    "id": "nj38FLOMtL84"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "# GAT-TCN 적용"
   ],
   "metadata": {
    "id": "2gL9v1aBFqxZ"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# seed 설정\n",
    "import random\n",
    "\n",
    "\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.use_deterministic_algorithms(True)\n",
    "\n",
    "\n",
    "set_seed(42)\n"
   ],
   "metadata": {
    "id": "YmnyK8iVm3nZ"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "1. 데이터셋 불러오기 및 라벨 생성"
   ],
   "metadata": {
    "id": "ZkK07jzudiAx"
   }
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "df_ = pd.read_csv(\"./data/daily_all.csv\")",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "# company_name =['TSLA', 'NVDA', 'MSFT', 'GOOG', 'AAPL', 'DIS', 'XOM', 'CRM', 'INTC', 'AMZN'][0]",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "for company_name in ['TSLA', 'NVDA', 'MSFT', 'GOOG', 'AAPL', 'DIS', 'XOM', 'CRM', 'INTC', 'AMZN']:\n",
    "    # company_name = \"AMZN\"  # 예측할 회사 선택\n",
    "    data = df_[\n",
    "        [f'prccd_{company}' for company in\n",
    "         ['TSLA', 'NVDA', 'MSFT', 'GOOG', 'AAPL', 'DIS', 'XOM', 'CRM', 'INTC', 'AMZN']] +\n",
    "        [f'cshtrd_{company_name}', f'sent_{company_name}', 'datadate']].copy()\n",
    "    data.set_index('datadate', inplace=True)\n",
    "    data.fillna(0, inplace=True)  # 감정분석 결측값을 0으로\n",
    "    data.iloc[:, :10] = data.iloc[:, :10].pct_change()\n",
    "            # returns = (close_prices[1:] / close_prices[:-1]) - 1\n",
    "        # labels = np.where(returns > 0.003, 1, 0)\n",
    "    data = data.dropna()\n",
    "    data_values = data.values\n",
    "    scaler = StandardScaler()\n",
    "    data_s = scaler.fit_transform(data_values[:, :-1])\n",
    "    df_preprocessed = np.hstack([data_s, data_values[:, -1].reshape(-1, 1)])\n",
    "    node_features = df_preprocessed\n",
    "    n_nodes = node_features.shape[0]  # 노드 수 (==날짜 수)\n",
    "\n",
    "    # 그래프 형태로 변환 : 엣지 생성 (시점 간 연결)\n",
    "    edge_list = []\n",
    "    for i in range(n_nodes):\n",
    "        for j in [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]:  # 이후 시점들에 단방향 연결; (휴장 같은 것은 생각하지 않음.... 시점 기준)\n",
    "            if i + j < n_nodes:\n",
    "                edge_list.append([i, i + j])\n",
    "    edge_index = torch.tensor(edge_list).t()\n",
    "\n",
    "    # edge_list = []\n",
    "    # for i in range(n_nodes):\n",
    "    #     for j in range(1, 11):  # 과거 1~10시점\n",
    "    #         if i - j >= 0:\n",
    "    #             edge_list.append([i, i - j])  # 현재 → 과거 방향\n",
    "    # edge_index = torch.tensor(edge_list).t()\n",
    "\n",
    "    # 라벨 생성\n",
    "    close_prices = data[f'prccd_{company_name}'].values\n",
    "    # returns = (close_prices[1:] / close_prices[:-1]) - 1\n",
    "    labels = np.where(close_prices > 0.003, 1, 0)  # 0.3% 초과만 1로\n",
    "    labels = torch.tensor(labels, dtype=torch.float32)\n",
    "    node_features = torch.tensor(node_features, dtype=torch.float)\n",
    "\n",
    "    X = node_features[:-1]\n",
    "    y = labels[1:]\n",
    "    split = int(len(X) * 0.8)\n",
    "\n",
    "    X_train = X[:split]\n",
    "    X_val   = X[split:]\n",
    "\n",
    "    y_train = y[:split]\n",
    "    y_val   = y[split:]\n",
    "    # Optuna 튜닝 실행\n",
    "    study = optuna.create_study(direction=\"maximize\")\n",
    "    study.optimize(objective, n_trials=20)\n",
    "    # 최적 하이퍼파라미터 출력\n",
    "    print(\"✅ Best Trial:\")\n",
    "    print(study.best_trial.params)\n",
    "    best_params = study.best_trial.params\n",
    "\n",
    "\n",
    "    # BestConfig를 이용해 모델 설정 후 학습 : optuna를 통해 정해진 최적 하이퍼파라미터\n",
    "\n",
    "    class BestConfig:\n",
    "        def __init__(self):\n",
    "            # GAT 설정\n",
    "            self.gat_out_features = best_params['gat_out_features']\n",
    "            self.gat_heads = best_params['gat_heads']\n",
    "            self.gat_dropout = best_params['gat_dropout']\n",
    "\n",
    "            # TCN 설정\n",
    "            self.enc_in = best_params['gat_out_features']  # GAT 출력 = TCN 입력\n",
    "            self.dims = [best_params['dim1'], best_params['dim2'], best_params['dim3']]\n",
    "            self.large_size = [best_params['k1'], best_params['k2'], best_params['k3']]\n",
    "            self.small_size = [best_params['s1'], best_params['s2'], best_params['s3']]\n",
    "            self.small_kernel_merged = False\n",
    "            self.dropout = best_params['dropout']\n",
    "            self.head_dropout = best_params['head_dropout']\n",
    "            self.revin = best_params['revin']\n",
    "            self.affine = best_params['affine']\n",
    "            self.decomposition = best_params['decomposition']\n",
    "            self.kernel_size = best_params['kernel_size']\n",
    "\n",
    "\n",
    "    model = GATCNModel(BestConfig())\n",
    "    train_losses, val_losses, val_accs = train_model(model, node_features, edge_index, X_train, y_train, X_val, y_val,\n",
    "                                                     epochs=75, lr=1e-3)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        pred_logits = model(node_features, edge_index).squeeze(0)  # [1, T] → [T]\n",
    "        split = len(y_train)\n",
    "        pred_logits = pred_logits[split:split + len(y_val)]\n",
    "        pred_probs = torch.sigmoid(pred_logits).cpu().numpy()\n",
    "        pred_labels = (pred_probs > 0.5).astype(int)\n",
    "\n",
    "    print(company_name)\n",
    "    visualize_training(company_name, train_losses, val_losses, val_accs)\n",
    "    visualize_match_only(company_name, pred_probs, y_val.cpu().numpy())\n",
    "\n",
    "    df = pd.DataFrame(np.vstack([y_val.cpu().numpy(), pred_labels])).T\n",
    "    df.columns = ['y_true', 'y_pred']  # 열 이름 지정\n",
    "\n",
    "    f1 = f1_score(df['y_true'], df['y_pred'], average='macro')  # or 'micro', 'weighted'\n",
    "    print(f\"F1 Score: {f1:.4f}\")\n",
    "    df.to_csv(f'data/{company_name}.csv')\n",
    "    pickle_it(model.to('cpu').state_dict(), pt.join('general_results', f'weights_{company_name}.torch'))"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 455
    },
    "collapsed": true,
    "id": "vTpEBkXIHHmR",
    "outputId": "12720ec1-bc4f-4ddb-afbe-aa46ce4821fb"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "3. 모델 생성 및 학습"
   ],
   "metadata": {
    "id": "XuoMYCoddnYg"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# BestConfig를 이용해 모델 설정 후 학습 :  여러 시행, 다양한 회사에서의 bestconfig 정보를 종합해 최적 파라미터 직접 설정 (아래는 예시)\n",
    "\n",
    "'''\n",
    "class BestConfig:\n",
    "    def __init__(self):\n",
    "        # GAT 설정\n",
    "        self.gat_out_features = 8\n",
    "        self.gat_heads = 4\n",
    "        self.gat_dropout = 0.2\n",
    "\n",
    "        # TCN 설정\n",
    "        self.enc_in = 8\n",
    "        self.dims = [8,16,32]\n",
    "        self.large_size = [7,5,7]\n",
    "        self.small_size = [5,1,1]\n",
    "        self.small_kernel_merged = False\n",
    "        self.dropout = 0.2\n",
    "        self.head_dropout = 0.25\n",
    "        self.revin = True\n",
    "        self.affine = False\n",
    "        self.decomposition = False\n",
    "        self.kernel_size = 31\n",
    "'''"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 73
    },
    "id": "NHiQbmBy1EIe",
    "outputId": "ad3aa8c6-1a3e-48bc-96b1-1505f73c0276"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "4. 예측 및 시각화 (best_params 기준으로 설정한 값)"
   ],
   "metadata": {
    "id": "IZzMcisGdqIl"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# from sklearn.metrics import classification_report, confusion_matrix\n",
    "#\n",
    "# print(classification_report(y_val.cpu(), pred_labels))\n",
    "# print(confusion_matrix(y_val.cpu(), pred_labels))"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HIg9asMg8vsg",
    "outputId": "d437479f-01c6-45e3-c38f-95aeb8c4b0aa"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "\n",
    "\n",
    "\"\"\"\n",
    "# 1. 실제 라벨\n",
    "true_labels = y_val.cpu().numpy()\n",
    "\n",
    "# 2. 다양한 threshold에 대해 f1-score 측정\n",
    "precisions, recalls, thresholds = precision_recall_curve(true_labels, pred_probs)\n",
    "\n",
    "f1s = 2 * (precisions * recalls) / (precisions + recalls + 1e-8)  # f1-score 계산\n",
    "best_idx = np.argmax(f1s)\n",
    "best_threshold = thresholds[best_idx]\n",
    "\n",
    "print(f\"✅ Best threshold by F1-score: {best_threshold:.4f}, F1: {f1s[best_idx]:.4f}\")\n",
    "\n",
    "# 3. 최적 threshold로 예측 라벨 생성\n",
    "pred_labels = (pred_probs > best_threshold).astype(int)\n",
    "# 6. 누적 수익률 (선택)\n",
    "# future_prices = close_prices[split+1:]  # 실제 수익률 계산용\n",
    "# visualize_cumulative_return(pred_probs, y_val.cpu().numpy(), future_prices)\n",
    "\"\"\"\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 73
    },
    "collapsed": true,
    "id": "FqU-FfFS7-OE",
    "outputId": "efce383a-255f-47b2-b9c4-39857d229915"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "weight_df = pd.DataFrame(index=df_[split+2:]['datadate'])\n",
    "for company_name in ['TSLA', 'NVDA', 'MSFT', 'GOOG', 'AAPL', 'DIS', 'XOM', 'CRM', 'INTC', 'AMZN']:\n",
    "    company_df = pd.read_csv(f'data/{company_name}.csv', index_col=0)['y_pred']\n",
    "    company_df.name = company_name\n",
    "    company_df.index = weight_df.index\n",
    "    weight_df = pd.concat([weight_df, company_df], axis=1)\n",
    "    row_sum = (weight_df == 1).sum(axis=1)\n",
    "    row_sum.replace(0, 0.1, inplace=True)\n",
    "    weight_df_1=weight_df.div(row_sum, axis=0)\n",
    "    weight_df_1.to_csv(f'data/GAT_TCN_weight.csv')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 455
    },
    "collapsed": true,
    "id": "vTpEBkXIHHmR",
    "outputId": "12720ec1-bc4f-4ddb-afbe-aa46ce4821fb"
   },
   "cell_type": "code",
   "source": [
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# from sklearn.metrics import f1_score\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "# from lightgbm import LGBMClassifier\n",
    "#\n",
    "#\n",
    "# def train_lgbm_model(X_train, y_train, X_val):\n",
    "#     model = LGBMClassifier(\n",
    "#         n_estimators=100,\n",
    "#         learning_rate=0.1,             # 너무 작으면 과적합/학습 지연\n",
    "#         verbosity=-1\n",
    "#     )\n",
    "#     model.fit(X_train, y_train)\n",
    "#\n",
    "#     pred_probs = model.predict_proba(X_val)[:, 1]\n",
    "#     pred_labels = (pred_probs > 0.5).astype(int)\n",
    "#\n",
    "#     return model, pred_probs, pred_labels\n",
    "#\n",
    "# # 가상의 node_features, labels, df_ 등이 정의되어 있어야 합니다.\n",
    "# # 위의 리팩토링은 함수 형태만 준비되어 있고, 본문 루프는 사용자가 가진 데이터 프레임 `df_`가 있어야 실행 가능\n",
    "#\n",
    "# def run_lgbm_pipeline(df_):\n",
    "#     results = []\n",
    "#     for company_name in ['TSLA', 'NVDA', 'MSFT', 'GOOG', 'AAPL', 'DIS', 'XOM', 'CRM', 'INTC', 'AMZN']:\n",
    "#         data = df_[\n",
    "#             [f'prccd_{company}' for company in\n",
    "#              ['TSLA', 'NVDA', 'MSFT', 'GOOG', 'AAPL', 'DIS', 'XOM', 'CRM', 'INTC', 'AMZN']] +\n",
    "#             [f'cshtrd_{company_name}', f'sent_{company_name}', 'datadate']].copy()\n",
    "#\n",
    "#         data.set_index('datadate', inplace=True)\n",
    "#         data.fillna(0, inplace=True)\n",
    "#         data.iloc[:, :10] = data.iloc[:, :10].pct_change()\n",
    "#         data = data.dropna()\n",
    "#         close_prices = data[f'prccd_{company_name}'].values\n",
    "#\n",
    "#         # ✅ 라벨: t+1 수익률 기준\n",
    "#         # returns = (close_prices[1:] / close_prices[:-1]) - 1\n",
    "#         labels = np.where(close_prices > 0.003, 1, 0)\n",
    "#\n",
    "#         # ✅ feature는 시점 t까지 (맨 마지막 row 제거)\n",
    "#         data_values = data.values\n",
    "#         scaler = StandardScaler()\n",
    "#         data_s = scaler.fit_transform(data_values[:, :-1])\n",
    "#         node_features = np.hstack([data_s, data_values[:, -1].reshape(-1, 1)])\n",
    "#\n",
    "#\n",
    "#         # ✅ train/val split\n",
    "#         X = node_features[:-1]\n",
    "#         y = labels[1:]\n",
    "#         split = int(len(X) * 0.8)\n",
    "#\n",
    "#         X_train = X[:split]\n",
    "#         X_val   = X[split:]\n",
    "#\n",
    "#         y_train = y[:split]\n",
    "#         y_val   = y[split:]\n",
    "#\n",
    "#         # ✅ 모델 학습 및 예측\n",
    "#         model, pred_probs, pred_labels = train_lgbm_model(X_train, y_train, X_val)\n",
    "#\n",
    "#         print(company_name)\n",
    "#         f1 = f1_score(y_val, pred_labels, average='macro')\n",
    "#         print(f\"F1 Score: {f1:.4f}\")\n",
    "#\n",
    "#         df_result = pd.DataFrame({'y_true': y_val, 'y_pred': pred_labels})\n",
    "#         df_result.to_csv(f'data/{company_name}_lgbm.csv', index=False)\n",
    "#\n",
    "#         results.append({\n",
    "#             \"company\": company_name,\n",
    "#             \"f1_score\": f1,\n",
    "#         })\n",
    "#\n",
    "#     return results"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "result=run_lgbm_pipeline(df_)\n",
    "result"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "weight_df = pd.DataFrame(index=df_[split+2:]['datadate'])\n",
    "for company_name in ['TSLA', 'NVDA', 'MSFT', 'GOOG', 'AAPL', 'DIS', 'XOM', 'CRM', 'INTC', 'AMZN']:\n",
    "    company_df = pd.read_csv(f'data/{company_name}_lgbm.csv', index_col=0)['y_pred']\n",
    "    company_df.name = company_name\n",
    "    company_df.index = weight_df.index\n",
    "    weight_df = pd.concat([weight_df, company_df], axis=1)\n",
    "    row_sum = (weight_df == 1).sum(axis=1)\n",
    "    row_sum.replace(0, 0.1, inplace=True)\n",
    "    weight_df_1=weight_df.div(row_sum, axis=0)\n",
    "    weight_df_1.to_csv(f'data/LGBM_weight.csv')"
   ],
   "outputs": [],
   "execution_count": null
  }
 ]
}

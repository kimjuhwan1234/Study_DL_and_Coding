{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "V28"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3 (ipykernel)",
   "language": "python"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "TPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# 모듈 설치 및 설정"
   ],
   "metadata": {
    "id": "BObOC16LAlS4"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import gc\n",
    "\n",
    "import torch\n",
    "import optuna\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from os import path as pt\n",
    "from lib.utils import pickle_it\n",
    "from sklearn.metrics import f1_score\n",
    "from torch_geometric.nn import GATv2Conv\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "gc.collect()"
   ],
   "metadata": {
    "id": "XgRsALeSVcJX",
    "ExecuteTime": {
     "end_time": "2025-05-27T07:21:22.856695Z",
     "start_time": "2025-05-27T07:21:18.195100Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "source": [
    "# GAT"
   ],
   "metadata": {
    "id": "XyHQgQwjAiTk"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "- 노드 : 각 시점\n",
    "- 피처 : 10개 주식 종가 + 예측 기업 거래량 + 예측 기업 감정분석\n",
    "- GAT의 역할 : 피처들의 관계(회사 간의 관계 등)을 파악해 **시점별** 임베딩 생성;  다른 시점과의 연관성을 반영    \n",
    "(ex. 1~10일 전과 연결이 되어있는 상태에서, 1일 전 정보는 얼마나 중요하고 10일 전 정보는 얼마나 중요한지 판단)"
   ],
   "metadata": {
    "id": "m6DEYVqJC0_l"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "'''\n",
    "class GAT(nn.Module):\n",
    "    def __init__(self, in_features, out_features, heads, dropout, concat=False):\n",
    "        super(GAT, self).__init__()\n",
    "\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.heads = heads\n",
    "        self.dropout = dropout\n",
    "        self.concat = concat\n",
    "\n",
    "        # 선형 변환을 위한 가중치 행렬\n",
    "        self.lin = nn.Linear(in_features, heads * out_features, bias=False)\n",
    "\n",
    "        # 어텐션 계수 계산을 위한 가중치\n",
    "        self.att = nn.Parameter(torch.Tensor(1, heads, out_features))\n",
    "\n",
    "        # 바이어스\n",
    "        self.bias = nn.Parameter(torch.Tensor(out_features))\n",
    "\n",
    "        # LeakyReLU\n",
    "        self.leakyrelu = nn.LeakyReLU(0.2)\n",
    "\n",
    "        # 드롭아웃\n",
    "        self.dropout_layer = nn.Dropout(dropout)\n",
    "\n",
    "        # 초기화\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        # 가중치 초기화\n",
    "        gain = nn.init.calculate_gain('relu')\n",
    "        nn.init.xavier_normal_(self.lin.weight, gain=gain)\n",
    "        nn.init.xavier_normal_(self.att, gain=gain)\n",
    "        nn.init.zeros_(self.bias)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        \"\"\"\n",
    "        x: 노드 특성 [N, in_features]\n",
    "        edge_index: 엣지 인덱스 [2, E]\n",
    "        \"\"\"\n",
    "        N = x.size(0)  # 노드 수\n",
    "\n",
    "        # 1. 선형 변환\n",
    "        x = self.dropout_layer(x)\n",
    "        x = self.lin(x)  # [N, heads * out_features]\n",
    "        x = x.view(N, self.heads, self.out_features)  # [N, heads, out_features]\n",
    "\n",
    "        # 2. 엣지 리스트로부터 어텐션 계산\n",
    "        edge_src, edge_dst = edge_index[0], edge_index[1]\n",
    "\n",
    "        # 3. GATv2 : 선형변환 후 어텐션\n",
    "        # 각 노드에 어텐션 가중치 적용\n",
    "        x_att = x * self.att  # [N, heads, out_features]\n",
    "\n",
    "        # 이웃 노드 쌍의 어텐션 점수 계산\n",
    "        alpha_src = x_att[edge_src].sum(dim=-1)  # [E, heads]\n",
    "        alpha_dst = x_att[edge_dst].sum(dim=-1)  # [E, heads]\n",
    "        alpha = alpha_src + alpha_dst  # [E, heads]\n",
    "        alpha = self.leakyrelu(alpha)  # [E, heads]\n",
    "\n",
    "        # 4. 소프트맥스로 정규화\n",
    "        alpha = self._edge_softmax(alpha, edge_index[1], N)\n",
    "        alpha = self.dropout_layer(alpha)\n",
    "\n",
    "        # 5. 메시지 집계\n",
    "        out = torch.zeros(N, self.heads, self.out_features, device=x.device)\n",
    "\n",
    "        # 각 엣지에 대해 메시지 전달\n",
    "        for i in range(edge_index.size(1)):\n",
    "            src, dst = edge_index[0, i], edge_index[1, i]\n",
    "            out[dst] += alpha[i].unsqueeze(-1) * x[src]\n",
    "\n",
    "        # 6. 최종 출력 형태 결정\n",
    "        if self.concat:\n",
    "            out = out.view(N, self.heads * self.out_features)\n",
    "        else:\n",
    "            out = out.mean(dim=1)  # 헤드 간 평균\n",
    "\n",
    "        # 7. 바이어스 추가\n",
    "        out = out + self.bias\n",
    "\n",
    "        return out\n",
    "\n",
    "    def _edge_softmax(self, alpha, target_nodes, num_nodes):\n",
    "\n",
    "        norm_alpha = torch.zeros_like(alpha)\n",
    "\n",
    "\n",
    "        for i in range(num_nodes):\n",
    "            # 현재 노드로 향하는 엣지 마스크\n",
    "            mask = (target_nodes == i)\n",
    "            if mask.sum() > 0:\n",
    "                # 해당 노드로 향하는 엣지에 대해 소프트맥스 적용\n",
    "                norm_alpha[mask] = F.softmax(alpha[mask], dim=0)\n",
    "\n",
    "        return norm_alpha\n",
    " '''"
   ],
   "metadata": {
    "id": "GegpQ78WROjC",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 91
    },
    "outputId": "7cc16100-9a36-400c-caf2-5b009a27f982",
    "ExecuteTime": {
     "end_time": "2025-05-27T07:21:24.230549Z",
     "start_time": "2025-05-27T07:21:24.215814Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nclass GAT(nn.Module):\\n    def __init__(self, in_features, out_features, heads, dropout, concat=False):\\n        super(GAT, self).__init__()\\n\\n        self.in_features = in_features\\n        self.out_features = out_features\\n        self.heads = heads\\n        self.dropout = dropout\\n        self.concat = concat\\n\\n        # 선형 변환을 위한 가중치 행렬\\n        self.lin = nn.Linear(in_features, heads * out_features, bias=False)\\n\\n        # 어텐션 계수 계산을 위한 가중치\\n        self.att = nn.Parameter(torch.Tensor(1, heads, out_features))\\n\\n        # 바이어스\\n        self.bias = nn.Parameter(torch.Tensor(out_features))\\n\\n        # LeakyReLU\\n        self.leakyrelu = nn.LeakyReLU(0.2)\\n\\n        # 드롭아웃\\n        self.dropout_layer = nn.Dropout(dropout)\\n\\n        # 초기화\\n        self.reset_parameters()\\n\\n    def reset_parameters(self):\\n        # 가중치 초기화\\n        gain = nn.init.calculate_gain(\\'relu\\')\\n        nn.init.xavier_normal_(self.lin.weight, gain=gain)\\n        nn.init.xavier_normal_(self.att, gain=gain)\\n        nn.init.zeros_(self.bias)\\n\\n    def forward(self, x, edge_index):\\n        \"\"\"\\n        x: 노드 특성 [N, in_features]\\n        edge_index: 엣지 인덱스 [2, E]\\n        \"\"\"\\n        N = x.size(0)  # 노드 수\\n\\n        # 1. 선형 변환\\n        x = self.dropout_layer(x)\\n        x = self.lin(x)  # [N, heads * out_features]\\n        x = x.view(N, self.heads, self.out_features)  # [N, heads, out_features]\\n\\n        # 2. 엣지 리스트로부터 어텐션 계산\\n        edge_src, edge_dst = edge_index[0], edge_index[1]\\n\\n        # 3. GATv2 : 선형변환 후 어텐션\\n        # 각 노드에 어텐션 가중치 적용\\n        x_att = x * self.att  # [N, heads, out_features]\\n\\n        # 이웃 노드 쌍의 어텐션 점수 계산\\n        alpha_src = x_att[edge_src].sum(dim=-1)  # [E, heads]\\n        alpha_dst = x_att[edge_dst].sum(dim=-1)  # [E, heads]\\n        alpha = alpha_src + alpha_dst  # [E, heads]\\n        alpha = self.leakyrelu(alpha)  # [E, heads]\\n\\n        # 4. 소프트맥스로 정규화\\n        alpha = self._edge_softmax(alpha, edge_index[1], N)\\n        alpha = self.dropout_layer(alpha)\\n\\n        # 5. 메시지 집계\\n        out = torch.zeros(N, self.heads, self.out_features, device=x.device)\\n\\n        # 각 엣지에 대해 메시지 전달\\n        for i in range(edge_index.size(1)):\\n            src, dst = edge_index[0, i], edge_index[1, i]\\n            out[dst] += alpha[i].unsqueeze(-1) * x[src]\\n\\n        # 6. 최종 출력 형태 결정\\n        if self.concat:\\n            out = out.view(N, self.heads * self.out_features)\\n        else:\\n            out = out.mean(dim=1)  # 헤드 간 평균\\n\\n        # 7. 바이어스 추가\\n        out = out + self.bias\\n\\n        return out\\n\\n    def _edge_softmax(self, alpha, target_nodes, num_nodes):\\n\\n        norm_alpha = torch.zeros_like(alpha)\\n\\n\\n        for i in range(num_nodes):\\n            # 현재 노드로 향하는 엣지 마스크\\n            mask = (target_nodes == i)\\n            if mask.sum() > 0:\\n                # 해당 노드로 향하는 엣지에 대해 소프트맥스 적용\\n                norm_alpha[mask] = F.softmax(alpha[mask], dim=0)\\n\\n        return norm_alpha\\n '"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "source": [
    "# GAT 레이어 정의\n",
    "\n",
    "class GATLayer(nn.Module):\n",
    "    def __init__(self, in_features, out_features, heads, dropout):\n",
    "        super().__init__()\n",
    "        self.gat = GATv2Conv(in_features, out_features, heads=heads, dropout=dropout, concat=False)  # PyG 사용\n",
    "        # self.gat = GAT(in_features, out_features, heads=heads, dropout=dropout, concat=False)            # 상단 구현 코드 (느리고, 느려서 optuna 중간에 끊고 첫번째걸로 해봤는데 ----로 예측됨;;;;)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        return self.gat(x, edge_index)"
   ],
   "metadata": {
    "id": "ZRWMJA0G_U2x",
    "ExecuteTime": {
     "end_time": "2025-05-27T07:21:24.649299Z",
     "start_time": "2025-05-27T07:21:24.635619Z"
    }
   },
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "source": [
    "# TCN"
   ],
   "metadata": {
    "id": "2_Ee7Aj6A3K_"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# --- 유틸 함수 ---\n",
    "def get_conv1d(in_channels, out_channels, kernel_size, stride, padding, dilation, groups, bias):\n",
    "    return nn.Conv1d(in_channels=in_channels, out_channels=out_channels,\n",
    "                     kernel_size=kernel_size, stride=stride,\n",
    "                     padding=padding, dilation=dilation,\n",
    "                     groups=groups, bias=bias)\n",
    "\n",
    "\n",
    "def get_bn(channels):\n",
    "    return nn.BatchNorm1d(channels)\n",
    "\n",
    "\n",
    "def conv_bn(in_channels, out_channels, kernel_size, stride, padding, groups, dilation=1, bias=False):\n",
    "    if padding is None:\n",
    "        padding = kernel_size // 2\n",
    "    result = nn.Sequential()\n",
    "    result.add_module('conv',\n",
    "                      get_conv1d(in_channels, out_channels, kernel_size, stride, padding, dilation, groups, bias))\n",
    "    result.add_module('bn', get_bn(out_channels))\n",
    "    return result\n",
    "\n",
    "\n",
    "# --- RevIN ---\n",
    "class RevIN(nn.Module):\n",
    "    def __init__(self, num_features: int, eps=1e-5, affine=True, subtract_last=False):\n",
    "        super().__init__()\n",
    "        self.num_features = num_features\n",
    "        self.eps = eps\n",
    "        self.affine = affine\n",
    "        self.subtract_last = subtract_last\n",
    "        if self.affine:\n",
    "            self._init_params()\n",
    "\n",
    "    def _init_params(self):\n",
    "        self.affine_weight = nn.Parameter(torch.ones(self.num_features))\n",
    "        self.affine_bias = nn.Parameter(torch.zeros(self.num_features))\n",
    "\n",
    "    def forward(self, x, mode: str):\n",
    "        if mode == 'norm':\n",
    "            self._get_statistics(x)\n",
    "            x = self._normalize(x)\n",
    "        elif mode == 'denorm':\n",
    "            x = self._denormalize(x)\n",
    "        return x\n",
    "\n",
    "    def _get_statistics(self, x):\n",
    "        dim2reduce = tuple(range(1, x.ndim - 1))\n",
    "        if self.subtract_last:\n",
    "            self.last = x[:, -1:, :].unsqueeze(1)\n",
    "        else:\n",
    "            self.mean = torch.mean(x, dim=dim2reduce, keepdim=True).detach()\n",
    "        self.stdev = torch.sqrt(torch.var(x, dim=dim2reduce, keepdim=True, unbiased=False) + self.eps).detach()\n",
    "\n",
    "    def _normalize(self, x):\n",
    "        x = (x - self.mean) / self.stdev\n",
    "        if self.affine:\n",
    "            x = x * self.affine_weight[None, None, :] + self.affine_bias[None, None, :]\n",
    "        return x\n",
    "\n",
    "    def _denormalize(self, x):\n",
    "        if self.affine:\n",
    "            x = (x - self.affine_bias[None, None, :]) / self.affine_weight[None, None, :]\n",
    "        x = x * self.stdev + self.mean\n",
    "        return x\n",
    "\n",
    "\n",
    "# --- 시계열 분해 ---\n",
    "class moving_avg(nn.Module):\n",
    "    def __init__(self, kernel_size, stride):\n",
    "        super().__init__()\n",
    "        self.kernel_size = kernel_size\n",
    "        self.avg = nn.AvgPool1d(kernel_size=kernel_size, stride=stride, padding=0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        front = x[:, 0:1, :].repeat(1, (self.kernel_size - 1) // 2, 1)\n",
    "        end = x[:, -1:, :].repeat(1, (self.kernel_size - 1) // 2, 1)\n",
    "        x = torch.cat([front, x, end], dim=1)\n",
    "        x = self.avg(x.permute(0, 2, 1))\n",
    "        return x.permute(0, 2, 1)\n",
    "\n",
    "\n",
    "class series_decomp(nn.Module):\n",
    "    def __init__(self, kernel_size):\n",
    "        super().__init__()\n",
    "        self.moving_avg = moving_avg(kernel_size, stride=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        moving_mean = self.moving_avg(x)\n",
    "        return x - moving_mean, moving_mean\n",
    "\n",
    "\n",
    "# --- 커스텀 커널 ---\n",
    "class ReparamLargeKernelConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride, groups, small_kernel, small_kernel_merged=False):\n",
    "        super().__init__()\n",
    "        self.kernel_size = kernel_size\n",
    "        self.small_kernel = small_kernel\n",
    "        padding = kernel_size // 2\n",
    "        if small_kernel_merged:\n",
    "            self.lkb_reparam = nn.Conv1d(in_channels, out_channels, kernel_size, stride, padding, groups=groups,\n",
    "                                         bias=True)\n",
    "        else:\n",
    "            self.lkb_origin = conv_bn(in_channels, out_channels, kernel_size, stride, padding, groups)\n",
    "            if small_kernel is not None:\n",
    "                self.small_conv = conv_bn(in_channels, out_channels, small_kernel, stride, small_kernel // 2, groups)\n",
    "\n",
    "    def forward(self, x):\n",
    "        if hasattr(self, 'lkb_reparam'):\n",
    "            return self.lkb_reparam(x)\n",
    "        out = self.lkb_origin(x)\n",
    "        if hasattr(self, 'small_conv'):\n",
    "            out += self.small_conv(x)\n",
    "        return out\n",
    "\n",
    "\n",
    "# --- 출력층 ---\n",
    "class Flatten_Head(nn.Module):\n",
    "    def __init__(self, d_model):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(d_model, 1)\n",
    "\n",
    "    def forward(self, x):  # x: [B, C, T]\n",
    "        x = x.permute(0, 2, 1)  # → [B, T, C]\n",
    "        x = self.linear(x)  # → [B, T, 1]\n",
    "        return x.squeeze(-1)  # → [B, T]"
   ],
   "metadata": {
    "id": "Ky1M6aXiA47i",
    "ExecuteTime": {
     "end_time": "2025-05-27T07:21:25.530783Z",
     "start_time": "2025-05-27T07:21:25.499092Z"
    }
   },
   "outputs": [],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "source": [
    "# --- ModernTCN 모델 ---\n",
    "class ModernTCN(nn.Module):\n",
    "    def __init__(self, configs):\n",
    "        super().__init__()\n",
    "        self.revin = RevIN(configs.enc_in, affine=configs.affine) if configs.revin else None\n",
    "        self.decomp = series_decomp(configs.kernel_size) if configs.decomposition else None\n",
    "\n",
    "        self.conv_layers = nn.ModuleList()\n",
    "        self.norm_layers = nn.ModuleList()\n",
    "\n",
    "        c_in = configs.enc_in\n",
    "        for i in range(len(configs.dims)):\n",
    "            conv = ReparamLargeKernelConv(c_in, configs.dims[i],\n",
    "                                          kernel_size=configs.large_size[i],\n",
    "                                          stride=1,\n",
    "                                          groups=1,\n",
    "                                          small_kernel=configs.small_size[i],\n",
    "                                          small_kernel_merged=configs.small_kernel_merged)\n",
    "            self.conv_layers.append(conv)\n",
    "            self.norm_layers.append(nn.BatchNorm1d(configs.dims[i]))\n",
    "            c_in = configs.dims[i]\n",
    "\n",
    "        self.head = Flatten_Head(configs.dims[-1])\n",
    "\n",
    "    def forward(self, x):  # x: [B, T, C]\n",
    "        if self.revin:\n",
    "            x = self.revin(x, 'norm')\n",
    "        if self.decomp:\n",
    "            x, _ = self.decomp(x)\n",
    "        x = x.permute(0, 2, 1)  # [B, C, T]\n",
    "        for conv, norm in zip(self.conv_layers, self.norm_layers):\n",
    "            x = conv(x)\n",
    "            x = norm(x)\n",
    "            x = F.relu(x)\n",
    "        out = self.head(x)  # [B, T]\n",
    "        return out"
   ],
   "metadata": {
    "id": "eySi1894CGFo",
    "ExecuteTime": {
     "end_time": "2025-05-27T07:21:25.857271Z",
     "start_time": "2025-05-27T07:21:25.843612Z"
    }
   },
   "outputs": [],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "source": [
    "# --- Config 클래스 ---\n",
    "class Configs:\n",
    "    def __init__(self, enc_in):\n",
    "        self.enc_in = enc_in\n",
    "        self.dims = [8, 16, 32]\n",
    "        self.large_size = [5, 5, 3]\n",
    "        self.small_size = [5, 3, 3]\n",
    "        self.small_kernel_merged = False\n",
    "        self.dropout = 0.1\n",
    "        self.head_dropout = 0.2\n",
    "        self.revin = True\n",
    "        self.affine = True\n",
    "        self.decomposition = True\n",
    "        self.kernel_size = 25"
   ],
   "metadata": {
    "id": "Bqr3BMHdCEI6",
    "ExecuteTime": {
     "end_time": "2025-05-27T07:21:26.386089Z",
     "start_time": "2025-05-27T07:21:26.367752Z"
    }
   },
   "outputs": [],
   "execution_count": 6
  },
  {
   "cell_type": "markdown",
   "source": [
    "# GAT-TCN"
   ],
   "metadata": {
    "id": "hC824c4hBPKK"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "class GATCNModel(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.gat = GATLayer(node_features.shape[1], config.gat_out_features,\n",
    "                            config.gat_heads, config.gat_dropout)  # GAT\n",
    "        self.tcn = ModernTCN(config)  # TCN\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        embeddings = self.gat(x, edge_index)  # GAT를 통한 임베딩 생성\n",
    "        tcn_input = embeddings.unsqueeze(0)  # TCN 입력 형태로 변환\n",
    "        output = self.tcn(tcn_input)  # TCN으로 예측\n",
    "        return output"
   ],
   "metadata": {
    "id": "nu0GYmnkBOtr",
    "ExecuteTime": {
     "end_time": "2025-05-27T07:21:27.304710Z",
     "start_time": "2025-05-27T07:21:27.289466Z"
    }
   },
   "outputs": [],
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "source": [
    "\n",
    "# GAT-TCN 모델의 최적 파라미터 탐색\n",
    "\n",
    "def train_model(model, node_features, edge_index, X_train, y_train, X_val, y_val, epochs=30, lr=1e-3, pos_weight=None):\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    #criterion = nn.BCEWithLogitsLoss()\n",
    "    criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight) if pos_weight is not None else nn.BCEWithLogitsLoss()\n",
    "    train_losses, val_losses, val_accs = [], [], []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        # 1.학습 단계\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        train_output = model(node_features, edge_index).squeeze(0)[:len(y_train)]  # 통합 모델에 데이터를 적용한 결과\n",
    "        loss = criterion(train_output, y_train.float())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_losses.append(loss.item())\n",
    "\n",
    "        # 2. 검증 단계\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_output = model(node_features, edge_index).squeeze(0)[-len(y_val):]\n",
    "            val_loss = criterion(val_output, y_val.float()).item()\n",
    "            pred = (torch.sigmoid(val_output) > 0.5).int()\n",
    "            acc = (pred == y_val).float().mean().item()\n",
    "\n",
    "        val_losses.append(val_loss)\n",
    "        val_accs.append(acc)\n",
    "\n",
    "        print(f\"[{epoch + 1}/{epochs}] Train Loss: {loss.item():.4f}, Val Loss: {val_loss:.4f}, Val Acc: {acc:.4f}\")\n",
    "\n",
    "    return train_losses, val_losses, val_accs\n"
   ],
   "metadata": {
    "id": "F5pcxCD-EjI4",
    "ExecuteTime": {
     "end_time": "2025-05-27T07:21:27.963107Z",
     "start_time": "2025-05-27T07:21:27.952437Z"
    }
   },
   "outputs": [],
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "source": [
    "def objective(trial):\n",
    "    # GAT\n",
    "    gat_out_features = trial.suggest_categorical(\"gat_out_features\", [4, 8, 12])  # 기존 features 수보다는 적은 것이 적합\n",
    "    gat_heads = trial.suggest_categorical(\"gat_heads\", [1, 2, 4, 8])\n",
    "    gat_dropout = trial.suggest_float(\"gat_dropout\", 0.0, 0.3)\n",
    "\n",
    "    # TCN\n",
    "    dims = [\n",
    "        trial.suggest_categorical(\"dim1\", [8, 16, 32, 64]),\n",
    "        trial.suggest_categorical(\"dim2\", [16, 32, 64, 128]),\n",
    "        trial.suggest_categorical(\"dim3\", [32, 64, 128, 256])\n",
    "    ]\n",
    "    large_size = [\n",
    "        trial.suggest_categorical(\"k1\", [3, 5, 7, 9, 11]),\n",
    "        trial.suggest_categorical(\"k2\", [3, 5, 7, 9]),\n",
    "        trial.suggest_categorical(\"k3\", [3, 5, 7])\n",
    "    ]\n",
    "    small_size = [\n",
    "        trial.suggest_categorical(\"s1\", [1, 3, 5]),\n",
    "        trial.suggest_categorical(\"s2\", [1, 3]),\n",
    "        trial.suggest_categorical(\"s3\", [1, 3])\n",
    "    ]\n",
    "    dropout = trial.suggest_float(\"dropout\", 0.0, 0.3)\n",
    "    head_dropout = trial.suggest_float(\"head_dropout\", 0.0, 0.3)\n",
    "    kernel_size = trial.suggest_categorical(\"kernel_size\", [5, 11, 15, 25, 31])\n",
    "    decomposition = trial.suggest_categorical(\"decomposition\", [True, False])\n",
    "    revin = trial.suggest_categorical(\"revin\", [True, False])\n",
    "    affine = trial.suggest_categorical(\"affine\", [True, False])\n",
    "\n",
    "    # 통합\n",
    "    class TrialConfig:\n",
    "        def __init__(self):\n",
    "            self.gat_out_features = gat_out_features\n",
    "            self.gat_heads = gat_heads\n",
    "            self.gat_dropout = gat_dropout\n",
    "\n",
    "            self.enc_in = gat_out_features\n",
    "            self.dims = dims\n",
    "            self.large_size = large_size\n",
    "            self.small_size = small_size\n",
    "            self.small_kernel_merged = False\n",
    "            self.dropout = dropout\n",
    "            self.head_dropout = head_dropout\n",
    "            self.revin = revin\n",
    "            self.affine = affine\n",
    "            self.decomposition = decomposition\n",
    "            self.kernel_size = kernel_size\n",
    "\n",
    "    model = GATCNModel(TrialConfig())\n",
    "\n",
    "    # Accuracy 기준 최적화 #####\n",
    "    _, _, val_accs = train_model(model, node_features, edge_index, X_train, y_train, X_val, y_val, epochs=50)\n",
    "    return max(val_accs)\n",
    "    ############################\n",
    "\n",
    "    # F1 Score 기준 최적화 #####################\n",
    "    # pos_weight 계산 (불균형 데이터 보정)\n",
    "    '''\n",
    "    pos_weight = torch.tensor([(y_train == 0).sum() / (y_train == 1).sum()]).to(y_train.device)\n",
    "\n",
    "    train_model(model, node_features, edge_index, X_train, y_train, X_val, y_val, epochs=15, pos_weight=pos_weight)\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        full_pred = model(node_features, edge_index).squeeze(0)\n",
    "        pred = full_pred[-len(y_val):]\n",
    "        probs = torch.sigmoid(pred).cpu().numpy()\n",
    "        preds = (probs > 0.5).astype(int)\n",
    "\n",
    "    y_true = y_val.cpu().numpy()\n",
    "    return f1_score(y_true, preds)\n",
    "    '''\n",
    "    ##########################################"
   ],
   "metadata": {
    "id": "Ri6Us2oFMJlq",
    "ExecuteTime": {
     "end_time": "2025-05-27T07:21:28.694110Z",
     "start_time": "2025-05-27T07:21:28.677429Z"
    }
   },
   "outputs": [],
   "execution_count": 9
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 시각화 코드"
   ],
   "metadata": {
    "id": "e6IIhl_Vo5x9"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def visualize_training(company_name, train_losses, val_losses, val_accs):\n",
    "    plt.figure(figsize=(12, 2), dpi=400)\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(train_losses, label='Train Loss')\n",
    "    plt.plot(val_losses, label='Val Loss')\n",
    "    plt.legend()\n",
    "    plt.title(\"Loss over Epochs\")\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(val_accs, label='Val Accuracy')\n",
    "    plt.legend()\n",
    "    plt.title(\"Validation Accuracy\")\n",
    "    plt.savefig(f'data/images/{company_name}_loss.png')\n",
    "    plt.close()\n",
    "\n",
    "from matplotlib.patches import Patch\n",
    "\n",
    "def visualize_match_only(company_name, pred_probs, true_labels, threshold=0.5):\n",
    "    # 이진 예측\n",
    "    pred_labels = (pred_probs >= threshold).astype(int)\n",
    "\n",
    "    # 정답과 예측이 일치하면 1, 다르면 0\n",
    "    match = (pred_labels == true_labels).astype(int)\n",
    "\n",
    "    # 시각화\n",
    "    plt.figure(figsize=(12, 2), dpi=400)\n",
    "    bar_heights = np.ones_like(match)\n",
    "    bar_colors = ['green' if m else 'red' for m in match]\n",
    "    plt.bar(np.arange(len(match)), bar_heights,\n",
    "            color=bar_colors,\n",
    "            width=1.0)\n",
    "\n",
    "    plt.title(f\"{company_name} - Prediction Match\")\n",
    "    plt.ylabel('Match')\n",
    "    plt.xlabel('Time')\n",
    "    plt.yticks([0, 1], ['Wrong', 'Correct'])\n",
    "\n",
    "    # ✅ 범례 추가\n",
    "    legend_elements = [\n",
    "        Patch(facecolor='green', label='Correct'),\n",
    "        Patch(facecolor='red', label='Wrong')\n",
    "    ]\n",
    "    plt.legend(handles=legend_elements, loc='upper right')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'data/images/{company_name}_match_only.png')\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def visualize_cumulative_return(pred_probs, true_labels, prices):\n",
    "    signal = (pred_probs > 0.5).astype(int)\n",
    "    returns = (prices[1:] / prices[:-1]) - 1\n",
    "    strategy_returns = returns * signal[:-1]\n",
    "\n",
    "    cumulative = (strategy_returns + 1).cumprod()\n",
    "    market = (returns + 1).cumprod()\n",
    "\n",
    "    plt.plot(cumulative, label='Strategy')\n",
    "    plt.plot(market, label='Market (buy & hold)')\n",
    "    plt.legend();\n",
    "    plt.title(\"Cumulative Return\")\n",
    "    plt.show()"
   ],
   "metadata": {
    "id": "nj38FLOMtL84",
    "ExecuteTime": {
     "end_time": "2025-05-27T07:21:30.155627Z",
     "start_time": "2025-05-27T07:21:30.133289Z"
    }
   },
   "outputs": [],
   "execution_count": 10
  },
  {
   "cell_type": "markdown",
   "source": [
    "# GAT-TCN 적용"
   ],
   "metadata": {
    "id": "2gL9v1aBFqxZ"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# seed 설정\n",
    "import random\n",
    "\n",
    "\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.use_deterministic_algorithms(True)\n",
    "\n",
    "\n",
    "set_seed(42)\n"
   ],
   "metadata": {
    "id": "YmnyK8iVm3nZ",
    "ExecuteTime": {
     "end_time": "2025-05-27T07:21:31.619843Z",
     "start_time": "2025-05-27T07:21:31.601672Z"
    }
   },
   "outputs": [],
   "execution_count": 11
  },
  {
   "cell_type": "markdown",
   "source": [
    "1. 데이터셋 불러오기 및 라벨 생성"
   ],
   "metadata": {
    "id": "ZkK07jzudiAx"
   }
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-27T07:21:35.780028Z",
     "start_time": "2025-05-27T07:21:35.752636Z"
    }
   },
   "cell_type": "code",
   "source": "df_ = pd.read_csv(\"./data/daily_all.csv\")",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-27T07:21:36.172012Z",
     "start_time": "2025-05-27T07:21:36.158361Z"
    }
   },
   "cell_type": "code",
   "source": "# company_name =['TSLA', 'NVDA', 'MSFT', 'GOOG', 'AAPL', 'DIS', 'XOM', 'CRM', 'INTC', 'AMZN'][0]",
   "outputs": [],
   "execution_count": 13
  },
  {
   "cell_type": "code",
   "source": [
    "for company_name in ['TSLA', 'NVDA', 'MSFT', 'GOOG', 'AAPL', 'DIS', 'XOM', 'CRM', 'INTC', 'AMZN']:\n",
    "    # company_name = \"AMZN\"  # 예측할 회사 선택\n",
    "    data = df_[\n",
    "        [f'prccd_{company}' for company in\n",
    "         ['TSLA', 'NVDA', 'MSFT', 'GOOG', 'AAPL', 'DIS', 'XOM', 'CRM', 'INTC', 'AMZN']] +\n",
    "        [f'cshtrd_{company_name}', f'sent_{company_name}', 'datadate']].copy()\n",
    "    data.set_index('datadate', inplace=True)\n",
    "    data.fillna(0, inplace=True)  # 감정분석 결측값을 0으로\n",
    "    data.iloc[:, :10] = data.iloc[:, :10].pct_change()\n",
    "            # returns = (close_prices[1:] / close_prices[:-1]) - 1\n",
    "        # labels = np.where(returns > 0.003, 1, 0)\n",
    "    data = data.dropna()\n",
    "    data_values = data.values\n",
    "    scaler = StandardScaler()\n",
    "    data_s = scaler.fit_transform(data_values[:, :-1])\n",
    "    df_preprocessed = np.hstack([data_s, data_values[:, -1].reshape(-1, 1)])\n",
    "    node_features = df_preprocessed\n",
    "    n_nodes = node_features.shape[0]  # 노드 수 (==날짜 수)\n",
    "\n",
    "    # 그래프 형태로 변환 : 엣지 생성 (시점 간 연결)\n",
    "    edge_list = []\n",
    "    for i in range(n_nodes):\n",
    "        for j in [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]:  # 이후 시점들에 단방향 연결; (휴장 같은 것은 생각하지 않음.... 시점 기준)\n",
    "            if i + j < n_nodes:\n",
    "                edge_list.append([i, i + j])\n",
    "    edge_index = torch.tensor(edge_list).t()\n",
    "\n",
    "\n",
    "    # edge_list = []\n",
    "    # for i in range(n_nodes):\n",
    "    #     for j in range(1, 11):  # 과거 1~10시점\n",
    "    #         if i - j >= 0:\n",
    "    #             edge_list.append([i, i - j])  # 현재 → 과거 방향\n",
    "    # edge_index = torch.tensor(edge_list).t()\n",
    "\n",
    "    # 라벨 생성\n",
    "    close_prices = data[f'prccd_{company_name}'].values\n",
    "    # returns = (close_prices[1:] / close_prices[:-1]) - 1\n",
    "    labels = np.where(close_prices > 0.003, 1, 0)  # 0.3% 초과만 1로\n",
    "    labels = torch.tensor(labels, dtype=torch.float32)\n",
    "    node_features = torch.tensor(node_features, dtype=torch.float)\n",
    "\n",
    "    split = int(len(node_features) * 0.8)\n",
    "    # print(split)\n",
    "    # print(len(labels))\n",
    "\n",
    "    # print(edge_index)\n",
    "    # train_edge_index = edge_index[:split]\n",
    "    # val_edge_index = edge_index[split:]\n",
    "\n",
    "    train_node_features = node_features[:split]\n",
    "    val_node_features = node_features[split:]\n",
    "\n",
    "    train_labels = labels[:split]\n",
    "    val_labels = labels[split:]\n",
    "    X = train_node_features[:-1]\n",
    "    y = train_labels[1:]\n",
    "    split = int(len(X) * 0.8)\n",
    "\n",
    "\n",
    "    X_train = X[:split]\n",
    "    X_val   = X[split:]\n",
    "\n",
    "    y_train = y[:split]\n",
    "    y_val   = y[split:]\n",
    "    # Optuna 튜닝 실행\n",
    "    study = optuna.create_study(direction=\"maximize\")\n",
    "    study.optimize(objective, n_trials=1)\n",
    "    # 최적 하이퍼파라미터 출력\n",
    "    print(\"✅ Best Trial:\")\n",
    "    print(study.best_trial.params)\n",
    "    best_params = study.best_trial.params\n",
    "\n",
    "\n",
    "    # BestConfig를 이용해 모델 설정 후 학습 : optuna를 통해 정해진 최적 하이퍼파라미터\n",
    "\n",
    "    class BestConfig:\n",
    "        def __init__(self):\n",
    "            # GAT 설정\n",
    "            self.gat_out_features = best_params['gat_out_features']\n",
    "            self.gat_heads = best_params['gat_heads']\n",
    "            self.gat_dropout = best_params['gat_dropout']\n",
    "\n",
    "            # TCN 설정\n",
    "            self.enc_in = best_params['gat_out_features']  # GAT 출력 = TCN 입력\n",
    "            self.dims = [best_params['dim1'], best_params['dim2'], best_params['dim3']]\n",
    "            self.large_size = [best_params['k1'], best_params['k2'], best_params['k3']]\n",
    "            self.small_size = [best_params['s1'], best_params['s2'], best_params['s3']]\n",
    "            self.small_kernel_merged = False\n",
    "            self.dropout = best_params['dropout']\n",
    "            self.head_dropout = best_params['head_dropout']\n",
    "            self.revin = best_params['revin']\n",
    "            self.affine = best_params['affine']\n",
    "            self.decomposition = best_params['decomposition']\n",
    "            self.kernel_size = best_params['kernel_size']\n",
    "\n",
    "\n",
    "    model = GATCNModel(BestConfig())\n",
    "    train_losses, val_losses, val_accs = train_model(model, train_node_features, edge_index, X_train, y_train, X_val, y_val,\n",
    "                                                     epochs=75, lr=1e-3)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        pred_logits = model(val_node_features, edge_index).squeeze(0)  # [1, T] → [T]\n",
    "        pred_probs = torch.sigmoid(pred_logits).cpu().numpy()\n",
    "        pred_labels = (pred_probs > 0.5).astype(int)\n",
    "\n",
    "    print(company_name)\n",
    "    visualize_training(company_name, train_losses, val_losses, val_accs)\n",
    "    visualize_match_only(company_name, pred_probs, val_labels.cpu().numpy())\n",
    "\n",
    "    df = pd.DataFrame(np.vstack([val_labels.cpu().numpy(), pred_labels])).T\n",
    "    df.columns = ['y_true', 'y_pred']  # 열 이름 지정\n",
    "\n",
    "    f1 = f1_score(df['y_true'], df['y_pred'], average='macro')  # or 'micro', 'weighted'\n",
    "    print(f\"F1 Score: {f1:.4f}\")\n",
    "    df.to_csv(f'data/{company_name}.csv')\n",
    "    pickle_it(model.to('cpu').state_dict(), pt.join('general_results', f'weights_{company_name}.torch'))"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 455
    },
    "collapsed": true,
    "id": "vTpEBkXIHHmR",
    "outputId": "12720ec1-bc4f-4ddb-afbe-aa46ce4821fb",
    "ExecuteTime": {
     "end_time": "2025-05-27T07:25:09.482611Z",
     "start_time": "2025-05-27T07:24:12.708918Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-27 16:24:12,977] A new study created in memory with name: no-name-2095b097-4a35-4103-a142-ea7b8780b8e3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/50] Train Loss: 0.7165, Val Loss: 0.7079, Val Acc: 0.4548\n",
      "[2/50] Train Loss: 0.6933, Val Loss: 0.7073, Val Acc: 0.4548\n",
      "[3/50] Train Loss: 0.6821, Val Loss: 0.7055, Val Acc: 0.4548\n",
      "[4/50] Train Loss: 0.6719, Val Loss: 0.7039, Val Acc: 0.4548\n",
      "[5/50] Train Loss: 0.6599, Val Loss: 0.7034, Val Acc: 0.4525\n",
      "[6/50] Train Loss: 0.6504, Val Loss: 0.7035, Val Acc: 0.4638\n",
      "[7/50] Train Loss: 0.6530, Val Loss: 0.7036, Val Acc: 0.4525\n",
      "[8/50] Train Loss: 0.6464, Val Loss: 0.7039, Val Acc: 0.4661\n",
      "[9/50] Train Loss: 0.6470, Val Loss: 0.7042, Val Acc: 0.4751\n",
      "[10/50] Train Loss: 0.6338, Val Loss: 0.7047, Val Acc: 0.4683\n",
      "[11/50] Train Loss: 0.6278, Val Loss: 0.7054, Val Acc: 0.4729\n",
      "[12/50] Train Loss: 0.6210, Val Loss: 0.7066, Val Acc: 0.4683\n",
      "[13/50] Train Loss: 0.6262, Val Loss: 0.7085, Val Acc: 0.4706\n",
      "[14/50] Train Loss: 0.6279, Val Loss: 0.7110, Val Acc: 0.4638\n",
      "[15/50] Train Loss: 0.6222, Val Loss: 0.7138, Val Acc: 0.4683\n",
      "[16/50] Train Loss: 0.6119, Val Loss: 0.7166, Val Acc: 0.4683\n",
      "[17/50] Train Loss: 0.6137, Val Loss: 0.7195, Val Acc: 0.4661\n",
      "[18/50] Train Loss: 0.6220, Val Loss: 0.7226, Val Acc: 0.4683\n",
      "[19/50] Train Loss: 0.6028, Val Loss: 0.7257, Val Acc: 0.4706\n",
      "[20/50] Train Loss: 0.6000, Val Loss: 0.7290, Val Acc: 0.4683\n",
      "[21/50] Train Loss: 0.6002, Val Loss: 0.7326, Val Acc: 0.4751\n",
      "[22/50] Train Loss: 0.5968, Val Loss: 0.7361, Val Acc: 0.4751\n",
      "[23/50] Train Loss: 0.6026, Val Loss: 0.7396, Val Acc: 0.4706\n",
      "[24/50] Train Loss: 0.5911, Val Loss: 0.7428, Val Acc: 0.4683\n",
      "[25/50] Train Loss: 0.6016, Val Loss: 0.7457, Val Acc: 0.4661\n",
      "[26/50] Train Loss: 0.5831, Val Loss: 0.7493, Val Acc: 0.4661\n",
      "[27/50] Train Loss: 0.5905, Val Loss: 0.7530, Val Acc: 0.4638\n",
      "[28/50] Train Loss: 0.5832, Val Loss: 0.7564, Val Acc: 0.4661\n",
      "[29/50] Train Loss: 0.5804, Val Loss: 0.7609, Val Acc: 0.4661\n",
      "[30/50] Train Loss: 0.5771, Val Loss: 0.7652, Val Acc: 0.4638\n",
      "[31/50] Train Loss: 0.5698, Val Loss: 0.7689, Val Acc: 0.4661\n",
      "[32/50] Train Loss: 0.5794, Val Loss: 0.7720, Val Acc: 0.4683\n",
      "[33/50] Train Loss: 0.5570, Val Loss: 0.7758, Val Acc: 0.4661\n",
      "[34/50] Train Loss: 0.5718, Val Loss: 0.7792, Val Acc: 0.4615\n",
      "[35/50] Train Loss: 0.5569, Val Loss: 0.7841, Val Acc: 0.4638\n",
      "[36/50] Train Loss: 0.5630, Val Loss: 0.7898, Val Acc: 0.4593\n",
      "[37/50] Train Loss: 0.5586, Val Loss: 0.7952, Val Acc: 0.4638\n",
      "[38/50] Train Loss: 0.5670, Val Loss: 0.8011, Val Acc: 0.4661\n",
      "[39/50] Train Loss: 0.5505, Val Loss: 0.8071, Val Acc: 0.4683\n",
      "[40/50] Train Loss: 0.5481, Val Loss: 0.8134, Val Acc: 0.4729\n",
      "[41/50] Train Loss: 0.5572, Val Loss: 0.8190, Val Acc: 0.4729\n",
      "[42/50] Train Loss: 0.5378, Val Loss: 0.8251, Val Acc: 0.4706\n",
      "[43/50] Train Loss: 0.5477, Val Loss: 0.8285, Val Acc: 0.4729\n",
      "[44/50] Train Loss: 0.5416, Val Loss: 0.8324, Val Acc: 0.4774\n",
      "[45/50] Train Loss: 0.5493, Val Loss: 0.8368, Val Acc: 0.4706\n",
      "[46/50] Train Loss: 0.5292, Val Loss: 0.8425, Val Acc: 0.4661\n",
      "[47/50] Train Loss: 0.5426, Val Loss: 0.8481, Val Acc: 0.4774\n",
      "[48/50] Train Loss: 0.5432, Val Loss: 0.8528, Val Acc: 0.4842\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-27 16:24:53,352] Trial 0 finished with value: 0.4841628968715668 and parameters: {'gat_out_features': 4, 'gat_heads': 2, 'gat_dropout': 0.18119751269098475, 'dim1': 32, 'dim2': 128, 'dim3': 32, 'k1': 9, 'k2': 5, 'k3': 5, 's1': 5, 's2': 3, 's3': 3, 'dropout': 0.20061806721053804, 'head_dropout': 0.0965352363231529, 'kernel_size': 11, 'decomposition': True, 'revin': True, 'affine': False}. Best is trial 0 with value: 0.4841628968715668.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[49/50] Train Loss: 0.5504, Val Loss: 0.8560, Val Acc: 0.4774\n",
      "[50/50] Train Loss: 0.5273, Val Loss: 0.8547, Val Acc: 0.4706\n",
      "✅ Best Trial:\n",
      "{'gat_out_features': 4, 'gat_heads': 2, 'gat_dropout': 0.18119751269098475, 'dim1': 32, 'dim2': 128, 'dim3': 32, 'k1': 9, 'k2': 5, 'k3': 5, 's1': 5, 's2': 3, 's3': 3, 'dropout': 0.20061806721053804, 'head_dropout': 0.0965352363231529, 'kernel_size': 11, 'decomposition': True, 'revin': True, 'affine': False}\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index out of range in self",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mIndexError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[17], line 99\u001B[0m\n\u001B[0;32m     95\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mkernel_size \u001B[38;5;241m=\u001B[39m best_params[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mkernel_size\u001B[39m\u001B[38;5;124m'\u001B[39m]\n\u001B[0;32m     98\u001B[0m model \u001B[38;5;241m=\u001B[39m GATCNModel(BestConfig())\n\u001B[1;32m---> 99\u001B[0m train_losses, val_losses, val_accs \u001B[38;5;241m=\u001B[39m \u001B[43mtrain_model\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrain_node_features\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43medge_index\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mX_train\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my_train\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mX_val\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my_val\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    100\u001B[0m \u001B[43m                                                 \u001B[49m\u001B[43mepochs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m75\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlr\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m1e-3\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m    102\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mno_grad():\n\u001B[0;32m    103\u001B[0m     pred_logits \u001B[38;5;241m=\u001B[39m model(val_node_features, edge_index)\u001B[38;5;241m.\u001B[39msqueeze(\u001B[38;5;241m0\u001B[39m)  \u001B[38;5;66;03m# [1, T] → [T]\u001B[39;00m\n",
      "Cell \u001B[1;32mIn[8], line 13\u001B[0m, in \u001B[0;36mtrain_model\u001B[1;34m(model, node_features, edge_index, X_train, y_train, X_val, y_val, epochs, lr, pos_weight)\u001B[0m\n\u001B[0;32m     11\u001B[0m model\u001B[38;5;241m.\u001B[39mtrain()\n\u001B[0;32m     12\u001B[0m optimizer\u001B[38;5;241m.\u001B[39mzero_grad()\n\u001B[1;32m---> 13\u001B[0m train_output \u001B[38;5;241m=\u001B[39m \u001B[43mmodel\u001B[49m\u001B[43m(\u001B[49m\u001B[43mnode_features\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43medge_index\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241m.\u001B[39msqueeze(\u001B[38;5;241m0\u001B[39m)[:\u001B[38;5;28mlen\u001B[39m(y_train)]  \u001B[38;5;66;03m# 통합 모델에 데이터를 적용한 결과\u001B[39;00m\n\u001B[0;32m     14\u001B[0m loss \u001B[38;5;241m=\u001B[39m criterion(train_output, y_train\u001B[38;5;241m.\u001B[39mfloat())\n\u001B[0;32m     15\u001B[0m loss\u001B[38;5;241m.\u001B[39mbackward()\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\myenv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1051\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *input, **kwargs)\u001B[0m\n\u001B[0;32m   1047\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1048\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1049\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1050\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1051\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1052\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[0;32m   1053\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "Cell \u001B[1;32mIn[7], line 9\u001B[0m, in \u001B[0;36mGATCNModel.forward\u001B[1;34m(self, x, edge_index)\u001B[0m\n\u001B[0;32m      8\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, x, edge_index):\n\u001B[1;32m----> 9\u001B[0m     embeddings \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgat\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43medge_index\u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# GAT를 통한 임베딩 생성\u001B[39;00m\n\u001B[0;32m     10\u001B[0m     tcn_input \u001B[38;5;241m=\u001B[39m embeddings\u001B[38;5;241m.\u001B[39munsqueeze(\u001B[38;5;241m0\u001B[39m)  \u001B[38;5;66;03m# TCN 입력 형태로 변환\u001B[39;00m\n\u001B[0;32m     11\u001B[0m     output \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtcn(tcn_input)  \u001B[38;5;66;03m# TCN으로 예측\u001B[39;00m\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\myenv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1051\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *input, **kwargs)\u001B[0m\n\u001B[0;32m   1047\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1048\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1049\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1050\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1051\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1052\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[0;32m   1053\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "Cell \u001B[1;32mIn[3], line 10\u001B[0m, in \u001B[0;36mGATLayer.forward\u001B[1;34m(self, x, edge_index)\u001B[0m\n\u001B[0;32m      9\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, x, edge_index):\n\u001B[1;32m---> 10\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgat\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43medge_index\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\myenv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1051\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *input, **kwargs)\u001B[0m\n\u001B[0;32m   1047\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1048\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1049\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1050\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1051\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1052\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[0;32m   1053\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\myenv\\lib\\site-packages\\torch_geometric\\nn\\conv\\gatv2_conv.py:235\u001B[0m, in \u001B[0;36mGATv2Conv.forward\u001B[1;34m(self, x, edge_index, edge_attr, return_attention_weights)\u001B[0m\n\u001B[0;32m    229\u001B[0m             \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mNotImplementedError\u001B[39;00m(\n\u001B[0;32m    230\u001B[0m                 \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mThe usage of \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124medge_attr\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m and \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124madd_self_loops\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    231\u001B[0m                 \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msimultaneously is currently not yet supported for \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    232\u001B[0m                 \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124medge_index\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m in a \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mSparseTensor\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m form\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m    234\u001B[0m \u001B[38;5;66;03m# propagate_type: (x: PairTensor, edge_attr: OptTensor)\u001B[39;00m\n\u001B[1;32m--> 235\u001B[0m out \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpropagate\u001B[49m\u001B[43m(\u001B[49m\u001B[43medge_index\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mx\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mx_l\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mx_r\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43medge_attr\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43medge_attr\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    236\u001B[0m \u001B[43m                     \u001B[49m\u001B[43msize\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[0;32m    238\u001B[0m alpha \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_alpha\n\u001B[0;32m    239\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_alpha \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\myenv\\lib\\site-packages\\torch_geometric\\nn\\conv\\message_passing.py:309\u001B[0m, in \u001B[0;36mMessagePassing.propagate\u001B[1;34m(self, edge_index, size, **kwargs)\u001B[0m\n\u001B[0;32m    306\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m arg \u001B[38;5;129;01min\u001B[39;00m decomp_args:\n\u001B[0;32m    307\u001B[0m         kwargs[arg] \u001B[38;5;241m=\u001B[39m decomp_kwargs[arg][i]\n\u001B[1;32m--> 309\u001B[0m coll_dict \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m__collect__\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m__user_args__\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43medge_index\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    310\u001B[0m \u001B[43m                             \u001B[49m\u001B[43msize\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    312\u001B[0m msg_kwargs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39minspector\u001B[38;5;241m.\u001B[39mdistribute(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mmessage\u001B[39m\u001B[38;5;124m'\u001B[39m, coll_dict)\n\u001B[0;32m    313\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m hook \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_message_forward_pre_hooks\u001B[38;5;241m.\u001B[39mvalues():\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\myenv\\lib\\site-packages\\torch_geometric\\nn\\conv\\message_passing.py:202\u001B[0m, in \u001B[0;36mMessagePassing.__collect__\u001B[1;34m(self, args, edge_index, size, kwargs)\u001B[0m\n\u001B[0;32m    200\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(data, Tensor):\n\u001B[0;32m    201\u001B[0m             \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m__set_size__(size, dim, data)\n\u001B[1;32m--> 202\u001B[0m             data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m__lift__\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdata\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43medge_index\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdim\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    204\u001B[0m         out[arg] \u001B[38;5;241m=\u001B[39m data\n\u001B[0;32m    206\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(edge_index, Tensor):\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\myenv\\lib\\site-packages\\torch_geometric\\nn\\conv\\message_passing.py:172\u001B[0m, in \u001B[0;36mMessagePassing.__lift__\u001B[1;34m(self, src, edge_index, dim)\u001B[0m\n\u001B[0;32m    170\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(edge_index, Tensor):\n\u001B[0;32m    171\u001B[0m     index \u001B[38;5;241m=\u001B[39m edge_index[dim]\n\u001B[1;32m--> 172\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43msrc\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mindex_select\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mnode_dim\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mindex\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    173\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(edge_index, SparseTensor):\n\u001B[0;32m    174\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m dim \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m1\u001B[39m:\n",
      "\u001B[1;31mIndexError\u001B[0m: index out of range in self"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "cell_type": "markdown",
   "source": [
    "3. 모델 생성 및 학습"
   ],
   "metadata": {
    "id": "XuoMYCoddnYg"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# BestConfig를 이용해 모델 설정 후 학습 :  여러 시행, 다양한 회사에서의 bestconfig 정보를 종합해 최적 파라미터 직접 설정 (아래는 예시)\n",
    "\n",
    "'''\n",
    "class BestConfig:\n",
    "    def __init__(self):\n",
    "        # GAT 설정\n",
    "        self.gat_out_features = 8\n",
    "        self.gat_heads = 4\n",
    "        self.gat_dropout = 0.2\n",
    "\n",
    "        # TCN 설정\n",
    "        self.enc_in = 8\n",
    "        self.dims = [8,16,32]\n",
    "        self.large_size = [7,5,7]\n",
    "        self.small_size = [5,1,1]\n",
    "        self.small_kernel_merged = False\n",
    "        self.dropout = 0.2\n",
    "        self.head_dropout = 0.25\n",
    "        self.revin = True\n",
    "        self.affine = False\n",
    "        self.decomposition = False\n",
    "        self.kernel_size = 31\n",
    "'''"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 73
    },
    "id": "NHiQbmBy1EIe",
    "outputId": "ad3aa8c6-1a3e-48bc-96b1-1505f73c0276"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "4. 예측 및 시각화 (best_params 기준으로 설정한 값)"
   ],
   "metadata": {
    "id": "IZzMcisGdqIl"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# from sklearn.metrics import classification_report, confusion_matrix\n",
    "#\n",
    "# print(classification_report(y_val.cpu(), pred_labels))\n",
    "# print(confusion_matrix(y_val.cpu(), pred_labels))"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HIg9asMg8vsg",
    "outputId": "d437479f-01c6-45e3-c38f-95aeb8c4b0aa"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "\n",
    "\n",
    "\"\"\"\n",
    "# 1. 실제 라벨\n",
    "true_labels = y_val.cpu().numpy()\n",
    "\n",
    "# 2. 다양한 threshold에 대해 f1-score 측정\n",
    "precisions, recalls, thresholds = precision_recall_curve(true_labels, pred_probs)\n",
    "\n",
    "f1s = 2 * (precisions * recalls) / (precisions + recalls + 1e-8)  # f1-score 계산\n",
    "best_idx = np.argmax(f1s)\n",
    "best_threshold = thresholds[best_idx]\n",
    "\n",
    "print(f\"✅ Best threshold by F1-score: {best_threshold:.4f}, F1: {f1s[best_idx]:.4f}\")\n",
    "\n",
    "# 3. 최적 threshold로 예측 라벨 생성\n",
    "pred_labels = (pred_probs > best_threshold).astype(int)\n",
    "# 6. 누적 수익률 (선택)\n",
    "# future_prices = close_prices[split+1:]  # 실제 수익률 계산용\n",
    "# visualize_cumulative_return(pred_probs, y_val.cpu().numpy(), future_prices)\n",
    "\"\"\"\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 73
    },
    "collapsed": true,
    "id": "FqU-FfFS7-OE",
    "outputId": "efce383a-255f-47b2-b9c4-39857d229915"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "weight_df = pd.DataFrame(index=df_[split+2:]['datadate'])\n",
    "for company_name in ['TSLA', 'NVDA', 'MSFT', 'GOOG', 'AAPL', 'DIS', 'XOM', 'CRM', 'INTC', 'AMZN']:\n",
    "    company_df = pd.read_csv(f'data/{company_name}.csv', index_col=0)['y_pred']\n",
    "    company_df.name = company_name\n",
    "    company_df.index = weight_df.index\n",
    "    weight_df = pd.concat([weight_df, company_df], axis=1)\n",
    "    row_sum = (weight_df == 1).sum(axis=1)\n",
    "    row_sum.replace(0, 0.1, inplace=True)\n",
    "    weight_df_1=weight_df.div(row_sum, axis=0)\n",
    "    weight_df_1.to_csv(f'data/GAT_TCN_weight.csv')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 455
    },
    "collapsed": true,
    "id": "vTpEBkXIHHmR",
    "outputId": "12720ec1-bc4f-4ddb-afbe-aa46ce4821fb"
   },
   "cell_type": "code",
   "source": [
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# from sklearn.metrics import f1_score\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "# from lightgbm import LGBMClassifier\n",
    "#\n",
    "#\n",
    "# def train_lgbm_model(X_train, y_train, X_val):\n",
    "#     model = LGBMClassifier(\n",
    "#         n_estimators=100,\n",
    "#         learning_rate=0.1,             # 너무 작으면 과적합/학습 지연\n",
    "#         verbosity=-1\n",
    "#     )\n",
    "#     model.fit(X_train, y_train)\n",
    "#\n",
    "#     pred_probs = model.predict_proba(X_val)[:, 1]\n",
    "#     pred_labels = (pred_probs > 0.5).astype(int)\n",
    "#\n",
    "#     return model, pred_probs, pred_labels\n",
    "#\n",
    "# # 가상의 node_features, labels, df_ 등이 정의되어 있어야 합니다.\n",
    "# # 위의 리팩토링은 함수 형태만 준비되어 있고, 본문 루프는 사용자가 가진 데이터 프레임 `df_`가 있어야 실행 가능\n",
    "#\n",
    "# def run_lgbm_pipeline(df_):\n",
    "#     results = []\n",
    "#     for company_name in ['TSLA', 'NVDA', 'MSFT', 'GOOG', 'AAPL', 'DIS', 'XOM', 'CRM', 'INTC', 'AMZN']:\n",
    "#         data = df_[\n",
    "#             [f'prccd_{company}' for company in\n",
    "#              ['TSLA', 'NVDA', 'MSFT', 'GOOG', 'AAPL', 'DIS', 'XOM', 'CRM', 'INTC', 'AMZN']] +\n",
    "#             [f'cshtrd_{company_name}', f'sent_{company_name}', 'datadate']].copy()\n",
    "#\n",
    "#         data.set_index('datadate', inplace=True)\n",
    "#         data.fillna(0, inplace=True)\n",
    "#         data.iloc[:, :10] = data.iloc[:, :10].pct_change()\n",
    "#         data = data.dropna()\n",
    "#         close_prices = data[f'prccd_{company_name}'].values\n",
    "#\n",
    "#         # ✅ 라벨: t+1 수익률 기준\n",
    "#         # returns = (close_prices[1:] / close_prices[:-1]) - 1\n",
    "#         labels = np.where(close_prices > 0.003, 1, 0)\n",
    "#\n",
    "#         # ✅ feature는 시점 t까지 (맨 마지막 row 제거)\n",
    "#         data_values = data.values\n",
    "#         scaler = StandardScaler()\n",
    "#         data_s = scaler.fit_transform(data_values[:, :-1])\n",
    "#         node_features = np.hstack([data_s, data_values[:, -1].reshape(-1, 1)])\n",
    "#\n",
    "#\n",
    "#         # ✅ train/val split\n",
    "#         X = node_features[:-1]\n",
    "#         y = labels[1:]\n",
    "#         split = int(len(X) * 0.8)\n",
    "#\n",
    "#         X_train = X[:split]\n",
    "#         X_val   = X[split:]\n",
    "#\n",
    "#         y_train = y[:split]\n",
    "#         y_val   = y[split:]\n",
    "#\n",
    "#         # ✅ 모델 학습 및 예측\n",
    "#         model, pred_probs, pred_labels = train_lgbm_model(X_train, y_train, X_val)\n",
    "#\n",
    "#         print(company_name)\n",
    "#         f1 = f1_score(y_val, pred_labels, average='macro')\n",
    "#         print(f\"F1 Score: {f1:.4f}\")\n",
    "#\n",
    "#         df_result = pd.DataFrame({'y_true': y_val, 'y_pred': pred_labels})\n",
    "#         df_result.to_csv(f'data/{company_name}_lgbm.csv', index=False)\n",
    "#\n",
    "#         results.append({\n",
    "#             \"company\": company_name,\n",
    "#             \"f1_score\": f1,\n",
    "#         })\n",
    "#\n",
    "#     return results"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "result=run_lgbm_pipeline(df_)\n",
    "result"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "weight_df = pd.DataFrame(index=df_[split+2:]['datadate'])\n",
    "for company_name in ['TSLA', 'NVDA', 'MSFT', 'GOOG', 'AAPL', 'DIS', 'XOM', 'CRM', 'INTC', 'AMZN']:\n",
    "    company_df = pd.read_csv(f'data/{company_name}_lgbm.csv', index_col=0)['y_pred']\n",
    "    company_df.name = company_name\n",
    "    company_df.index = weight_df.index\n",
    "    weight_df = pd.concat([weight_df, company_df], axis=1)\n",
    "    row_sum = (weight_df == 1).sum(axis=1)\n",
    "    row_sum.replace(0, 0.1, inplace=True)\n",
    "    weight_df_1=weight_df.div(row_sum, axis=0)\n",
    "    weight_df_1.to_csv(f'data/LGBM_weight.csv')"
   ],
   "outputs": [],
   "execution_count": null
  }
 ]
}

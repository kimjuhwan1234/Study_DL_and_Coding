{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BObOC16LAlS4"
   },
   "source": [
    "# 모듈 설치 및 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/research/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.9.0+cu111\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__version__)           # 1.9.0+cu111\n",
    "print(torch.cuda.is_available())   # True\n",
    "\n",
    "from torch_geometric.nn import GATv2Conv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-27T07:21:22.856695Z",
     "start_time": "2025-05-27T07:21:18.195100Z"
    },
    "id": "XgRsALeSVcJX"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.environ['LD_LIBRARY_PATH'] = '/usr/local/cuda-11.1/targets/x86_64-linux/lib:' + os.environ.get('LD_LIBRARY_PATH', '')\n",
    "import gc\n",
    "\n",
    "import torch\n",
    "import optuna\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from os import path as pt\n",
    "from lib.utils import pickle_it\n",
    "from sklearn.metrics import f1_score\n",
    "from torch_geometric.nn import GATv2Conv\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XyHQgQwjAiTk"
   },
   "source": [
    "# GAT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m6DEYVqJC0_l"
   },
   "source": [
    "- 노드 : 각 시점\n",
    "- 피처 : 10개 주식 종가 + 예측 기업 거래량 + 예측 기업 감정분석\n",
    "- GAT의 역할 : 피처들의 관계(회사 간의 관계 등)을 파악해 **시점별** 임베딩 생성;  다른 시점과의 연관성을 반영    \n",
    "(ex. 1~10일 전과 연결이 되어있는 상태에서, 1일 전 정보는 얼마나 중요하고 10일 전 정보는 얼마나 중요한지 판단)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-27T07:21:24.230549Z",
     "start_time": "2025-05-27T07:21:24.215814Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 91
    },
    "id": "GegpQ78WROjC",
    "outputId": "7cc16100-9a36-400c-caf2-5b009a27f982"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nclass GAT(nn.Module):\\n    def __init__(self, in_features, out_features, heads, dropout, concat=False):\\n        super(GAT, self).__init__()\\n\\n        self.in_features = in_features\\n        self.out_features = out_features\\n        self.heads = heads\\n        self.dropout = dropout\\n        self.concat = concat\\n\\n        # 선형 변환을 위한 가중치 행렬\\n        self.lin = nn.Linear(in_features, heads * out_features, bias=False)\\n\\n        # 어텐션 계수 계산을 위한 가중치\\n        self.att = nn.Parameter(torch.Tensor(1, heads, out_features))\\n\\n        # 바이어스\\n        self.bias = nn.Parameter(torch.Tensor(out_features))\\n\\n        # LeakyReLU\\n        self.leakyrelu = nn.LeakyReLU(0.2)\\n\\n        # 드롭아웃\\n        self.dropout_layer = nn.Dropout(dropout)\\n\\n        # 초기화\\n        self.reset_parameters()\\n\\n    def reset_parameters(self):\\n        # 가중치 초기화\\n        gain = nn.init.calculate_gain(\\'relu\\')\\n        nn.init.xavier_normal_(self.lin.weight, gain=gain)\\n        nn.init.xavier_normal_(self.att, gain=gain)\\n        nn.init.zeros_(self.bias)\\n\\n    def forward(self, x, edge_index):\\n        \"\"\"\\n        x: 노드 특성 [N, in_features]\\n        edge_index: 엣지 인덱스 [2, E]\\n        \"\"\"\\n        N = x.size(0)  # 노드 수\\n\\n        # 1. 선형 변환\\n        x = self.dropout_layer(x)\\n        x = self.lin(x)  # [N, heads * out_features]\\n        x = x.view(N, self.heads, self.out_features)  # [N, heads, out_features]\\n\\n        # 2. 엣지 리스트로부터 어텐션 계산\\n        edge_src, edge_dst = edge_index[0], edge_index[1]\\n\\n        # 3. GATv2 : 선형변환 후 어텐션\\n        # 각 노드에 어텐션 가중치 적용\\n        x_att = x * self.att  # [N, heads, out_features]\\n\\n        # 이웃 노드 쌍의 어텐션 점수 계산\\n        alpha_src = x_att[edge_src].sum(dim=-1)  # [E, heads]\\n        alpha_dst = x_att[edge_dst].sum(dim=-1)  # [E, heads]\\n        alpha = alpha_src + alpha_dst  # [E, heads]\\n        alpha = self.leakyrelu(alpha)  # [E, heads]\\n\\n        # 4. 소프트맥스로 정규화\\n        alpha = self._edge_softmax(alpha, edge_index[1], N)\\n        alpha = self.dropout_layer(alpha)\\n\\n        # 5. 메시지 집계\\n        out = torch.zeros(N, self.heads, self.out_features, device=x.device)\\n\\n        # 각 엣지에 대해 메시지 전달\\n        for i in range(edge_index.size(1)):\\n            src, dst = edge_index[0, i], edge_index[1, i]\\n            out[dst] += alpha[i].unsqueeze(-1) * x[src]\\n\\n        # 6. 최종 출력 형태 결정\\n        if self.concat:\\n            out = out.view(N, self.heads * self.out_features)\\n        else:\\n            out = out.mean(dim=1)  # 헤드 간 평균\\n\\n        # 7. 바이어스 추가\\n        out = out + self.bias\\n\\n        return out\\n\\n    def _edge_softmax(self, alpha, target_nodes, num_nodes):\\n\\n        norm_alpha = torch.zeros_like(alpha)\\n\\n\\n        for i in range(num_nodes):\\n            # 현재 노드로 향하는 엣지 마스크\\n            mask = (target_nodes == i)\\n            if mask.sum() > 0:\\n                # 해당 노드로 향하는 엣지에 대해 소프트맥스 적용\\n                norm_alpha[mask] = F.softmax(alpha[mask], dim=0)\\n\\n        return norm_alpha\\n '"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "class GAT(nn.Module):\n",
    "    def __init__(self, in_features, out_features, heads, dropout, concat=False):\n",
    "        super(GAT, self).__init__()\n",
    "\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.heads = heads\n",
    "        self.dropout = dropout\n",
    "        self.concat = concat\n",
    "\n",
    "        # 선형 변환을 위한 가중치 행렬\n",
    "        self.lin = nn.Linear(in_features, heads * out_features, bias=False)\n",
    "\n",
    "        # 어텐션 계수 계산을 위한 가중치\n",
    "        self.att = nn.Parameter(torch.Tensor(1, heads, out_features))\n",
    "\n",
    "        # 바이어스\n",
    "        self.bias = nn.Parameter(torch.Tensor(out_features))\n",
    "\n",
    "        # LeakyReLU\n",
    "        self.leakyrelu = nn.LeakyReLU(0.2)\n",
    "\n",
    "        # 드롭아웃\n",
    "        self.dropout_layer = nn.Dropout(dropout)\n",
    "\n",
    "        # 초기화\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        # 가중치 초기화\n",
    "        gain = nn.init.calculate_gain('relu')\n",
    "        nn.init.xavier_normal_(self.lin.weight, gain=gain)\n",
    "        nn.init.xavier_normal_(self.att, gain=gain)\n",
    "        nn.init.zeros_(self.bias)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        \"\"\"\n",
    "        x: 노드 특성 [N, in_features]\n",
    "        edge_index: 엣지 인덱스 [2, E]\n",
    "        \"\"\"\n",
    "        N = x.size(0)  # 노드 수\n",
    "\n",
    "        # 1. 선형 변환\n",
    "        x = self.dropout_layer(x)\n",
    "        x = self.lin(x)  # [N, heads * out_features]\n",
    "        x = x.view(N, self.heads, self.out_features)  # [N, heads, out_features]\n",
    "\n",
    "        # 2. 엣지 리스트로부터 어텐션 계산\n",
    "        edge_src, edge_dst = edge_index[0], edge_index[1]\n",
    "\n",
    "        # 3. GATv2 : 선형변환 후 어텐션\n",
    "        # 각 노드에 어텐션 가중치 적용\n",
    "        x_att = x * self.att  # [N, heads, out_features]\n",
    "\n",
    "        # 이웃 노드 쌍의 어텐션 점수 계산\n",
    "        alpha_src = x_att[edge_src].sum(dim=-1)  # [E, heads]\n",
    "        alpha_dst = x_att[edge_dst].sum(dim=-1)  # [E, heads]\n",
    "        alpha = alpha_src + alpha_dst  # [E, heads]\n",
    "        alpha = self.leakyrelu(alpha)  # [E, heads]\n",
    "\n",
    "        # 4. 소프트맥스로 정규화\n",
    "        alpha = self._edge_softmax(alpha, edge_index[1], N)\n",
    "        alpha = self.dropout_layer(alpha)\n",
    "\n",
    "        # 5. 메시지 집계\n",
    "        out = torch.zeros(N, self.heads, self.out_features, device=x.device)\n",
    "\n",
    "        # 각 엣지에 대해 메시지 전달\n",
    "        for i in range(edge_index.size(1)):\n",
    "            src, dst = edge_index[0, i], edge_index[1, i]\n",
    "            out[dst] += alpha[i].unsqueeze(-1) * x[src]\n",
    "\n",
    "        # 6. 최종 출력 형태 결정\n",
    "        if self.concat:\n",
    "            out = out.view(N, self.heads * self.out_features)\n",
    "        else:\n",
    "            out = out.mean(dim=1)  # 헤드 간 평균\n",
    "\n",
    "        # 7. 바이어스 추가\n",
    "        out = out + self.bias\n",
    "\n",
    "        return out\n",
    "\n",
    "    def _edge_softmax(self, alpha, target_nodes, num_nodes):\n",
    "\n",
    "        norm_alpha = torch.zeros_like(alpha)\n",
    "\n",
    "\n",
    "        for i in range(num_nodes):\n",
    "            # 현재 노드로 향하는 엣지 마스크\n",
    "            mask = (target_nodes == i)\n",
    "            if mask.sum() > 0:\n",
    "                # 해당 노드로 향하는 엣지에 대해 소프트맥스 적용\n",
    "                norm_alpha[mask] = F.softmax(alpha[mask], dim=0)\n",
    "\n",
    "        return norm_alpha\n",
    " '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-27T07:21:24.649299Z",
     "start_time": "2025-05-27T07:21:24.635619Z"
    },
    "id": "ZRWMJA0G_U2x"
   },
   "outputs": [],
   "source": [
    "# GAT 레이어 정의\n",
    "\n",
    "class GATLayer(nn.Module):\n",
    "    def __init__(self, in_features, out_features, heads, dropout):\n",
    "        super().__init__()\n",
    "        self.gat = GATv2Conv(in_features, out_features, heads=heads, dropout=dropout, concat=False)  # PyG 사용\n",
    "        # self.gat = GAT(in_features, out_features, heads=heads, dropout=dropout, concat=False)            # 상단 구현 코드 (느리고, 느려서 optuna 중간에 끊고 첫번째걸로 해봤는데 ----로 예측됨;;;;)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        return self.gat(x, edge_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2_Ee7Aj6A3K_"
   },
   "source": [
    "# TCN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-27T07:21:25.530783Z",
     "start_time": "2025-05-27T07:21:25.499092Z"
    },
    "id": "Ky1M6aXiA47i"
   },
   "outputs": [],
   "source": [
    "# --- 유틸 함수 ---\n",
    "def get_conv1d(in_channels, out_channels, kernel_size, stride, padding, dilation, groups, bias):\n",
    "    return nn.Conv1d(in_channels=in_channels, out_channels=out_channels,\n",
    "                     kernel_size=kernel_size, stride=stride,\n",
    "                     padding=padding, dilation=dilation,\n",
    "                     groups=groups, bias=bias)\n",
    "\n",
    "\n",
    "def get_bn(channels):\n",
    "    return nn.BatchNorm1d(channels)\n",
    "\n",
    "\n",
    "def conv_bn(in_channels, out_channels, kernel_size, stride, padding, groups, dilation=1, bias=False):\n",
    "    if padding is None:\n",
    "        padding = kernel_size // 2\n",
    "    result = nn.Sequential()\n",
    "    result.add_module('conv',\n",
    "                      get_conv1d(in_channels, out_channels, kernel_size, stride, padding, dilation, groups, bias))\n",
    "    result.add_module('bn', get_bn(out_channels))\n",
    "    return result\n",
    "\n",
    "\n",
    "# --- RevIN ---\n",
    "class RevIN(nn.Module):\n",
    "    def __init__(self, num_features: int, eps=1e-5, affine=True, subtract_last=False):\n",
    "        super().__init__()\n",
    "        self.num_features = num_features\n",
    "        self.eps = eps\n",
    "        self.affine = affine\n",
    "        self.subtract_last = subtract_last\n",
    "        if self.affine:\n",
    "            self._init_params()\n",
    "\n",
    "    def _init_params(self):\n",
    "        self.affine_weight = nn.Parameter(torch.ones(self.num_features))\n",
    "        self.affine_bias = nn.Parameter(torch.zeros(self.num_features))\n",
    "\n",
    "    def forward(self, x, mode: str):\n",
    "        if mode == 'norm':\n",
    "            self._get_statistics(x)\n",
    "            x = self._normalize(x)\n",
    "        elif mode == 'denorm':\n",
    "            x = self._denormalize(x)\n",
    "        return x\n",
    "\n",
    "    def _get_statistics(self, x):\n",
    "        dim2reduce = tuple(range(1, x.ndim - 1))\n",
    "        if self.subtract_last:\n",
    "            self.last = x[:, -1:, :].unsqueeze(1)\n",
    "        else:\n",
    "            self.mean = torch.mean(x, dim=dim2reduce, keepdim=True).detach()\n",
    "        self.stdev = torch.sqrt(torch.var(x, dim=dim2reduce, keepdim=True, unbiased=False) + self.eps).detach()\n",
    "\n",
    "    def _normalize(self, x):\n",
    "        x = (x - self.mean) / self.stdev\n",
    "        if self.affine:\n",
    "            x = x * self.affine_weight[None, None, :] + self.affine_bias[None, None, :]\n",
    "        return x\n",
    "\n",
    "    def _denormalize(self, x):\n",
    "        if self.affine:\n",
    "            x = (x - self.affine_bias[None, None, :]) / self.affine_weight[None, None, :]\n",
    "        x = x * self.stdev + self.mean\n",
    "        return x\n",
    "\n",
    "\n",
    "# --- 시계열 분해 ---\n",
    "class moving_avg(nn.Module):\n",
    "    def __init__(self, kernel_size, stride):\n",
    "        super().__init__()\n",
    "        self.kernel_size = kernel_size\n",
    "        self.avg = nn.AvgPool1d(kernel_size=kernel_size, stride=stride, padding=0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        front = x[:, 0:1, :].repeat(1, (self.kernel_size - 1) // 2, 1)\n",
    "        end = x[:, -1:, :].repeat(1, (self.kernel_size - 1) // 2, 1)\n",
    "        x = torch.cat([front, x, end], dim=1)\n",
    "        x = self.avg(x.permute(0, 2, 1))\n",
    "        return x.permute(0, 2, 1)\n",
    "\n",
    "\n",
    "class series_decomp(nn.Module):\n",
    "    def __init__(self, kernel_size):\n",
    "        super().__init__()\n",
    "        self.moving_avg = moving_avg(kernel_size, stride=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        moving_mean = self.moving_avg(x)\n",
    "        return x - moving_mean, moving_mean\n",
    "\n",
    "\n",
    "# --- 커스텀 커널 ---\n",
    "class ReparamLargeKernelConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride, groups, small_kernel, small_kernel_merged=False):\n",
    "        super().__init__()\n",
    "        self.kernel_size = kernel_size\n",
    "        self.small_kernel = small_kernel\n",
    "        padding = kernel_size // 2\n",
    "        if small_kernel_merged:\n",
    "            self.lkb_reparam = nn.Conv1d(in_channels, out_channels, kernel_size, stride, padding, groups=groups,\n",
    "                                         bias=True)\n",
    "        else:\n",
    "            self.lkb_origin = conv_bn(in_channels, out_channels, kernel_size, stride, padding, groups)\n",
    "            if small_kernel is not None:\n",
    "                self.small_conv = conv_bn(in_channels, out_channels, small_kernel, stride, small_kernel // 2, groups)\n",
    "\n",
    "    def forward(self, x):\n",
    "        if hasattr(self, 'lkb_reparam'):\n",
    "            return self.lkb_reparam(x)\n",
    "        out = self.lkb_origin(x)\n",
    "        if hasattr(self, 'small_conv'):\n",
    "            out += self.small_conv(x)\n",
    "        return out\n",
    "\n",
    "\n",
    "# --- 출력층 ---\n",
    "class Flatten_Head(nn.Module):\n",
    "    def __init__(self, d_model):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(d_model, 1)\n",
    "\n",
    "    def forward(self, x):  # x: [B, C, T]\n",
    "        x = x.permute(0, 2, 1)  # → [B, T, C]\n",
    "        x = self.linear(x)  # → [B, T, 1]\n",
    "        return x.squeeze(-1)  # → [B, T]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-27T07:21:25.857271Z",
     "start_time": "2025-05-27T07:21:25.843612Z"
    },
    "id": "eySi1894CGFo"
   },
   "outputs": [],
   "source": [
    "# --- ModernTCN 모델 ---\n",
    "class ModernTCN(nn.Module):\n",
    "    def __init__(self, configs):\n",
    "        super().__init__()\n",
    "        self.revin = RevIN(configs.enc_in, affine=configs.affine) if configs.revin else None\n",
    "        self.decomp = series_decomp(configs.kernel_size) if configs.decomposition else None\n",
    "\n",
    "        self.conv_layers = nn.ModuleList()\n",
    "        self.norm_layers = nn.ModuleList()\n",
    "\n",
    "        c_in = configs.enc_in\n",
    "        for i in range(len(configs.dims)):\n",
    "            conv = ReparamLargeKernelConv(c_in, configs.dims[i],\n",
    "                                          kernel_size=configs.large_size[i],\n",
    "                                          stride=1,\n",
    "                                          groups=1,\n",
    "                                          small_kernel=configs.small_size[i],\n",
    "                                          small_kernel_merged=configs.small_kernel_merged)\n",
    "            self.conv_layers.append(conv)\n",
    "            self.norm_layers.append(nn.BatchNorm1d(configs.dims[i]))\n",
    "            c_in = configs.dims[i]\n",
    "\n",
    "        self.head = Flatten_Head(configs.dims[-1])\n",
    "\n",
    "    def forward(self, x):  # x: [B, T, C]\n",
    "        if self.revin:\n",
    "            x = self.revin(x, 'norm')\n",
    "        if self.decomp:\n",
    "            x, _ = self.decomp(x)\n",
    "        x = x.permute(0, 2, 1)  # [B, C, T]\n",
    "        for conv, norm in zip(self.conv_layers, self.norm_layers):\n",
    "            x = conv(x)\n",
    "            x = norm(x)\n",
    "            x = F.relu(x)\n",
    "        out = self.head(x)  # [B, T]\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-27T07:21:26.386089Z",
     "start_time": "2025-05-27T07:21:26.367752Z"
    },
    "id": "Bqr3BMHdCEI6"
   },
   "outputs": [],
   "source": [
    "# --- Config 클래스 ---\n",
    "class Configs:\n",
    "    def __init__(self, enc_in):\n",
    "        self.enc_in = enc_in\n",
    "        self.dims = [8, 16, 32]\n",
    "        self.large_size = [5, 5, 3]\n",
    "        self.small_size = [5, 3, 3]\n",
    "        self.small_kernel_merged = False\n",
    "        self.dropout = 0.1\n",
    "        self.head_dropout = 0.2\n",
    "        self.revin = True\n",
    "        self.affine = True\n",
    "        self.decomposition = True\n",
    "        self.kernel_size = 25"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hC824c4hBPKK"
   },
   "source": [
    "# GAT-TCN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-27T07:21:27.304710Z",
     "start_time": "2025-05-27T07:21:27.289466Z"
    },
    "id": "nu0GYmnkBOtr"
   },
   "outputs": [],
   "source": [
    "class GATCNModel(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.gat = GATLayer(node_features.shape[1], config.gat_out_features,\n",
    "                            config.gat_heads, config.gat_dropout)  # GAT\n",
    "        self.tcn = ModernTCN(config)  # TCN\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        embeddings = self.gat(x, edge_index)  # GAT를 통한 임베딩 생성\n",
    "        tcn_input = embeddings.unsqueeze(0)  # TCN 입력 형태로 변환\n",
    "        output = self.tcn(tcn_input)  # TCN으로 예측\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-27T07:21:27.963107Z",
     "start_time": "2025-05-27T07:21:27.952437Z"
    },
    "id": "F5pcxCD-EjI4"
   },
   "outputs": [],
   "source": [
    "\n",
    "# GAT-TCN 모델의 최적 파라미터 탐색\n",
    "\n",
    "def train_model(model, node_features, edge_index, X_train, y_train, X_val, y_val, epochs=30, lr=1e-3, pos_weight=None):\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    #criterion = nn.BCEWithLogitsLoss()\n",
    "    criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight) if pos_weight is not None else nn.BCEWithLogitsLoss()\n",
    "    train_losses, val_losses, val_accs = [], [], []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        # 1.학습 단계\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        train_output = model(node_features, edge_index).squeeze(0)[:len(y_train)]  # 통합 모델에 데이터를 적용한 결과\n",
    "        loss = criterion(train_output, y_train.float())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_losses.append(loss.item())\n",
    "\n",
    "        # 2. 검증 단계\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_output = model(node_features, edge_index).squeeze(0)[-len(y_val):]\n",
    "            val_loss = criterion(val_output, y_val.float()).item()\n",
    "            pred = (torch.sigmoid(val_output) > 0.5).int()\n",
    "            acc = (pred == y_val).float().mean().item()\n",
    "\n",
    "        val_losses.append(val_loss)\n",
    "        val_accs.append(acc)\n",
    "\n",
    "        print(f\"[{epoch + 1}/{epochs}] Train Loss: {loss.item():.4f}, Val Loss: {val_loss:.4f}, Val Acc: {acc:.4f}\")\n",
    "\n",
    "    return train_losses, val_losses, val_accs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-27T07:21:28.694110Z",
     "start_time": "2025-05-27T07:21:28.677429Z"
    },
    "id": "Ri6Us2oFMJlq"
   },
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    # GAT\n",
    "    gat_out_features = trial.suggest_categorical(\"gat_out_features\", [4, 8, 12])  # 기존 features 수보다는 적은 것이 적합\n",
    "    gat_heads = trial.suggest_categorical(\"gat_heads\", [1, 2, 4, 8])\n",
    "    gat_dropout = trial.suggest_float(\"gat_dropout\", 0.0, 0.3)\n",
    "\n",
    "    # TCN\n",
    "    dims = [\n",
    "        trial.suggest_categorical(\"dim1\", [8, 16, 32, 64]),\n",
    "        trial.suggest_categorical(\"dim2\", [16, 32, 64, 128]),\n",
    "        trial.suggest_categorical(\"dim3\", [32, 64, 128, 256])\n",
    "    ]\n",
    "    large_size = [\n",
    "        trial.suggest_categorical(\"k1\", [3, 5, 7, 9, 11]),\n",
    "        trial.suggest_categorical(\"k2\", [3, 5, 7, 9]),\n",
    "        trial.suggest_categorical(\"k3\", [3, 5, 7])\n",
    "    ]\n",
    "    small_size = [\n",
    "        trial.suggest_categorical(\"s1\", [1, 3, 5]),\n",
    "        trial.suggest_categorical(\"s2\", [1, 3]),\n",
    "        trial.suggest_categorical(\"s3\", [1, 3])\n",
    "    ]\n",
    "    dropout = trial.suggest_float(\"dropout\", 0.0, 0.3)\n",
    "    head_dropout = trial.suggest_float(\"head_dropout\", 0.0, 0.3)\n",
    "    kernel_size = trial.suggest_categorical(\"kernel_size\", [5, 11, 15, 25, 31])\n",
    "    decomposition = trial.suggest_categorical(\"decomposition\", [True, False])\n",
    "    revin = trial.suggest_categorical(\"revin\", [True, False])\n",
    "    affine = trial.suggest_categorical(\"affine\", [True, False])\n",
    "\n",
    "    # 통합\n",
    "    class TrialConfig:\n",
    "        def __init__(self):\n",
    "            self.gat_out_features = gat_out_features\n",
    "            self.gat_heads = gat_heads\n",
    "            self.gat_dropout = gat_dropout\n",
    "\n",
    "            self.enc_in = gat_out_features\n",
    "            self.dims = dims\n",
    "            self.large_size = large_size\n",
    "            self.small_size = small_size\n",
    "            self.small_kernel_merged = False\n",
    "            self.dropout = dropout\n",
    "            self.head_dropout = head_dropout\n",
    "            self.revin = revin\n",
    "            self.affine = affine\n",
    "            self.decomposition = decomposition\n",
    "            self.kernel_size = kernel_size\n",
    "\n",
    "    model = GATCNModel(TrialConfig())\n",
    "\n",
    "    # Accuracy 기준 최적화 #####\n",
    "    _, _, val_accs = train_model(model, node_features, edge_index, X_train, y_train, X_val, y_val, epochs=50)\n",
    "    return max(val_accs)\n",
    "    ############################\n",
    "\n",
    "    # F1 Score 기준 최적화 #####################\n",
    "    # pos_weight 계산 (불균형 데이터 보정)\n",
    "    '''\n",
    "    pos_weight = torch.tensor([(y_train == 0).sum() / (y_train == 1).sum()]).to(y_train.device)\n",
    "\n",
    "    train_model(model, node_features, edge_index, X_train, y_train, X_val, y_val, epochs=15, pos_weight=pos_weight)\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        full_pred = model(node_features, edge_index).squeeze(0)\n",
    "        pred = full_pred[-len(y_val):]\n",
    "        probs = torch.sigmoid(pred).cpu().numpy()\n",
    "        preds = (probs > 0.5).astype(int)\n",
    "\n",
    "    y_true = y_val.cpu().numpy()\n",
    "    return f1_score(y_true, preds)\n",
    "    '''\n",
    "    ##########################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e6IIhl_Vo5x9"
   },
   "source": [
    "# 시각화 코드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-27T07:21:30.155627Z",
     "start_time": "2025-05-27T07:21:30.133289Z"
    },
    "id": "nj38FLOMtL84"
   },
   "outputs": [],
   "source": [
    "def visualize_training(company_name, train_losses, val_losses, val_accs):\n",
    "    plt.figure(figsize=(12, 2), dpi=400)\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(train_losses, label='Train Loss')\n",
    "    plt.plot(val_losses, label='Val Loss')\n",
    "    plt.legend()\n",
    "    plt.title(\"Loss over Epochs\")\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(val_accs, label='Val Accuracy')\n",
    "    plt.legend()\n",
    "    plt.title(\"Validation Accuracy\")\n",
    "    plt.savefig(f'data/images/{company_name}_loss.png')\n",
    "    plt.close()\n",
    "\n",
    "from matplotlib.patches import Patch\n",
    "\n",
    "def visualize_match_only(company_name, pred_probs, true_labels, threshold=0.5):\n",
    "    # 이진 예측\n",
    "    pred_labels = (pred_probs >= threshold).astype(int)\n",
    "\n",
    "    # 정답과 예측이 일치하면 1, 다르면 0\n",
    "    match = (pred_labels == true_labels).astype(int)\n",
    "\n",
    "    # 시각화\n",
    "    plt.figure(figsize=(12, 2), dpi=400)\n",
    "    bar_heights = np.ones_like(match)\n",
    "    bar_colors = ['green' if m else 'red' for m in match]\n",
    "    plt.bar(np.arange(len(match)), bar_heights,\n",
    "            color=bar_colors,\n",
    "            width=1.0)\n",
    "\n",
    "    plt.title(f\"{company_name} - Prediction Match\")\n",
    "    plt.ylabel('Match')\n",
    "    plt.xlabel('Time')\n",
    "    plt.yticks([0, 1], ['Wrong', 'Correct'])\n",
    "\n",
    "    # ✅ 범례 추가\n",
    "    legend_elements = [\n",
    "        Patch(facecolor='green', label='Correct'),\n",
    "        Patch(facecolor='red', label='Wrong')\n",
    "    ]\n",
    "    plt.legend(handles=legend_elements, loc='upper right')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'data/images/{company_name}_match_only.png')\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def visualize_cumulative_return(pred_probs, true_labels, prices):\n",
    "    signal = (pred_probs > 0.5).astype(int)\n",
    "    returns = (prices[1:] / prices[:-1]) - 1\n",
    "    strategy_returns = returns * signal[:-1]\n",
    "\n",
    "    cumulative = (strategy_returns + 1).cumprod()\n",
    "    market = (returns + 1).cumprod()\n",
    "\n",
    "    plt.plot(cumulative, label='Strategy')\n",
    "    plt.plot(market, label='Market (buy & hold)')\n",
    "    plt.legend();\n",
    "    plt.title(\"Cumulative Return\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2gL9v1aBFqxZ"
   },
   "source": [
    "# GAT-TCN 적용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-27T07:21:31.619843Z",
     "start_time": "2025-05-27T07:21:31.601672Z"
    },
    "id": "YmnyK8iVm3nZ"
   },
   "outputs": [],
   "source": [
    "# seed 설정\n",
    "import random\n",
    "\n",
    "\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.use_deterministic_algorithms(True)\n",
    "\n",
    "\n",
    "set_seed(42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZkK07jzudiAx"
   },
   "source": [
    "1. 데이터셋 불러오기 및 라벨 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-27T07:21:35.780028Z",
     "start_time": "2025-05-27T07:21:35.752636Z"
    }
   },
   "outputs": [],
   "source": [
    "df_ = pd.read_csv(\"./data/daily_all.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-27T07:21:36.172012Z",
     "start_time": "2025-05-27T07:21:36.158361Z"
    }
   },
   "outputs": [],
   "source": [
    "# company_name =['TSLA', 'NVDA', 'MSFT', 'GOOG', 'AAPL', 'DIS', 'XOM', 'CRM', 'INTC', 'AMZN'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-27T07:25:09.482611Z",
     "start_time": "2025-05-27T07:24:12.708918Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 455
    },
    "collapsed": true,
    "id": "vTpEBkXIHHmR",
    "outputId": "12720ec1-bc4f-4ddb-afbe-aa46ce4821fb"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-07-07 18:23:10,138] A new study created in memory with name: no-name-5a4e0113-70e3-46a6-8362-e890a6a1b6b8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2210 553 2210\n",
      "[1/50] Train Loss: 0.6972, Val Loss: 0.6995, Val Acc: 0.4548\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W 2025-07-07 18:23:16,294] Trial 0 failed with parameters: {'gat_out_features': 8, 'gat_heads': 4, 'gat_dropout': 0.06229555376312562, 'dim1': 64, 'dim2': 32, 'dim3': 128, 'k1': 5, 'k2': 9, 'k3': 7, 's1': 3, 's2': 3, 's3': 3, 'dropout': 0.03591070747826878, 'head_dropout': 0.01317070291432486, 'kernel_size': 11, 'decomposition': False, 'revin': False, 'affine': True} because of the following error: KeyboardInterrupt().\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/miniconda3/envs/research/lib/python3.9/site-packages/optuna/study/_optimize.py\", line 201, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"/tmp/ipykernel_25678/4135078835.py\", line 52, in objective\n",
      "    _, _, val_accs = train_model(model, node_features, edge_index, X_train, y_train, X_val, y_val, epochs=50)\n",
      "  File \"/tmp/ipykernel_25678/1244807327.py\", line 15, in train_model\n",
      "    loss.backward()\n",
      "  File \"/opt/miniconda3/envs/research/lib/python3.9/site-packages/torch/_tensor.py\", line 255, in backward\n",
      "    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)\n",
      "  File \"/opt/miniconda3/envs/research/lib/python3.9/site-packages/torch/autograd/__init__.py\", line 147, in backward\n",
      "    Variable._execution_engine.run_backward(\n",
      "KeyboardInterrupt\n",
      "[W 2025-07-07 18:23:16,305] Trial 0 failed with value None.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 66\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;66;03m# Optuna 튜닝 실행\u001b[39;00m\n\u001b[1;32m     65\u001b[0m study \u001b[38;5;241m=\u001b[39m optuna\u001b[38;5;241m.\u001b[39mcreate_study(direction\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmaximize\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 66\u001b[0m \u001b[43mstudy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobjective\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     67\u001b[0m \u001b[38;5;66;03m# 최적 하이퍼파라미터 출력\u001b[39;00m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m✅ Best Trial:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/miniconda3/envs/research/lib/python3.9/site-packages/optuna/study/study.py:489\u001b[0m, in \u001b[0;36mStudy.optimize\u001b[0;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m    387\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21moptimize\u001b[39m(\n\u001b[1;32m    388\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    389\u001b[0m     func: ObjectiveFuncType,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    396\u001b[0m     show_progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    397\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    398\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Optimize an objective function.\u001b[39;00m\n\u001b[1;32m    399\u001b[0m \n\u001b[1;32m    400\u001b[0m \u001b[38;5;124;03m    Optimization is done by choosing a suitable set of hyperparameter values from a given\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    487\u001b[0m \u001b[38;5;124;03m            If nested invocation of this method occurs.\u001b[39;00m\n\u001b[1;32m    488\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 489\u001b[0m     \u001b[43m_optimize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    490\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstudy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    491\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    492\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    493\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    494\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    495\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcatch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mIterable\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    496\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    497\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    498\u001b[0m \u001b[43m        \u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshow_progress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    499\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/miniconda3/envs/research/lib/python3.9/site-packages/optuna/study/_optimize.py:64\u001b[0m, in \u001b[0;36m_optimize\u001b[0;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     63\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m---> 64\u001b[0m         \u001b[43m_optimize_sequential\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     65\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     67\u001b[0m \u001b[43m            \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     68\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     69\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     70\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     71\u001b[0m \u001b[43m            \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     72\u001b[0m \u001b[43m            \u001b[49m\u001b[43mreseed_sampler_rng\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     73\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtime_start\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     74\u001b[0m \u001b[43m            \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     75\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     76\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     77\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[0;32m/opt/miniconda3/envs/research/lib/python3.9/site-packages/optuna/study/_optimize.py:161\u001b[0m, in \u001b[0;36m_optimize_sequential\u001b[0;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[1;32m    158\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m    160\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 161\u001b[0m     frozen_trial \u001b[38;5;241m=\u001b[39m \u001b[43m_run_trial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    162\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    163\u001b[0m     \u001b[38;5;66;03m# The following line mitigates memory problems that can be occurred in some\u001b[39;00m\n\u001b[1;32m    164\u001b[0m     \u001b[38;5;66;03m# environments (e.g., services that use computing containers such as GitHub Actions).\u001b[39;00m\n\u001b[1;32m    165\u001b[0m     \u001b[38;5;66;03m# Please refer to the following PR for further details:\u001b[39;00m\n\u001b[1;32m    166\u001b[0m     \u001b[38;5;66;03m# https://github.com/optuna/optuna/pull/325.\u001b[39;00m\n\u001b[1;32m    167\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m gc_after_trial:\n",
      "File \u001b[0;32m/opt/miniconda3/envs/research/lib/python3.9/site-packages/optuna/study/_optimize.py:253\u001b[0m, in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    246\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShould not reach.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    248\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    249\u001b[0m     frozen_trial\u001b[38;5;241m.\u001b[39mstate \u001b[38;5;241m==\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mFAIL\n\u001b[1;32m    250\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m func_err \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    251\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(func_err, catch)\n\u001b[1;32m    252\u001b[0m ):\n\u001b[0;32m--> 253\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m func_err\n\u001b[1;32m    254\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m frozen_trial\n",
      "File \u001b[0;32m/opt/miniconda3/envs/research/lib/python3.9/site-packages/optuna/study/_optimize.py:201\u001b[0m, in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m get_heartbeat_thread(trial\u001b[38;5;241m.\u001b[39m_trial_id, study\u001b[38;5;241m.\u001b[39m_storage):\n\u001b[1;32m    200\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 201\u001b[0m         value_or_values \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    202\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mTrialPruned \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    203\u001b[0m         \u001b[38;5;66;03m# TODO(mamu): Handle multi-objective cases.\u001b[39;00m\n\u001b[1;32m    204\u001b[0m         state \u001b[38;5;241m=\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mPRUNED\n",
      "Cell \u001b[0;32mIn[9], line 52\u001b[0m, in \u001b[0;36mobjective\u001b[0;34m(trial)\u001b[0m\n\u001b[1;32m     49\u001b[0m model \u001b[38;5;241m=\u001b[39m GATCNModel(TrialConfig())\n\u001b[1;32m     51\u001b[0m \u001b[38;5;66;03m# Accuracy 기준 최적화 #####\u001b[39;00m\n\u001b[0;32m---> 52\u001b[0m _, _, val_accs \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnode_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medge_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mmax\u001b[39m(val_accs)\n\u001b[1;32m     54\u001b[0m \u001b[38;5;66;03m############################\u001b[39;00m\n\u001b[1;32m     55\u001b[0m \n\u001b[1;32m     56\u001b[0m \u001b[38;5;66;03m# F1 Score 기준 최적화 #####################\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;66;03m# pos_weight 계산 (불균형 데이터 보정)\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[8], line 15\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, node_features, edge_index, X_train, y_train, X_val, y_val, epochs, lr, pos_weight)\u001b[0m\n\u001b[1;32m     13\u001b[0m train_output \u001b[38;5;241m=\u001b[39m model(node_features, edge_index)\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m0\u001b[39m)[:\u001b[38;5;28mlen\u001b[39m(y_train)]  \u001b[38;5;66;03m# 통합 모델에 데이터를 적용한 결과\u001b[39;00m\n\u001b[1;32m     14\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(train_output, y_train\u001b[38;5;241m.\u001b[39mfloat())\n\u001b[0;32m---> 15\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     18\u001b[0m train_losses\u001b[38;5;241m.\u001b[39mappend(loss\u001b[38;5;241m.\u001b[39mitem())\n",
      "File \u001b[0;32m/opt/miniconda3/envs/research/lib/python3.9/site-packages/torch/_tensor.py:255\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    246\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    247\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    248\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    249\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    253\u001b[0m         create_graph\u001b[38;5;241m=\u001b[39mcreate_graph,\n\u001b[1;32m    254\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs)\n\u001b[0;32m--> 255\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/miniconda3/envs/research/lib/python3.9/site-packages/torch/autograd/__init__.py:147\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    144\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m retain_graph \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    145\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m--> 147\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    148\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    149\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for company_name in ['TSLA', 'NVDA', 'MSFT', 'GOOG', 'AAPL', 'DIS', 'XOM', 'CRM', 'INTC', 'AMZN']:\n",
    "    # company_name = \"AMZN\"  # 예측할 회사 선택\n",
    "    data = df_[\n",
    "        [f'prccd_{company}' for company in\n",
    "         ['TSLA', 'NVDA', 'MSFT', 'GOOG', 'AAPL', 'DIS', 'XOM', 'CRM', 'INTC', 'AMZN']] +\n",
    "        [f'cshtrd_{company_name}', f'sent_{company_name}', 'datadate']].copy()\n",
    "    data.set_index('datadate', inplace=True)\n",
    "    data.fillna(0, inplace=True)  # 감정분석 결측값을 0으로\n",
    "    data.iloc[:, :10] = data.iloc[:, :10].pct_change()\n",
    "            # returns = (close_prices[1:] / close_prices[:-1]) - 1\n",
    "        # labels = np.where(returns > 0.003, 1, 0)\n",
    "    data = data.dropna()\n",
    "    data_values = data.values\n",
    "    scaler = StandardScaler()\n",
    "    data_s = scaler.fit_transform(data_values[:, :-1])\n",
    "    df_preprocessed = np.hstack([data_s, data_values[:, -1].reshape(-1, 1)])\n",
    "    node_features = df_preprocessed\n",
    "    # n_nodes = node_features.shape[0]  # 노드 수 (==날짜 수)\n",
    "\n",
    "    # 그래프 형태로 변환 : 엣지 생성 (시점 간 연결)\n",
    "\n",
    "\n",
    "    # 라벨 생성\n",
    "    close_prices = data[f'prccd_{company_name}'].values\n",
    "    # returns = (close_prices[1:] / close_prices[:-1]) - 1\n",
    "    labels = np.where(close_prices > 0.003, 1, 0)  # 0.3% 초과만 1로\n",
    "    labels = torch.tensor(labels, dtype=torch.float32)\n",
    "    node_features = torch.tensor(node_features, dtype=torch.float)\n",
    "\n",
    "    split = int(len(node_features) * 0.8)\n",
    "    # print(len(labels))\n",
    "\n",
    "    # print(edge_index)\n",
    "    # train_edge_index = edge_index[:split]\n",
    "    # val_edge_index = edge_index[split:]\n",
    "\n",
    "    train_node_features = node_features[:split]\n",
    "    val_node_features = node_features[split:]\n",
    "    \n",
    "    edge_list = []\n",
    "    n_nodes = len(train_node_features)  # 노드 수 (==날짜 수)\n",
    "    for i in range(n_nodes):\n",
    "        for j in [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]:  # 이후 시점들에 단방향 연결; (휴장 같은 것은 생각하지 않음.... 시점 기준)\n",
    "            if i + j < n_nodes:\n",
    "                edge_list.append([i, i + j])\n",
    "    edge_index = torch.tensor(edge_list).t()\n",
    "    \n",
    "    print(len(train_node_features), len(val_node_features), n_nodes)\n",
    "    \n",
    "    \n",
    "\n",
    "    train_labels = labels[:split]\n",
    "    val_labels = labels[split:]\n",
    "    X = train_node_features[:-1]\n",
    "    y = train_labels[1:]\n",
    "    split = int(len(X) * 0.8)\n",
    "\n",
    "\n",
    "    X_train = X[:split]\n",
    "    X_val   = X[split:]\n",
    "\n",
    "    y_train = y[:split]\n",
    "    y_val   = y[split:]\n",
    "    # Optuna 튜닝 실행\n",
    "    study = optuna.create_study(direction=\"maximize\")\n",
    "    study.optimize(objective, n_trials=50)\n",
    "    # 최적 하이퍼파라미터 출력\n",
    "    print(\"✅ Best Trial:\")\n",
    "    print(study.best_trial.params)\n",
    "    best_params = study.best_trial.params\n",
    "\n",
    "\n",
    "    # BestConfig를 이용해 모델 설정 후 학습 : optuna를 통해 정해진 최적 하이퍼파라미터\n",
    "\n",
    "    class BestConfig:\n",
    "        def __init__(self):\n",
    "            # GAT 설정\n",
    "            self.gat_out_features = best_params['gat_out_features']\n",
    "            self.gat_heads = best_params['gat_heads']\n",
    "            self.gat_dropout = best_params['gat_dropout']\n",
    "\n",
    "            # TCN 설정\n",
    "            self.enc_in = best_params['gat_out_features']  # GAT 출력 = TCN 입력\n",
    "            self.dims = [best_params['dim1'], best_params['dim2'], best_params['dim3']]\n",
    "            self.large_size = [best_params['k1'], best_params['k2'], best_params['k3']]\n",
    "            self.small_size = [best_params['s1'], best_params['s2'], best_params['s3']]\n",
    "            self.small_kernel_merged = False\n",
    "            self.dropout = best_params['dropout']\n",
    "            self.head_dropout = best_params['head_dropout']\n",
    "            self.revin = best_params['revin']\n",
    "            self.affine = best_params['affine']\n",
    "            self.decomposition = best_params['decomposition']\n",
    "            self.kernel_size = best_params['kernel_size']\n",
    "\n",
    "\n",
    "    model = GATCNModel(BestConfig())\n",
    "    edge_list = []\n",
    "    n_nodes = len(val_node_features)\n",
    "    for i in range(n_nodes):\n",
    "        for j in [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]:  # 이후 시점들에 단방향 연결; (휴장 같은 것은 생각하지 않음.... 시점 기준)\n",
    "            if i + j < n_nodes:\n",
    "                edge_list.append([i, i + j])\n",
    "    edge_index = torch.tensor(edge_list).t()\n",
    "    \n",
    "    print(len(train_node_features), len(val_node_features), n_nodes)\n",
    "    \n",
    "    \n",
    "    train_losses, val_losses, val_accs = train_model(model, train_node_features, edge_index, X_train, y_train, X_val, y_val,\n",
    "                                                     epochs=75, lr=1e-3)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        pred_logits = model(val_node_features, edge_index).squeeze(0)  # [1, T] → [T]\n",
    "        pred_probs = torch.sigmoid(pred_logits).cpu().numpy()\n",
    "        pred_labels = (pred_probs > 0.5).astype(int)\n",
    "\n",
    "    print(company_name)\n",
    "    visualize_training(company_name, train_losses, val_losses, val_accs)\n",
    "    visualize_match_only(company_name, pred_probs, val_labels.cpu().numpy())\n",
    "\n",
    "    df = pd.DataFrame(np.vstack([val_labels.cpu().numpy(), pred_labels])).T\n",
    "    df.columns = ['y_true', 'y_pred']  # 열 이름 지정\n",
    "\n",
    "    f1 = f1_score(df['y_true'], df['y_pred'], average='macro')  # or 'micro', 'weighted'\n",
    "    print(f\"F1 Score: {f1:.4f}\")\n",
    "    df.to_csv(f'data/{company_name}.csv')\n",
    "    pickle_it(model.to('cpu').state_dict(), pt.join('general_results', f'weights_{company_name}.torch'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XuoMYCoddnYg"
   },
   "source": [
    "3. 모델 생성 및 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 73
    },
    "id": "NHiQbmBy1EIe",
    "outputId": "ad3aa8c6-1a3e-48bc-96b1-1505f73c0276"
   },
   "outputs": [],
   "source": [
    "# BestConfig를 이용해 모델 설정 후 학습 :  여러 시행, 다양한 회사에서의 bestconfig 정보를 종합해 최적 파라미터 직접 설정 (아래는 예시)\n",
    "\n",
    "'''\n",
    "class BestConfig:\n",
    "    def __init__(self):\n",
    "        # GAT 설정\n",
    "        self.gat_out_features = 8\n",
    "        self.gat_heads = 4\n",
    "        self.gat_dropout = 0.2\n",
    "\n",
    "        # TCN 설정\n",
    "        self.enc_in = 8\n",
    "        self.dims = [8,16,32]\n",
    "        self.large_size = [7,5,7]\n",
    "        self.small_size = [5,1,1]\n",
    "        self.small_kernel_merged = False\n",
    "        self.dropout = 0.2\n",
    "        self.head_dropout = 0.25\n",
    "        self.revin = True\n",
    "        self.affine = False\n",
    "        self.decomposition = False\n",
    "        self.kernel_size = 31\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IZzMcisGdqIl"
   },
   "source": [
    "4. 예측 및 시각화 (best_params 기준으로 설정한 값)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HIg9asMg8vsg",
    "outputId": "d437479f-01c6-45e3-c38f-95aeb8c4b0aa"
   },
   "outputs": [],
   "source": [
    "# from sklearn.metrics import classification_report, confusion_matrix\n",
    "#\n",
    "# print(classification_report(y_val.cpu(), pred_labels))\n",
    "# print(confusion_matrix(y_val.cpu(), pred_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 73
    },
    "collapsed": true,
    "id": "FqU-FfFS7-OE",
    "outputId": "efce383a-255f-47b2-b9c4-39857d229915"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\"\"\"\n",
    "# 1. 실제 라벨\n",
    "true_labels = y_val.cpu().numpy()\n",
    "\n",
    "# 2. 다양한 threshold에 대해 f1-score 측정\n",
    "precisions, recalls, thresholds = precision_recall_curve(true_labels, pred_probs)\n",
    "\n",
    "f1s = 2 * (precisions * recalls) / (precisions + recalls + 1e-8)  # f1-score 계산\n",
    "best_idx = np.argmax(f1s)\n",
    "best_threshold = thresholds[best_idx]\n",
    "\n",
    "print(f\"✅ Best threshold by F1-score: {best_threshold:.4f}, F1: {f1s[best_idx]:.4f}\")\n",
    "\n",
    "# 3. 최적 threshold로 예측 라벨 생성\n",
    "pred_labels = (pred_probs > best_threshold).astype(int)\n",
    "# 6. 누적 수익률 (선택)\n",
    "# future_prices = close_prices[split+1:]  # 실제 수익률 계산용\n",
    "# visualize_cumulative_return(pred_probs, y_val.cpu().numpy(), future_prices)\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>datadate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2023-01-06</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-01-09</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-01-10</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-01-11</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-01-12</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-03-17</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-03-18</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-03-19</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-03-20</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-03-21</th>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>553 rows × 0 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: []\n",
       "Index: [2023-01-06, 2023-01-09, 2023-01-10, 2023-01-11, 2023-01-12, 2023-01-13, 2023-01-17, 2023-01-18, 2023-01-19, 2023-01-20, 2023-01-23, 2023-01-24, 2023-01-25, 2023-01-26, 2023-01-27, 2023-01-30, 2023-01-31, 2023-02-01, 2023-02-02, 2023-02-03, 2023-02-06, 2023-02-07, 2023-02-08, 2023-02-09, 2023-02-10, 2023-02-13, 2023-02-14, 2023-02-15, 2023-02-16, 2023-02-17, 2023-02-21, 2023-02-22, 2023-02-23, 2023-02-24, 2023-02-27, 2023-02-28, 2023-03-01, 2023-03-02, 2023-03-03, 2023-03-06, 2023-03-07, 2023-03-08, 2023-03-09, 2023-03-10, 2023-03-13, 2023-03-14, 2023-03-15, 2023-03-16, 2023-03-17, 2023-03-20, 2023-03-21, 2023-03-22, 2023-03-23, 2023-03-24, 2023-03-27, 2023-03-28, 2023-03-29, 2023-03-30, 2023-03-31, 2023-04-03, 2023-04-04, 2023-04-05, 2023-04-06, 2023-04-10, 2023-04-11, 2023-04-12, 2023-04-13, 2023-04-14, 2023-04-17, 2023-04-18, 2023-04-19, 2023-04-20, 2023-04-21, 2023-04-24, 2023-04-25, 2023-04-26, 2023-04-27, 2023-04-28, 2023-05-01, 2023-05-02, 2023-05-03, 2023-05-04, 2023-05-05, 2023-05-08, 2023-05-09, 2023-05-10, 2023-05-11, 2023-05-12, 2023-05-15, 2023-05-16, 2023-05-17, 2023-05-18, 2023-05-19, 2023-05-22, 2023-05-23, 2023-05-24, 2023-05-25, 2023-05-26, 2023-05-30, 2023-05-31, ...]\n",
       "\n",
       "[553 rows x 0 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_df = pd.DataFrame(index=df_[-len(val_node_features):]['datadate'])\n",
    "for company_name in ['TSLA', 'NVDA', 'MSFT', 'GOOG', 'AAPL', 'DIS', 'XOM', 'CRM', 'INTC', 'AMZN']:\n",
    "    company_df = pd.read_csv(f'data/{company_name}.csv', index_col=0)['y_pred']\n",
    "    company_df.name = company_name\n",
    "    company_df.index = weight_df.index\n",
    "    weight_df = pd.concat([weight_df, company_df], axis=1)\n",
    "    row_sum = (weight_df == 1).sum(axis=1)\n",
    "    row_sum.replace(0, 0.1, inplace=True)\n",
    "    weight_df_1=weight_df.div(row_sum, axis=0)\n",
    "    weight_df_1.to_csv(f'data/GAT_TCN_weight.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TSLA</th>\n",
       "      <th>NVDA</th>\n",
       "      <th>MSFT</th>\n",
       "      <th>GOOG</th>\n",
       "      <th>AAPL</th>\n",
       "      <th>DIS</th>\n",
       "      <th>XOM</th>\n",
       "      <th>CRM</th>\n",
       "      <th>INTC</th>\n",
       "      <th>AMZN</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>datadate</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2023-01-06</th>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.142857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-01-09</th>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-01-10</th>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-01-11</th>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.166667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-01-12</th>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-03-17</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-03-18</th>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-03-19</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-03-20</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-03-21</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>553 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                TSLA      NVDA      MSFT      GOOG      AAPL       DIS  \\\n",
       "datadate                                                                 \n",
       "2023-01-06  0.142857  0.142857  0.142857  0.142857  0.142857  0.000000   \n",
       "2023-01-09  0.200000  0.000000  0.000000  0.200000  0.200000  0.000000   \n",
       "2023-01-10  0.200000  0.000000  0.200000  0.000000  0.200000  0.000000   \n",
       "2023-01-11  0.166667  0.000000  0.166667  0.000000  0.166667  0.166667   \n",
       "2023-01-12  0.250000  0.000000  0.000000  0.000000  0.250000  0.000000   \n",
       "...              ...       ...       ...       ...       ...       ...   \n",
       "2025-03-17  0.000000  0.333333  0.000000  0.000000  0.000000  0.000000   \n",
       "2025-03-18  0.142857  0.000000  0.142857  0.000000  0.142857  0.142857   \n",
       "2025-03-19  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "2025-03-20  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "2025-03-21  0.000000  1.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "\n",
       "                 XOM       CRM      INTC      AMZN  \n",
       "datadate                                            \n",
       "2023-01-06  0.000000  0.142857  0.000000  0.142857  \n",
       "2023-01-09  0.000000  0.200000  0.000000  0.200000  \n",
       "2023-01-10  0.200000  0.000000  0.000000  0.200000  \n",
       "2023-01-11  0.166667  0.000000  0.000000  0.166667  \n",
       "2023-01-12  0.000000  0.000000  0.250000  0.250000  \n",
       "...              ...       ...       ...       ...  \n",
       "2025-03-17  0.333333  0.333333  0.000000  0.000000  \n",
       "2025-03-18  0.142857  0.142857  0.142857  0.000000  \n",
       "2025-03-19  1.000000  0.000000  0.000000  0.000000  \n",
       "2025-03-20  0.000000  0.333333  0.333333  0.333333  \n",
       "2025-03-21  0.000000  0.000000  0.000000  0.000000  \n",
       "\n",
       "[553 rows x 10 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weight_df_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 455
    },
    "collapsed": true,
    "id": "vTpEBkXIHHmR",
    "outputId": "12720ec1-bc4f-4ddb-afbe-aa46ce4821fb"
   },
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# from sklearn.metrics import f1_score\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "# from lightgbm import LGBMClassifier\n",
    "#\n",
    "#\n",
    "# def train_lgbm_model(X_train, y_train, X_val):\n",
    "#     model = LGBMClassifier(\n",
    "#         n_estimators=100,\n",
    "#         learning_rate=0.1,             # 너무 작으면 과적합/학습 지연\n",
    "#         verbosity=-1\n",
    "#     )\n",
    "#     model.fit(X_train, y_train)\n",
    "#\n",
    "#     pred_probs = model.predict_proba(X_val)[:, 1]\n",
    "#     pred_labels = (pred_probs > 0.5).astype(int)\n",
    "#\n",
    "#     return model, pred_probs, pred_labels\n",
    "#\n",
    "# # 가상의 node_features, labels, df_ 등이 정의되어 있어야 합니다.\n",
    "# # 위의 리팩토링은 함수 형태만 준비되어 있고, 본문 루프는 사용자가 가진 데이터 프레임 `df_`가 있어야 실행 가능\n",
    "#\n",
    "# def run_lgbm_pipeline(df_):\n",
    "#     results = []\n",
    "#     for company_name in ['TSLA', 'NVDA', 'MSFT', 'GOOG', 'AAPL', 'DIS', 'XOM', 'CRM', 'INTC', 'AMZN']:\n",
    "#         data = df_[\n",
    "#             [f'prccd_{company}' for company in\n",
    "#              ['TSLA', 'NVDA', 'MSFT', 'GOOG', 'AAPL', 'DIS', 'XOM', 'CRM', 'INTC', 'AMZN']] +\n",
    "#             [f'cshtrd_{company_name}', f'sent_{company_name}', 'datadate']].copy()\n",
    "#\n",
    "#         data.set_index('datadate', inplace=True)\n",
    "#         data.fillna(0, inplace=True)\n",
    "#         data.iloc[:, :10] = data.iloc[:, :10].pct_change()\n",
    "#         data = data.dropna()\n",
    "#         close_prices = data[f'prccd_{company_name}'].values\n",
    "#\n",
    "#         # ✅ 라벨: t+1 수익률 기준\n",
    "#         # returns = (close_prices[1:] / close_prices[:-1]) - 1\n",
    "#         labels = np.where(close_prices > 0.003, 1, 0)\n",
    "#\n",
    "#         # ✅ feature는 시점 t까지 (맨 마지막 row 제거)\n",
    "#         data_values = data.values\n",
    "#         scaler = StandardScaler()\n",
    "#         data_s = scaler.fit_transform(data_values[:, :-1])\n",
    "#         node_features = np.hstack([data_s, data_values[:, -1].reshape(-1, 1)])\n",
    "#\n",
    "#\n",
    "#         # ✅ train/val split\n",
    "#         X = node_features[:-1]\n",
    "#         y = labels[1:]\n",
    "#         split = int(len(X) * 0.8)\n",
    "#\n",
    "#         X_train = X[:split]\n",
    "#         X_val   = X[split:]\n",
    "#\n",
    "#         y_train = y[:split]\n",
    "#         y_val   = y[split:]\n",
    "#\n",
    "#         # ✅ 모델 학습 및 예측\n",
    "#         model, pred_probs, pred_labels = train_lgbm_model(X_train, y_train, X_val)\n",
    "#\n",
    "#         print(company_name)\n",
    "#         f1 = f1_score(y_val, pred_labels, average='macro')\n",
    "#         print(f\"F1 Score: {f1:.4f}\")\n",
    "#\n",
    "#         df_result = pd.DataFrame({'y_true': y_val, 'y_pred': pred_labels})\n",
    "#         df_result.to_csv(f'data/{company_name}_lgbm.csv', index=False)\n",
    "#\n",
    "#         results.append({\n",
    "#             \"company\": company_name,\n",
    "#             \"f1_score\": f1,\n",
    "#         })\n",
    "#\n",
    "#     return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result=run_lgbm_pipeline(df_)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_df = pd.DataFrame(index=df_[split+2:]['datadate'])\n",
    "for company_name in ['TSLA', 'NVDA', 'MSFT', 'GOOG', 'AAPL', 'DIS', 'XOM', 'CRM', 'INTC', 'AMZN']:\n",
    "    company_df = pd.read_csv(f'data/{company_name}_lgbm.csv', index_col=0)['y_pred']\n",
    "    company_df.name = company_name\n",
    "    company_df.index = weight_df.index\n",
    "    weight_df = pd.concat([weight_df, company_df], axis=1)\n",
    "    row_sum = (weight_df == 1).sum(axis=1)\n",
    "    row_sum.replace(0, 0.1, inplace=True)\n",
    "    weight_df_1=weight_df.div(row_sum, axis=0)\n",
    "    weight_df_1.to_csv(f'data/LGBM_weight.csv')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "gpuType": "V28",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "research",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

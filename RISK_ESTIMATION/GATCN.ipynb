{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BObOC16LAlS4"
   },
   "source": [
    "# 모듈 설치 및 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-27T07:21:22.856695Z",
     "start_time": "2025-05-27T07:21:18.195100Z"
    },
    "id": "XgRsALeSVcJX"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\asas4\\miniconda3\\envs\\research\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "\n",
    "import torch\n",
    "import optuna\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from os import path as pt\n",
    "from lib.utils import pickle_it\n",
    "from sklearn.metrics import f1_score\n",
    "from torch_geometric.nn import GATv2Conv\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XyHQgQwjAiTk"
   },
   "source": [
    "# GAT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m6DEYVqJC0_l"
   },
   "source": [
    "- 노드 : 각 시점\n",
    "- 피처 : 10개 주식 종가 + 예측 기업 거래량 + 예측 기업 감정분석\n",
    "- GAT의 역할 : 피처들의 관계(회사 간의 관계 등)을 파악해 **시점별** 임베딩 생성;  다른 시점과의 연관성을 반영    \n",
    "(ex. 1~10일 전과 연결이 되어있는 상태에서, 1일 전 정보는 얼마나 중요하고 10일 전 정보는 얼마나 중요한지 판단)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-27T07:21:24.230549Z",
     "start_time": "2025-05-27T07:21:24.215814Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 91
    },
    "id": "GegpQ78WROjC",
    "outputId": "7cc16100-9a36-400c-caf2-5b009a27f982"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nclass GAT(nn.Module):\\n    def __init__(self, in_features, out_features, heads, dropout, concat=False):\\n        super(GAT, self).__init__()\\n\\n        self.in_features = in_features\\n        self.out_features = out_features\\n        self.heads = heads\\n        self.dropout = dropout\\n        self.concat = concat\\n\\n        # 선형 변환을 위한 가중치 행렬\\n        self.lin = nn.Linear(in_features, heads * out_features, bias=False)\\n\\n        # 어텐션 계수 계산을 위한 가중치\\n        self.att = nn.Parameter(torch.Tensor(1, heads, out_features))\\n\\n        # 바이어스\\n        self.bias = nn.Parameter(torch.Tensor(out_features))\\n\\n        # LeakyReLU\\n        self.leakyrelu = nn.LeakyReLU(0.2)\\n\\n        # 드롭아웃\\n        self.dropout_layer = nn.Dropout(dropout)\\n\\n        # 초기화\\n        self.reset_parameters()\\n\\n    def reset_parameters(self):\\n        # 가중치 초기화\\n        gain = nn.init.calculate_gain(\\'relu\\')\\n        nn.init.xavier_normal_(self.lin.weight, gain=gain)\\n        nn.init.xavier_normal_(self.att, gain=gain)\\n        nn.init.zeros_(self.bias)\\n\\n    def forward(self, x, edge_index):\\n        \"\"\"\\n        x: 노드 특성 [N, in_features]\\n        edge_index: 엣지 인덱스 [2, E]\\n        \"\"\"\\n        N = x.size(0)  # 노드 수\\n\\n        # 1. 선형 변환\\n        x = self.dropout_layer(x)\\n        x = self.lin(x)  # [N, heads * out_features]\\n        x = x.view(N, self.heads, self.out_features)  # [N, heads, out_features]\\n\\n        # 2. 엣지 리스트로부터 어텐션 계산\\n        edge_src, edge_dst = edge_index[0], edge_index[1]\\n\\n        # 3. GATv2 : 선형변환 후 어텐션\\n        # 각 노드에 어텐션 가중치 적용\\n        x_att = x * self.att  # [N, heads, out_features]\\n\\n        # 이웃 노드 쌍의 어텐션 점수 계산\\n        alpha_src = x_att[edge_src].sum(dim=-1)  # [E, heads]\\n        alpha_dst = x_att[edge_dst].sum(dim=-1)  # [E, heads]\\n        alpha = alpha_src + alpha_dst  # [E, heads]\\n        alpha = self.leakyrelu(alpha)  # [E, heads]\\n\\n        # 4. 소프트맥스로 정규화\\n        alpha = self._edge_softmax(alpha, edge_index[1], N)\\n        alpha = self.dropout_layer(alpha)\\n\\n        # 5. 메시지 집계\\n        out = torch.zeros(N, self.heads, self.out_features, device=x.device)\\n\\n        # 각 엣지에 대해 메시지 전달\\n        for i in range(edge_index.size(1)):\\n            src, dst = edge_index[0, i], edge_index[1, i]\\n            out[dst] += alpha[i].unsqueeze(-1) * x[src]\\n\\n        # 6. 최종 출력 형태 결정\\n        if self.concat:\\n            out = out.view(N, self.heads * self.out_features)\\n        else:\\n            out = out.mean(dim=1)  # 헤드 간 평균\\n\\n        # 7. 바이어스 추가\\n        out = out + self.bias\\n\\n        return out\\n\\n    def _edge_softmax(self, alpha, target_nodes, num_nodes):\\n\\n        norm_alpha = torch.zeros_like(alpha)\\n\\n\\n        for i in range(num_nodes):\\n            # 현재 노드로 향하는 엣지 마스크\\n            mask = (target_nodes == i)\\n            if mask.sum() > 0:\\n                # 해당 노드로 향하는 엣지에 대해 소프트맥스 적용\\n                norm_alpha[mask] = F.softmax(alpha[mask], dim=0)\\n\\n        return norm_alpha\\n '"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "class GAT(nn.Module):\n",
    "    def __init__(self, in_features, out_features, heads, dropout, concat=False):\n",
    "        super(GAT, self).__init__()\n",
    "\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.heads = heads\n",
    "        self.dropout = dropout\n",
    "        self.concat = concat\n",
    "\n",
    "        # 선형 변환을 위한 가중치 행렬\n",
    "        self.lin = nn.Linear(in_features, heads * out_features, bias=False)\n",
    "\n",
    "        # 어텐션 계수 계산을 위한 가중치\n",
    "        self.att = nn.Parameter(torch.Tensor(1, heads, out_features))\n",
    "\n",
    "        # 바이어스\n",
    "        self.bias = nn.Parameter(torch.Tensor(out_features))\n",
    "\n",
    "        # LeakyReLU\n",
    "        self.leakyrelu = nn.LeakyReLU(0.2)\n",
    "\n",
    "        # 드롭아웃\n",
    "        self.dropout_layer = nn.Dropout(dropout)\n",
    "\n",
    "        # 초기화\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        # 가중치 초기화\n",
    "        gain = nn.init.calculate_gain('relu')\n",
    "        nn.init.xavier_normal_(self.lin.weight, gain=gain)\n",
    "        nn.init.xavier_normal_(self.att, gain=gain)\n",
    "        nn.init.zeros_(self.bias)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        \"\"\"\n",
    "        x: 노드 특성 [N, in_features]\n",
    "        edge_index: 엣지 인덱스 [2, E]\n",
    "        \"\"\"\n",
    "        N = x.size(0)  # 노드 수\n",
    "\n",
    "        # 1. 선형 변환\n",
    "        x = self.dropout_layer(x)\n",
    "        x = self.lin(x)  # [N, heads * out_features]\n",
    "        x = x.view(N, self.heads, self.out_features)  # [N, heads, out_features]\n",
    "\n",
    "        # 2. 엣지 리스트로부터 어텐션 계산\n",
    "        edge_src, edge_dst = edge_index[0], edge_index[1]\n",
    "\n",
    "        # 3. GATv2 : 선형변환 후 어텐션\n",
    "        # 각 노드에 어텐션 가중치 적용\n",
    "        x_att = x * self.att  # [N, heads, out_features]\n",
    "\n",
    "        # 이웃 노드 쌍의 어텐션 점수 계산\n",
    "        alpha_src = x_att[edge_src].sum(dim=-1)  # [E, heads]\n",
    "        alpha_dst = x_att[edge_dst].sum(dim=-1)  # [E, heads]\n",
    "        alpha = alpha_src + alpha_dst  # [E, heads]\n",
    "        alpha = self.leakyrelu(alpha)  # [E, heads]\n",
    "\n",
    "        # 4. 소프트맥스로 정규화\n",
    "        alpha = self._edge_softmax(alpha, edge_index[1], N)\n",
    "        alpha = self.dropout_layer(alpha)\n",
    "\n",
    "        # 5. 메시지 집계\n",
    "        out = torch.zeros(N, self.heads, self.out_features, device=x.device)\n",
    "\n",
    "        # 각 엣지에 대해 메시지 전달\n",
    "        for i in range(edge_index.size(1)):\n",
    "            src, dst = edge_index[0, i], edge_index[1, i]\n",
    "            out[dst] += alpha[i].unsqueeze(-1) * x[src]\n",
    "\n",
    "        # 6. 최종 출력 형태 결정\n",
    "        if self.concat:\n",
    "            out = out.view(N, self.heads * self.out_features)\n",
    "        else:\n",
    "            out = out.mean(dim=1)  # 헤드 간 평균\n",
    "\n",
    "        # 7. 바이어스 추가\n",
    "        out = out + self.bias\n",
    "\n",
    "        return out\n",
    "\n",
    "    def _edge_softmax(self, alpha, target_nodes, num_nodes):\n",
    "\n",
    "        norm_alpha = torch.zeros_like(alpha)\n",
    "\n",
    "\n",
    "        for i in range(num_nodes):\n",
    "            # 현재 노드로 향하는 엣지 마스크\n",
    "            mask = (target_nodes == i)\n",
    "            if mask.sum() > 0:\n",
    "                # 해당 노드로 향하는 엣지에 대해 소프트맥스 적용\n",
    "                norm_alpha[mask] = F.softmax(alpha[mask], dim=0)\n",
    "\n",
    "        return norm_alpha\n",
    " '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-27T07:21:24.649299Z",
     "start_time": "2025-05-27T07:21:24.635619Z"
    },
    "id": "ZRWMJA0G_U2x"
   },
   "outputs": [],
   "source": [
    "# GAT 레이어 정의\n",
    "\n",
    "class GATLayer(nn.Module):\n",
    "    def __init__(self, in_features, out_features, heads, dropout):\n",
    "        super().__init__()\n",
    "        self.gat = GATv2Conv(in_features, out_features, heads=heads, dropout=dropout, concat=False)  # PyG 사용\n",
    "        # self.gat = GAT(in_features, out_features, heads=heads, dropout=dropout, concat=False)            # 상단 구현 코드 (느리고, 느려서 optuna 중간에 끊고 첫번째걸로 해봤는데 ----로 예측됨;;;;)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        return self.gat(x, edge_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2_Ee7Aj6A3K_"
   },
   "source": [
    "# TCN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-27T07:21:25.530783Z",
     "start_time": "2025-05-27T07:21:25.499092Z"
    },
    "id": "Ky1M6aXiA47i"
   },
   "outputs": [],
   "source": [
    "# --- 유틸 함수 ---\n",
    "def get_conv1d(in_channels, out_channels, kernel_size, stride, padding, dilation, groups, bias):\n",
    "    return nn.Conv1d(in_channels=in_channels, out_channels=out_channels,\n",
    "                     kernel_size=kernel_size, stride=stride,\n",
    "                     padding=padding, dilation=dilation,\n",
    "                     groups=groups, bias=bias)\n",
    "\n",
    "\n",
    "def get_bn(channels):\n",
    "    return nn.BatchNorm1d(channels)\n",
    "\n",
    "\n",
    "def conv_bn(in_channels, out_channels, kernel_size, stride, padding, groups, dilation=1, bias=False):\n",
    "    if padding is None:\n",
    "        padding = kernel_size // 2\n",
    "    result = nn.Sequential()\n",
    "    result.add_module('conv',\n",
    "                      get_conv1d(in_channels, out_channels, kernel_size, stride, padding, dilation, groups, bias))\n",
    "    result.add_module('bn', get_bn(out_channels))\n",
    "    return result\n",
    "\n",
    "\n",
    "# --- RevIN ---\n",
    "class RevIN(nn.Module):\n",
    "    def __init__(self, num_features: int, eps=1e-5, affine=True, subtract_last=False):\n",
    "        super().__init__()\n",
    "        self.num_features = num_features\n",
    "        self.eps = eps\n",
    "        self.affine = affine\n",
    "        self.subtract_last = subtract_last\n",
    "        if self.affine:\n",
    "            self._init_params()\n",
    "\n",
    "    def _init_params(self):\n",
    "        self.affine_weight = nn.Parameter(torch.ones(self.num_features))\n",
    "        self.affine_bias = nn.Parameter(torch.zeros(self.num_features))\n",
    "\n",
    "    def forward(self, x, mode: str):\n",
    "        if mode == 'norm':\n",
    "            self._get_statistics(x)\n",
    "            x = self._normalize(x)\n",
    "        elif mode == 'denorm':\n",
    "            x = self._denormalize(x)\n",
    "        return x\n",
    "\n",
    "    def _get_statistics(self, x):\n",
    "        dim2reduce = tuple(range(1, x.ndim - 1))\n",
    "        if self.subtract_last:\n",
    "            self.last = x[:, -1:, :].unsqueeze(1)\n",
    "        else:\n",
    "            self.mean = torch.mean(x, dim=dim2reduce, keepdim=True).detach()\n",
    "        self.stdev = torch.sqrt(torch.var(x, dim=dim2reduce, keepdim=True, unbiased=False) + self.eps).detach()\n",
    "\n",
    "    def _normalize(self, x):\n",
    "        x = (x - self.mean) / self.stdev\n",
    "        if self.affine:\n",
    "            x = x * self.affine_weight[None, None, :] + self.affine_bias[None, None, :]\n",
    "        return x\n",
    "\n",
    "    def _denormalize(self, x):\n",
    "        if self.affine:\n",
    "            x = (x - self.affine_bias[None, None, :]) / self.affine_weight[None, None, :]\n",
    "        x = x * self.stdev + self.mean\n",
    "        return x\n",
    "\n",
    "\n",
    "# --- 시계열 분해 ---\n",
    "class moving_avg(nn.Module):\n",
    "    def __init__(self, kernel_size, stride):\n",
    "        super().__init__()\n",
    "        self.kernel_size = kernel_size\n",
    "        self.avg = nn.AvgPool1d(kernel_size=kernel_size, stride=stride, padding=0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        front = x[:, 0:1, :].repeat(1, (self.kernel_size - 1) // 2, 1)\n",
    "        end = x[:, -1:, :].repeat(1, (self.kernel_size - 1) // 2, 1)\n",
    "        x = torch.cat([front, x, end], dim=1)\n",
    "        x = self.avg(x.permute(0, 2, 1))\n",
    "        return x.permute(0, 2, 1)\n",
    "\n",
    "\n",
    "class series_decomp(nn.Module):\n",
    "    def __init__(self, kernel_size):\n",
    "        super().__init__()\n",
    "        self.moving_avg = moving_avg(kernel_size, stride=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        moving_mean = self.moving_avg(x)\n",
    "        return x - moving_mean, moving_mean\n",
    "\n",
    "\n",
    "# --- 커스텀 커널 ---\n",
    "class ReparamLargeKernelConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride, groups, small_kernel, small_kernel_merged=False):\n",
    "        super().__init__()\n",
    "        self.kernel_size = kernel_size\n",
    "        self.small_kernel = small_kernel\n",
    "        padding = kernel_size // 2\n",
    "        if small_kernel_merged:\n",
    "            self.lkb_reparam = nn.Conv1d(in_channels, out_channels, kernel_size, stride, padding, groups=groups,\n",
    "                                         bias=True)\n",
    "        else:\n",
    "            self.lkb_origin = conv_bn(in_channels, out_channels, kernel_size, stride, padding, groups)\n",
    "            if small_kernel is not None:\n",
    "                self.small_conv = conv_bn(in_channels, out_channels, small_kernel, stride, small_kernel // 2, groups)\n",
    "\n",
    "    def forward(self, x):\n",
    "        if hasattr(self, 'lkb_reparam'):\n",
    "            return self.lkb_reparam(x)\n",
    "        out = self.lkb_origin(x)\n",
    "        if hasattr(self, 'small_conv'):\n",
    "            out += self.small_conv(x)\n",
    "        return out\n",
    "\n",
    "\n",
    "# --- 출력층 ---\n",
    "class Flatten_Head(nn.Module):\n",
    "    def __init__(self, d_model):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(d_model, 1)\n",
    "\n",
    "    def forward(self, x):  # x: [B, C, T]\n",
    "        x = x.permute(0, 2, 1)  # → [B, T, C]\n",
    "        x = self.linear(x)  # → [B, T, 1]\n",
    "        return x.squeeze(-1)  # → [B, T]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-27T07:21:25.857271Z",
     "start_time": "2025-05-27T07:21:25.843612Z"
    },
    "id": "eySi1894CGFo"
   },
   "outputs": [],
   "source": [
    "# --- ModernTCN 모델 ---\n",
    "class ModernTCN(nn.Module):\n",
    "    def __init__(self, configs):\n",
    "        super().__init__()\n",
    "        self.revin = RevIN(configs.enc_in, affine=configs.affine) if configs.revin else None\n",
    "        self.decomp = series_decomp(configs.kernel_size) if configs.decomposition else None\n",
    "\n",
    "        self.conv_layers = nn.ModuleList()\n",
    "        self.norm_layers = nn.ModuleList()\n",
    "\n",
    "        c_in = configs.enc_in\n",
    "        for i in range(len(configs.dims)):\n",
    "            conv = ReparamLargeKernelConv(c_in, configs.dims[i],\n",
    "                                          kernel_size=configs.large_size[i],\n",
    "                                          stride=1,\n",
    "                                          groups=1,\n",
    "                                          small_kernel=configs.small_size[i],\n",
    "                                          small_kernel_merged=configs.small_kernel_merged)\n",
    "            self.conv_layers.append(conv)\n",
    "            self.norm_layers.append(nn.BatchNorm1d(configs.dims[i]))\n",
    "            c_in = configs.dims[i]\n",
    "\n",
    "        self.head = Flatten_Head(configs.dims[-1])\n",
    "\n",
    "    def forward(self, x):  # x: [B, T, C]\n",
    "        if self.revin:\n",
    "            x = self.revin(x, 'norm')\n",
    "        if self.decomp:\n",
    "            x, _ = self.decomp(x)\n",
    "        x = x.permute(0, 2, 1)  # [B, C, T]\n",
    "        for conv, norm in zip(self.conv_layers, self.norm_layers):\n",
    "            x = conv(x)\n",
    "            x = norm(x)\n",
    "            x = F.relu(x)\n",
    "        out = self.head(x)  # [B, T]\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-27T07:21:26.386089Z",
     "start_time": "2025-05-27T07:21:26.367752Z"
    },
    "id": "Bqr3BMHdCEI6"
   },
   "outputs": [],
   "source": [
    "# --- Config 클래스 ---\n",
    "class Configs:\n",
    "    def __init__(self, enc_in):\n",
    "        self.enc_in = enc_in\n",
    "        self.dims = [8, 16, 32]\n",
    "        self.large_size = [5, 5, 3]\n",
    "        self.small_size = [5, 3, 3]\n",
    "        self.small_kernel_merged = False\n",
    "        self.dropout = 0.1\n",
    "        self.head_dropout = 0.2\n",
    "        self.revin = True\n",
    "        self.affine = True\n",
    "        self.decomposition = True\n",
    "        self.kernel_size = 25"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hC824c4hBPKK"
   },
   "source": [
    "# GAT-TCN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-27T07:21:27.304710Z",
     "start_time": "2025-05-27T07:21:27.289466Z"
    },
    "id": "nu0GYmnkBOtr"
   },
   "outputs": [],
   "source": [
    "class GATCNModel(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.gat = GATLayer(node_features.shape[1], config.gat_out_features,\n",
    "                            config.gat_heads, config.gat_dropout)  # GAT\n",
    "        self.tcn = ModernTCN(config)  # TCN\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        embeddings = self.gat(x, edge_index)  # GAT를 통한 임베딩 생성\n",
    "        tcn_input = embeddings.unsqueeze(0)  # TCN 입력 형태로 변환\n",
    "        output = self.tcn(tcn_input)  # TCN으로 예측\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-27T07:21:27.963107Z",
     "start_time": "2025-05-27T07:21:27.952437Z"
    },
    "id": "F5pcxCD-EjI4"
   },
   "outputs": [],
   "source": [
    "\n",
    "# GAT-TCN 모델의 최적 파라미터 탐색\n",
    "\n",
    "def train_model(model, node_features, edge_index, X_train, y_train, X_val, y_val, epochs=30, lr=1e-3, pos_weight=None):\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    #criterion = nn.BCEWithLogitsLoss()\n",
    "    criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight) if pos_weight is not None else nn.BCEWithLogitsLoss()\n",
    "    train_losses, val_losses, val_accs = [], [], []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        # 1.학습 단계\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        train_output = model(node_features, edge_index).squeeze(0)[:len(y_train)]  # 통합 모델에 데이터를 적용한 결과\n",
    "        loss = criterion(train_output, y_train.float())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_losses.append(loss.item())\n",
    "\n",
    "        # 2. 검증 단계\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_output = model(node_features, edge_index).squeeze(0)[-len(y_val):]\n",
    "            val_loss = criterion(val_output, y_val.float()).item()\n",
    "            pred = (torch.sigmoid(val_output) > 0.5).int()\n",
    "            acc = (pred == y_val).float().mean().item()\n",
    "\n",
    "        val_losses.append(val_loss)\n",
    "        val_accs.append(acc)\n",
    "\n",
    "        print(f\"[{epoch + 1}/{epochs}] Train Loss: {loss.item():.4f}, Val Loss: {val_loss:.4f}, Val Acc: {acc:.4f}\")\n",
    "\n",
    "    return train_losses, val_losses, val_accs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-27T07:21:28.694110Z",
     "start_time": "2025-05-27T07:21:28.677429Z"
    },
    "id": "Ri6Us2oFMJlq"
   },
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    # GAT\n",
    "    gat_out_features = trial.suggest_categorical(\"gat_out_features\", [4, 8, 12])  # 기존 features 수보다는 적은 것이 적합\n",
    "    gat_heads = trial.suggest_categorical(\"gat_heads\", [1, 2, 4, 8])\n",
    "    gat_dropout = trial.suggest_float(\"gat_dropout\", 0.0, 0.3)\n",
    "\n",
    "    # TCN\n",
    "    dims = [\n",
    "        trial.suggest_categorical(\"dim1\", [8, 16, 32, 64]),\n",
    "        trial.suggest_categorical(\"dim2\", [16, 32, 64, 128]),\n",
    "        trial.suggest_categorical(\"dim3\", [32, 64, 128, 256])\n",
    "    ]\n",
    "    large_size = [\n",
    "        trial.suggest_categorical(\"k1\", [3, 5, 7, 9, 11]),\n",
    "        trial.suggest_categorical(\"k2\", [3, 5, 7, 9]),\n",
    "        trial.suggest_categorical(\"k3\", [3, 5, 7])\n",
    "    ]\n",
    "    small_size = [\n",
    "        trial.suggest_categorical(\"s1\", [1, 3, 5]),\n",
    "        trial.suggest_categorical(\"s2\", [1, 3]),\n",
    "        trial.suggest_categorical(\"s3\", [1, 3])\n",
    "    ]\n",
    "    dropout = trial.suggest_float(\"dropout\", 0.0, 0.3)\n",
    "    head_dropout = trial.suggest_float(\"head_dropout\", 0.0, 0.3)\n",
    "    kernel_size = trial.suggest_categorical(\"kernel_size\", [5, 11, 15, 25, 31])\n",
    "    decomposition = trial.suggest_categorical(\"decomposition\", [True, False])\n",
    "    revin = trial.suggest_categorical(\"revin\", [True, False])\n",
    "    affine = trial.suggest_categorical(\"affine\", [True, False])\n",
    "\n",
    "    # 통합\n",
    "    class TrialConfig:\n",
    "        def __init__(self):\n",
    "            self.gat_out_features = gat_out_features\n",
    "            self.gat_heads = gat_heads\n",
    "            self.gat_dropout = gat_dropout\n",
    "\n",
    "            self.enc_in = gat_out_features\n",
    "            self.dims = dims\n",
    "            self.large_size = large_size\n",
    "            self.small_size = small_size\n",
    "            self.small_kernel_merged = False\n",
    "            self.dropout = dropout\n",
    "            self.head_dropout = head_dropout\n",
    "            self.revin = revin\n",
    "            self.affine = affine\n",
    "            self.decomposition = decomposition\n",
    "            self.kernel_size = kernel_size\n",
    "\n",
    "    model = GATCNModel(TrialConfig())\n",
    "\n",
    "    # Accuracy 기준 최적화 #####\n",
    "    _, _, val_accs = train_model(model, node_features, edge_index, X_train, y_train, X_val, y_val, epochs=50)\n",
    "    return max(val_accs)\n",
    "    ############################\n",
    "\n",
    "    # F1 Score 기준 최적화 #####################\n",
    "    # pos_weight 계산 (불균형 데이터 보정)\n",
    "    '''\n",
    "    pos_weight = torch.tensor([(y_train == 0).sum() / (y_train == 1).sum()]).to(y_train.device)\n",
    "\n",
    "    train_model(model, node_features, edge_index, X_train, y_train, X_val, y_val, epochs=15, pos_weight=pos_weight)\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        full_pred = model(node_features, edge_index).squeeze(0)\n",
    "        pred = full_pred[-len(y_val):]\n",
    "        probs = torch.sigmoid(pred).cpu().numpy()\n",
    "        preds = (probs > 0.5).astype(int)\n",
    "\n",
    "    y_true = y_val.cpu().numpy()\n",
    "    return f1_score(y_true, preds)\n",
    "    '''\n",
    "    ##########################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e6IIhl_Vo5x9"
   },
   "source": [
    "# 시각화 코드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-27T07:21:30.155627Z",
     "start_time": "2025-05-27T07:21:30.133289Z"
    },
    "id": "nj38FLOMtL84"
   },
   "outputs": [],
   "source": [
    "def visualize_training(company_name, train_losses, val_losses, val_accs):\n",
    "    plt.figure(figsize=(12, 2), dpi=400)\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(train_losses, label='Train Loss')\n",
    "    plt.plot(val_losses, label='Val Loss')\n",
    "    plt.legend()\n",
    "    plt.title(\"Loss over Epochs\")\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(val_accs, label='Val Accuracy')\n",
    "    plt.legend()\n",
    "    plt.title(\"Validation Accuracy\")\n",
    "    plt.savefig(f'data/images/{company_name}_loss.png')\n",
    "    plt.close()\n",
    "\n",
    "from matplotlib.patches import Patch\n",
    "\n",
    "def visualize_match_only(company_name, pred_probs, true_labels, threshold=0.5):\n",
    "    # 이진 예측\n",
    "    pred_labels = (pred_probs >= threshold).astype(int)\n",
    "\n",
    "    # 정답과 예측이 일치하면 1, 다르면 0\n",
    "    match = (pred_labels == true_labels).astype(int)\n",
    "\n",
    "    # 시각화\n",
    "    plt.figure(figsize=(12, 2), dpi=400)\n",
    "    bar_heights = np.ones_like(match)\n",
    "    bar_colors = ['green' if m else 'red' for m in match]\n",
    "    plt.bar(np.arange(len(match)), bar_heights,\n",
    "            color=bar_colors,\n",
    "            width=1.0)\n",
    "\n",
    "    plt.title(f\"{company_name} - Prediction Match\")\n",
    "    plt.ylabel('Match')\n",
    "    plt.xlabel('Time')\n",
    "    plt.yticks([0, 1], ['Wrong', 'Correct'])\n",
    "\n",
    "    # ✅ 범례 추가\n",
    "    legend_elements = [\n",
    "        Patch(facecolor='green', label='Correct'),\n",
    "        Patch(facecolor='red', label='Wrong')\n",
    "    ]\n",
    "    plt.legend(handles=legend_elements, loc='upper right')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'data/images/{company_name}_match_only.png')\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def visualize_cumulative_return(pred_probs, true_labels, prices):\n",
    "    signal = (pred_probs > 0.5).astype(int)\n",
    "    returns = (prices[1:] / prices[:-1]) - 1\n",
    "    strategy_returns = returns * signal[:-1]\n",
    "\n",
    "    cumulative = (strategy_returns + 1).cumprod()\n",
    "    market = (returns + 1).cumprod()\n",
    "\n",
    "    plt.plot(cumulative, label='Strategy')\n",
    "    plt.plot(market, label='Market (buy & hold)')\n",
    "    plt.legend();\n",
    "    plt.title(\"Cumulative Return\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2gL9v1aBFqxZ"
   },
   "source": [
    "# GAT-TCN 적용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-27T07:21:31.619843Z",
     "start_time": "2025-05-27T07:21:31.601672Z"
    },
    "id": "YmnyK8iVm3nZ"
   },
   "outputs": [],
   "source": [
    "# seed 설정\n",
    "import random\n",
    "\n",
    "\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.use_deterministic_algorithms(True)\n",
    "\n",
    "\n",
    "set_seed(42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZkK07jzudiAx"
   },
   "source": [
    "1. 데이터셋 불러오기 및 라벨 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-27T07:21:35.780028Z",
     "start_time": "2025-05-27T07:21:35.752636Z"
    }
   },
   "outputs": [],
   "source": [
    "df_ = pd.read_csv(\"./data/daily_all.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-27T07:21:36.172012Z",
     "start_time": "2025-05-27T07:21:36.158361Z"
    }
   },
   "outputs": [],
   "source": [
    "# company_name =['TSLA', 'NVDA', 'MSFT', 'GOOG', 'AAPL', 'DIS', 'XOM', 'CRM', 'INTC', 'AMZN'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-27T07:25:09.482611Z",
     "start_time": "2025-05-27T07:24:12.708918Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 455
    },
    "collapsed": true,
    "id": "vTpEBkXIHHmR",
    "outputId": "12720ec1-bc4f-4ddb-afbe-aa46ce4821fb"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-17 16:55:09,930] A new study created in memory with name: no-name-aacc1488-164f-4a0f-9405-77d51d8c9c8e\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/50] Train Loss: 0.7041, Val Loss: 0.6965, Val Acc: 0.4548\n",
      "[2/50] Train Loss: 0.6820, Val Loss: 0.6966, Val Acc: 0.4548\n",
      "[3/50] Train Loss: 0.6742, Val Loss: 0.6962, Val Acc: 0.4548\n",
      "[4/50] Train Loss: 0.6679, Val Loss: 0.6953, Val Acc: 0.4548\n",
      "[5/50] Train Loss: 0.6588, Val Loss: 0.6941, Val Acc: 0.4751\n",
      "[6/50] Train Loss: 0.6515, Val Loss: 0.6930, Val Acc: 0.4910\n",
      "[7/50] Train Loss: 0.6453, Val Loss: 0.6921, Val Acc: 0.5249\n",
      "[8/50] Train Loss: 0.6410, Val Loss: 0.6916, Val Acc: 0.5452\n",
      "[9/50] Train Loss: 0.6336, Val Loss: 0.6913, Val Acc: 0.5452\n",
      "[10/50] Train Loss: 0.6242, Val Loss: 0.6912, Val Acc: 0.5385\n",
      "[11/50] Train Loss: 0.6208, Val Loss: 0.6911, Val Acc: 0.5294\n",
      "[12/50] Train Loss: 0.6097, Val Loss: 0.6910, Val Acc: 0.5271\n",
      "[13/50] Train Loss: 0.6121, Val Loss: 0.6908, Val Acc: 0.5339\n",
      "[14/50] Train Loss: 0.6051, Val Loss: 0.6908, Val Acc: 0.5294\n",
      "[15/50] Train Loss: 0.5938, Val Loss: 0.6908, Val Acc: 0.5362\n",
      "[16/50] Train Loss: 0.5854, Val Loss: 0.6908, Val Acc: 0.5430\n",
      "[17/50] Train Loss: 0.5781, Val Loss: 0.6909, Val Acc: 0.5317\n",
      "[18/50] Train Loss: 0.5696, Val Loss: 0.6913, Val Acc: 0.5158\n",
      "[19/50] Train Loss: 0.5642, Val Loss: 0.6919, Val Acc: 0.5181\n",
      "[20/50] Train Loss: 0.5649, Val Loss: 0.6926, Val Acc: 0.5113\n",
      "[21/50] Train Loss: 0.5526, Val Loss: 0.6932, Val Acc: 0.5045\n",
      "[22/50] Train Loss: 0.5349, Val Loss: 0.6936, Val Acc: 0.4864\n",
      "[23/50] Train Loss: 0.5509, Val Loss: 0.6940, Val Acc: 0.4932\n",
      "[24/50] Train Loss: 0.5384, Val Loss: 0.6945, Val Acc: 0.5045\n",
      "[25/50] Train Loss: 0.5352, Val Loss: 0.6957, Val Acc: 0.4977\n",
      "[26/50] Train Loss: 0.5182, Val Loss: 0.6974, Val Acc: 0.5090\n",
      "[27/50] Train Loss: 0.5150, Val Loss: 0.6990, Val Acc: 0.5000\n",
      "[28/50] Train Loss: 0.5091, Val Loss: 0.6995, Val Acc: 0.4977\n",
      "[29/50] Train Loss: 0.5011, Val Loss: 0.6990, Val Acc: 0.5023\n",
      "[30/50] Train Loss: 0.4975, Val Loss: 0.6985, Val Acc: 0.5023\n",
      "[31/50] Train Loss: 0.4839, Val Loss: 0.6992, Val Acc: 0.5113\n",
      "[32/50] Train Loss: 0.4712, Val Loss: 0.7009, Val Acc: 0.5181\n",
      "[33/50] Train Loss: 0.4813, Val Loss: 0.7042, Val Acc: 0.5136\n",
      "[34/50] Train Loss: 0.4651, Val Loss: 0.7063, Val Acc: 0.5113\n",
      "[35/50] Train Loss: 0.4533, Val Loss: 0.7078, Val Acc: 0.5045\n",
      "[36/50] Train Loss: 0.4576, Val Loss: 0.7069, Val Acc: 0.5023\n",
      "[37/50] Train Loss: 0.4516, Val Loss: 0.7067, Val Acc: 0.4977\n",
      "[38/50] Train Loss: 0.4433, Val Loss: 0.7083, Val Acc: 0.5068\n",
      "[39/50] Train Loss: 0.4282, Val Loss: 0.7124, Val Acc: 0.5136\n",
      "[40/50] Train Loss: 0.4335, Val Loss: 0.7171, Val Acc: 0.5068\n",
      "[41/50] Train Loss: 0.4327, Val Loss: 0.7170, Val Acc: 0.5249\n",
      "[42/50] Train Loss: 0.4037, Val Loss: 0.7162, Val Acc: 0.5090\n",
      "[43/50] Train Loss: 0.4113, Val Loss: 0.7179, Val Acc: 0.5136\n",
      "[44/50] Train Loss: 0.3982, Val Loss: 0.7216, Val Acc: 0.5113\n",
      "[45/50] Train Loss: 0.3870, Val Loss: 0.7262, Val Acc: 0.5226\n",
      "[46/50] Train Loss: 0.4009, Val Loss: 0.7270, Val Acc: 0.5271\n",
      "[47/50] Train Loss: 0.3715, Val Loss: 0.7252, Val Acc: 0.5226\n",
      "[48/50] Train Loss: 0.3663, Val Loss: 0.7219, Val Acc: 0.5181\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-17 16:55:17,374] Trial 0 finished with value: 0.5452488660812378 and parameters: {'gat_out_features': 12, 'gat_heads': 8, 'gat_dropout': 0.19360173812906803, 'dim1': 64, 'dim2': 64, 'dim3': 64, 'k1': 7, 'k2': 9, 'k3': 5, 's1': 5, 's2': 3, 's3': 1, 'dropout': 0.10399854914514718, 'head_dropout': 0.0569077663317042, 'kernel_size': 15, 'decomposition': False, 'revin': False, 'affine': True}. Best is trial 0 with value: 0.5452488660812378.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[49/50] Train Loss: 0.3920, Val Loss: 0.7240, Val Acc: 0.5136\n",
      "[50/50] Train Loss: 0.3581, Val Loss: 0.7281, Val Acc: 0.5045\n",
      "✅ Best Trial:\n",
      "{'gat_out_features': 12, 'gat_heads': 8, 'gat_dropout': 0.19360173812906803, 'dim1': 64, 'dim2': 64, 'dim3': 64, 'k1': 7, 'k2': 9, 'k3': 5, 's1': 5, 's2': 3, 's3': 1, 'dropout': 0.10399854914514718, 'head_dropout': 0.0569077663317042, 'kernel_size': 15, 'decomposition': False, 'revin': False, 'affine': True}\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index out of range in self",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 99\u001b[0m\n\u001b[0;32m     95\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkernel_size \u001b[38;5;241m=\u001b[39m best_params[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mkernel_size\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m     98\u001b[0m model \u001b[38;5;241m=\u001b[39m GATCNModel(BestConfig())\n\u001b[1;32m---> 99\u001b[0m train_losses, val_losses, val_accs \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_node_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medge_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_val\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    100\u001b[0m \u001b[43m                                                 \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m75\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1e-3\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    102\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m    103\u001b[0m     pred_logits \u001b[38;5;241m=\u001b[39m model(val_node_features, edge_index)\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m0\u001b[39m)  \u001b[38;5;66;03m# [1, T] → [T]\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[8], line 13\u001b[0m, in \u001b[0;36mtrain_model\u001b[1;34m(model, node_features, edge_index, X_train, y_train, X_val, y_val, epochs, lr, pos_weight)\u001b[0m\n\u001b[0;32m     11\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[0;32m     12\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m---> 13\u001b[0m train_output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnode_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medge_index\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m0\u001b[39m)[:\u001b[38;5;28mlen\u001b[39m(y_train)]  \u001b[38;5;66;03m# 통합 모델에 데이터를 적용한 결과\u001b[39;00m\n\u001b[0;32m     14\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(train_output, y_train\u001b[38;5;241m.\u001b[39mfloat())\n\u001b[0;32m     15\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[1;32mc:\\Users\\asas4\\miniconda3\\envs\\research\\lib\\site-packages\\torch\\nn\\modules\\module.py:1051\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1047\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1048\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1049\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1050\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1051\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1052\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1053\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[1;32mIn[7], line 9\u001b[0m, in \u001b[0;36mGATCNModel.forward\u001b[1;34m(self, x, edge_index)\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, edge_index):\n\u001b[1;32m----> 9\u001b[0m     embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medge_index\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# GAT를 통한 임베딩 생성\u001b[39;00m\n\u001b[0;32m     10\u001b[0m     tcn_input \u001b[38;5;241m=\u001b[39m embeddings\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)  \u001b[38;5;66;03m# TCN 입력 형태로 변환\u001b[39;00m\n\u001b[0;32m     11\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtcn(tcn_input)  \u001b[38;5;66;03m# TCN으로 예측\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\asas4\\miniconda3\\envs\\research\\lib\\site-packages\\torch\\nn\\modules\\module.py:1051\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1047\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1048\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1049\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1050\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1051\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1052\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1053\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[1;32mIn[3], line 10\u001b[0m, in \u001b[0;36mGATLayer.forward\u001b[1;34m(self, x, edge_index)\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, edge_index):\n\u001b[1;32m---> 10\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medge_index\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\asas4\\miniconda3\\envs\\research\\lib\\site-packages\\torch\\nn\\modules\\module.py:1051\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1047\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1048\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1049\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1050\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1051\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1052\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1053\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\asas4\\miniconda3\\envs\\research\\lib\\site-packages\\torch_geometric\\nn\\conv\\gatv2_conv.py:235\u001b[0m, in \u001b[0;36mGATv2Conv.forward\u001b[1;34m(self, x, edge_index, edge_attr, return_attention_weights)\u001b[0m\n\u001b[0;32m    229\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\n\u001b[0;32m    230\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe usage of \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124medge_attr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m and \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124madd_self_loops\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    231\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msimultaneously is currently not yet supported for \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    232\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124medge_index\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m in a \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSparseTensor\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m form\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    234\u001b[0m \u001b[38;5;66;03m# propagate_type: (x: PairTensor, edge_attr: OptTensor)\u001b[39;00m\n\u001b[1;32m--> 235\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpropagate\u001b[49m\u001b[43m(\u001b[49m\u001b[43medge_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mx_l\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_r\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medge_attr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43medge_attr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    236\u001b[0m \u001b[43m                     \u001b[49m\u001b[43msize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    238\u001b[0m alpha \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_alpha\n\u001b[0;32m    239\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_alpha \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\asas4\\miniconda3\\envs\\research\\lib\\site-packages\\torch_geometric\\nn\\conv\\message_passing.py:309\u001b[0m, in \u001b[0;36mMessagePassing.propagate\u001b[1;34m(self, edge_index, size, **kwargs)\u001b[0m\n\u001b[0;32m    306\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m arg \u001b[38;5;129;01min\u001b[39;00m decomp_args:\n\u001b[0;32m    307\u001b[0m         kwargs[arg] \u001b[38;5;241m=\u001b[39m decomp_kwargs[arg][i]\n\u001b[1;32m--> 309\u001b[0m coll_dict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__collect__\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__user_args__\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medge_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    310\u001b[0m \u001b[43m                             \u001b[49m\u001b[43msize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    312\u001b[0m msg_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minspector\u001b[38;5;241m.\u001b[39mdistribute(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmessage\u001b[39m\u001b[38;5;124m'\u001b[39m, coll_dict)\n\u001b[0;32m    313\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_message_forward_pre_hooks\u001b[38;5;241m.\u001b[39mvalues():\n",
      "File \u001b[1;32mc:\\Users\\asas4\\miniconda3\\envs\\research\\lib\\site-packages\\torch_geometric\\nn\\conv\\message_passing.py:202\u001b[0m, in \u001b[0;36mMessagePassing.__collect__\u001b[1;34m(self, args, edge_index, size, kwargs)\u001b[0m\n\u001b[0;32m    200\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, Tensor):\n\u001b[0;32m    201\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__set_size__(size, dim, data)\n\u001b[1;32m--> 202\u001b[0m             data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__lift__\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medge_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    204\u001b[0m         out[arg] \u001b[38;5;241m=\u001b[39m data\n\u001b[0;32m    206\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(edge_index, Tensor):\n",
      "File \u001b[1;32mc:\\Users\\asas4\\miniconda3\\envs\\research\\lib\\site-packages\\torch_geometric\\nn\\conv\\message_passing.py:172\u001b[0m, in \u001b[0;36mMessagePassing.__lift__\u001b[1;34m(self, src, edge_index, dim)\u001b[0m\n\u001b[0;32m    170\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(edge_index, Tensor):\n\u001b[0;32m    171\u001b[0m     index \u001b[38;5;241m=\u001b[39m edge_index[dim]\n\u001b[1;32m--> 172\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msrc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindex_select\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnode_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    173\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(edge_index, SparseTensor):\n\u001b[0;32m    174\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m dim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "\u001b[1;31mIndexError\u001b[0m: index out of range in self"
     ]
    }
   ],
   "source": [
    "for company_name in ['TSLA', 'NVDA', 'MSFT', 'GOOG', 'AAPL', 'DIS', 'XOM', 'CRM', 'INTC', 'AMZN']:\n",
    "    # company_name = \"AMZN\"  # 예측할 회사 선택\n",
    "    data = df_[\n",
    "        [f'prccd_{company}' for company in\n",
    "         ['TSLA', 'NVDA', 'MSFT', 'GOOG', 'AAPL', 'DIS', 'XOM', 'CRM', 'INTC', 'AMZN']] +\n",
    "        [f'cshtrd_{company_name}', f'sent_{company_name}', 'datadate']].copy()\n",
    "    data.set_index('datadate', inplace=True)\n",
    "    data.fillna(0, inplace=True)  # 감정분석 결측값을 0으로\n",
    "    data.iloc[:, :10] = data.iloc[:, :10].pct_change()\n",
    "            # returns = (close_prices[1:] / close_prices[:-1]) - 1\n",
    "        # labels = np.where(returns > 0.003, 1, 0)\n",
    "    data = data.dropna()\n",
    "    data_values = data.values\n",
    "    scaler = StandardScaler()\n",
    "    data_s = scaler.fit_transform(data_values[:, :-1])\n",
    "    df_preprocessed = np.hstack([data_s, data_values[:, -1].reshape(-1, 1)])\n",
    "    node_features = df_preprocessed\n",
    "    n_nodes = node_features.shape[0]  # 노드 수 (==날짜 수)\n",
    "\n",
    "    # 그래프 형태로 변환 : 엣지 생성 (시점 간 연결)\n",
    "    edge_list = []\n",
    "    for i in range(n_nodes):\n",
    "        for j in [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]:  # 이후 시점들에 단방향 연결; (휴장 같은 것은 생각하지 않음.... 시점 기준)\n",
    "            if i + j < n_nodes:\n",
    "                edge_list.append([i, i + j])\n",
    "    edge_index = torch.tensor(edge_list).t()\n",
    "\n",
    "\n",
    "    # edge_list = []\n",
    "    # for i in range(n_nodes):\n",
    "    #     for j in range(1, 11):  # 과거 1~10시점\n",
    "    #         if i - j >= 0:\n",
    "    #             edge_list.append([i, i - j])  # 현재 → 과거 방향\n",
    "    # edge_index = torch.tensor(edge_list).t()\n",
    "\n",
    "    # 라벨 생성\n",
    "    close_prices = data[f'prccd_{company_name}'].values\n",
    "    # returns = (close_prices[1:] / close_prices[:-1]) - 1\n",
    "    labels = np.where(close_prices > 0.003, 1, 0)  # 0.3% 초과만 1로\n",
    "    labels = torch.tensor(labels, dtype=torch.float32)\n",
    "    node_features = torch.tensor(node_features, dtype=torch.float)\n",
    "\n",
    "    split = int(len(node_features) * 0.8)\n",
    "    # print(split)\n",
    "    # print(len(labels))\n",
    "\n",
    "    # print(edge_index)\n",
    "    # train_edge_index = edge_index[:split]\n",
    "    # val_edge_index = edge_index[split:]\n",
    "\n",
    "    train_node_features = node_features[:split]\n",
    "    val_node_features = node_features[split:]\n",
    "\n",
    "    train_labels = labels[:split]\n",
    "    val_labels = labels[split:]\n",
    "    X = train_node_features[:-1]\n",
    "    y = train_labels[1:]\n",
    "    split = int(len(X) * 0.8)\n",
    "\n",
    "\n",
    "    X_train = X[:split]\n",
    "    X_val   = X[split:]\n",
    "\n",
    "    y_train = y[:split]\n",
    "    y_val   = y[split:]\n",
    "    # Optuna 튜닝 실행\n",
    "    study = optuna.create_study(direction=\"maximize\")\n",
    "    study.optimize(objective, n_trials=1)\n",
    "    # 최적 하이퍼파라미터 출력\n",
    "    print(\"✅ Best Trial:\")\n",
    "    print(study.best_trial.params)\n",
    "    best_params = study.best_trial.params\n",
    "\n",
    "\n",
    "    # BestConfig를 이용해 모델 설정 후 학습 : optuna를 통해 정해진 최적 하이퍼파라미터\n",
    "\n",
    "    class BestConfig:\n",
    "        def __init__(self):\n",
    "            # GAT 설정\n",
    "            self.gat_out_features = best_params['gat_out_features']\n",
    "            self.gat_heads = best_params['gat_heads']\n",
    "            self.gat_dropout = best_params['gat_dropout']\n",
    "\n",
    "            # TCN 설정\n",
    "            self.enc_in = best_params['gat_out_features']  # GAT 출력 = TCN 입력\n",
    "            self.dims = [best_params['dim1'], best_params['dim2'], best_params['dim3']]\n",
    "            self.large_size = [best_params['k1'], best_params['k2'], best_params['k3']]\n",
    "            self.small_size = [best_params['s1'], best_params['s2'], best_params['s3']]\n",
    "            self.small_kernel_merged = False\n",
    "            self.dropout = best_params['dropout']\n",
    "            self.head_dropout = best_params['head_dropout']\n",
    "            self.revin = best_params['revin']\n",
    "            self.affine = best_params['affine']\n",
    "            self.decomposition = best_params['decomposition']\n",
    "            self.kernel_size = best_params['kernel_size']\n",
    "\n",
    "\n",
    "    model = GATCNModel(BestConfig())\n",
    "    train_losses, val_losses, val_accs = train_model(model, train_node_features, edge_index, X_train, y_train, X_val, y_val,\n",
    "                                                     epochs=75, lr=1e-3)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        pred_logits = model(val_node_features, edge_index).squeeze(0)  # [1, T] → [T]\n",
    "        pred_probs = torch.sigmoid(pred_logits).cpu().numpy()\n",
    "        pred_labels = (pred_probs > 0.5).astype(int)\n",
    "\n",
    "    print(company_name)\n",
    "    visualize_training(company_name, train_losses, val_losses, val_accs)\n",
    "    visualize_match_only(company_name, pred_probs, val_labels.cpu().numpy())\n",
    "\n",
    "    df = pd.DataFrame(np.vstack([val_labels.cpu().numpy(), pred_labels])).T\n",
    "    df.columns = ['y_true', 'y_pred']  # 열 이름 지정\n",
    "\n",
    "    f1 = f1_score(df['y_true'], df['y_pred'], average='macro')  # or 'micro', 'weighted'\n",
    "    print(f\"F1 Score: {f1:.4f}\")\n",
    "    df.to_csv(f'data/{company_name}.csv')\n",
    "    pickle_it(model.to('cpu').state_dict(), pt.join('general_results', f'weights_{company_name}.torch'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XuoMYCoddnYg"
   },
   "source": [
    "3. 모델 생성 및 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 73
    },
    "id": "NHiQbmBy1EIe",
    "outputId": "ad3aa8c6-1a3e-48bc-96b1-1505f73c0276"
   },
   "outputs": [],
   "source": [
    "# BestConfig를 이용해 모델 설정 후 학습 :  여러 시행, 다양한 회사에서의 bestconfig 정보를 종합해 최적 파라미터 직접 설정 (아래는 예시)\n",
    "\n",
    "'''\n",
    "class BestConfig:\n",
    "    def __init__(self):\n",
    "        # GAT 설정\n",
    "        self.gat_out_features = 8\n",
    "        self.gat_heads = 4\n",
    "        self.gat_dropout = 0.2\n",
    "\n",
    "        # TCN 설정\n",
    "        self.enc_in = 8\n",
    "        self.dims = [8,16,32]\n",
    "        self.large_size = [7,5,7]\n",
    "        self.small_size = [5,1,1]\n",
    "        self.small_kernel_merged = False\n",
    "        self.dropout = 0.2\n",
    "        self.head_dropout = 0.25\n",
    "        self.revin = True\n",
    "        self.affine = False\n",
    "        self.decomposition = False\n",
    "        self.kernel_size = 31\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IZzMcisGdqIl"
   },
   "source": [
    "4. 예측 및 시각화 (best_params 기준으로 설정한 값)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HIg9asMg8vsg",
    "outputId": "d437479f-01c6-45e3-c38f-95aeb8c4b0aa"
   },
   "outputs": [],
   "source": [
    "# from sklearn.metrics import classification_report, confusion_matrix\n",
    "#\n",
    "# print(classification_report(y_val.cpu(), pred_labels))\n",
    "# print(confusion_matrix(y_val.cpu(), pred_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 73
    },
    "collapsed": true,
    "id": "FqU-FfFS7-OE",
    "outputId": "efce383a-255f-47b2-b9c4-39857d229915"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\"\"\"\n",
    "# 1. 실제 라벨\n",
    "true_labels = y_val.cpu().numpy()\n",
    "\n",
    "# 2. 다양한 threshold에 대해 f1-score 측정\n",
    "precisions, recalls, thresholds = precision_recall_curve(true_labels, pred_probs)\n",
    "\n",
    "f1s = 2 * (precisions * recalls) / (precisions + recalls + 1e-8)  # f1-score 계산\n",
    "best_idx = np.argmax(f1s)\n",
    "best_threshold = thresholds[best_idx]\n",
    "\n",
    "print(f\"✅ Best threshold by F1-score: {best_threshold:.4f}, F1: {f1s[best_idx]:.4f}\")\n",
    "\n",
    "# 3. 최적 threshold로 예측 라벨 생성\n",
    "pred_labels = (pred_probs > best_threshold).astype(int)\n",
    "# 6. 누적 수익률 (선택)\n",
    "# future_prices = close_prices[split+1:]  # 실제 수익률 계산용\n",
    "# visualize_cumulative_return(pred_probs, y_val.cpu().numpy(), future_prices)\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_df = pd.DataFrame(index=df_[split+2:]['datadate'])\n",
    "for company_name in ['TSLA', 'NVDA', 'MSFT', 'GOOG', 'AAPL', 'DIS', 'XOM', 'CRM', 'INTC', 'AMZN']:\n",
    "    company_df = pd.read_csv(f'data/{company_name}.csv', index_col=0)['y_pred']\n",
    "    company_df.name = company_name\n",
    "    company_df.index = weight_df.index\n",
    "    weight_df = pd.concat([weight_df, company_df], axis=1)\n",
    "    row_sum = (weight_df == 1).sum(axis=1)\n",
    "    row_sum.replace(0, 0.1, inplace=True)\n",
    "    weight_df_1=weight_df.div(row_sum, axis=0)\n",
    "    weight_df_1.to_csv(f'data/GAT_TCN_weight.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 455
    },
    "collapsed": true,
    "id": "vTpEBkXIHHmR",
    "outputId": "12720ec1-bc4f-4ddb-afbe-aa46ce4821fb"
   },
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# from sklearn.metrics import f1_score\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "# from lightgbm import LGBMClassifier\n",
    "#\n",
    "#\n",
    "# def train_lgbm_model(X_train, y_train, X_val):\n",
    "#     model = LGBMClassifier(\n",
    "#         n_estimators=100,\n",
    "#         learning_rate=0.1,             # 너무 작으면 과적합/학습 지연\n",
    "#         verbosity=-1\n",
    "#     )\n",
    "#     model.fit(X_train, y_train)\n",
    "#\n",
    "#     pred_probs = model.predict_proba(X_val)[:, 1]\n",
    "#     pred_labels = (pred_probs > 0.5).astype(int)\n",
    "#\n",
    "#     return model, pred_probs, pred_labels\n",
    "#\n",
    "# # 가상의 node_features, labels, df_ 등이 정의되어 있어야 합니다.\n",
    "# # 위의 리팩토링은 함수 형태만 준비되어 있고, 본문 루프는 사용자가 가진 데이터 프레임 `df_`가 있어야 실행 가능\n",
    "#\n",
    "# def run_lgbm_pipeline(df_):\n",
    "#     results = []\n",
    "#     for company_name in ['TSLA', 'NVDA', 'MSFT', 'GOOG', 'AAPL', 'DIS', 'XOM', 'CRM', 'INTC', 'AMZN']:\n",
    "#         data = df_[\n",
    "#             [f'prccd_{company}' for company in\n",
    "#              ['TSLA', 'NVDA', 'MSFT', 'GOOG', 'AAPL', 'DIS', 'XOM', 'CRM', 'INTC', 'AMZN']] +\n",
    "#             [f'cshtrd_{company_name}', f'sent_{company_name}', 'datadate']].copy()\n",
    "#\n",
    "#         data.set_index('datadate', inplace=True)\n",
    "#         data.fillna(0, inplace=True)\n",
    "#         data.iloc[:, :10] = data.iloc[:, :10].pct_change()\n",
    "#         data = data.dropna()\n",
    "#         close_prices = data[f'prccd_{company_name}'].values\n",
    "#\n",
    "#         # ✅ 라벨: t+1 수익률 기준\n",
    "#         # returns = (close_prices[1:] / close_prices[:-1]) - 1\n",
    "#         labels = np.where(close_prices > 0.003, 1, 0)\n",
    "#\n",
    "#         # ✅ feature는 시점 t까지 (맨 마지막 row 제거)\n",
    "#         data_values = data.values\n",
    "#         scaler = StandardScaler()\n",
    "#         data_s = scaler.fit_transform(data_values[:, :-1])\n",
    "#         node_features = np.hstack([data_s, data_values[:, -1].reshape(-1, 1)])\n",
    "#\n",
    "#\n",
    "#         # ✅ train/val split\n",
    "#         X = node_features[:-1]\n",
    "#         y = labels[1:]\n",
    "#         split = int(len(X) * 0.8)\n",
    "#\n",
    "#         X_train = X[:split]\n",
    "#         X_val   = X[split:]\n",
    "#\n",
    "#         y_train = y[:split]\n",
    "#         y_val   = y[split:]\n",
    "#\n",
    "#         # ✅ 모델 학습 및 예측\n",
    "#         model, pred_probs, pred_labels = train_lgbm_model(X_train, y_train, X_val)\n",
    "#\n",
    "#         print(company_name)\n",
    "#         f1 = f1_score(y_val, pred_labels, average='macro')\n",
    "#         print(f\"F1 Score: {f1:.4f}\")\n",
    "#\n",
    "#         df_result = pd.DataFrame({'y_true': y_val, 'y_pred': pred_labels})\n",
    "#         df_result.to_csv(f'data/{company_name}_lgbm.csv', index=False)\n",
    "#\n",
    "#         results.append({\n",
    "#             \"company\": company_name,\n",
    "#             \"f1_score\": f1,\n",
    "#         })\n",
    "#\n",
    "#     return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result=run_lgbm_pipeline(df_)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_df = pd.DataFrame(index=df_[split+2:]['datadate'])\n",
    "for company_name in ['TSLA', 'NVDA', 'MSFT', 'GOOG', 'AAPL', 'DIS', 'XOM', 'CRM', 'INTC', 'AMZN']:\n",
    "    company_df = pd.read_csv(f'data/{company_name}_lgbm.csv', index_col=0)['y_pred']\n",
    "    company_df.name = company_name\n",
    "    company_df.index = weight_df.index\n",
    "    weight_df = pd.concat([weight_df, company_df], axis=1)\n",
    "    row_sum = (weight_df == 1).sum(axis=1)\n",
    "    row_sum.replace(0, 0.1, inplace=True)\n",
    "    weight_df_1=weight_df.div(row_sum, axis=0)\n",
    "    weight_df_1.to_csv(f'data/LGBM_weight.csv')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "gpuType": "V28",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "research",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
